[
  {
    "objectID": "tutorial_10.html#packages",
    "href": "tutorial_10.html#packages",
    "title": "Tutorial 10: It depends! Interactive regression models",
    "section": "3.1 Packages",
    "text": "3.1 Packages\nAs before, we use the tidyverse to help with data management and visualization and texreg to make neat-looking regression tables.\n\nlibrary(tidyverse)\nlibrary(texreg)\n\nBut we now also need to load the margins package, which allows you to calculate marginal effect estimates. If you have not yet installed this package, you need to do so with install.packages(\"margins\"). Once this is done, you load the package with library(). In addition, we will use the prediction package:\n\nlibrary(margins)\nlibrary(prediction)",
    "crumbs": [
      "Tutorial 10: *It depends!* Interactive regression models"
    ]
  },
  {
    "objectID": "tutorial_10.html#data-import-and-data-cleaning",
    "href": "tutorial_10.html#data-import-and-data-cleaning",
    "title": "Tutorial 10: It depends! Interactive regression models",
    "section": "3.2 Data import and data cleaning",
    "text": "3.2 Data import and data cleaning\nThis part is exactly as before and you should now already know what to do here (see Tutorials 8 and 9 for details):\n\nUse haven::read_dta() to import the ESS round 7 (2014) dataset; save it as ess7\nTransform the dataset into the familiar format using labelled:: unlabelled();\nTrim the dataset:\n\nKeep only observations from Norway;\nSelect the following variables: essround, idno, cntry, trstplt, eduyrs — and also agea, and gndr;\nUse the pipe to link everything;\nSave the trimmed dataset as ess7;\n\nIf you like, create a data dictionary using labelled::generate_dictionary();\nTransform the trstplt variable from factor to numeric using as.numeric(); do not forget to adjust the scores; store the new variable as trstplt_num;\nDrop the empty levels of the gndr variable with droplevels();",
    "crumbs": [
      "Tutorial 10: *It depends!* Interactive regression models"
    ]
  },
  {
    "objectID": "tutorial_10.html#models-formulas",
    "href": "tutorial_10.html#models-formulas",
    "title": "Tutorial 10: It depends! Interactive regression models",
    "section": "4.1 Models & formulas",
    "text": "4.1 Models & formulas\nIn the previous two tutorials, we first estimated a bivariate regression model (which included only one independent variable) and then a multivariate regression model (which included three independent variables). Just to refresh your memory, the model equations looked like this:\nThe bivariate model included only a single independent variable (eduyrs) but no control variables:\n\\[\\begin{align*}\n  \\texttt{trstplt\\_num} = \\alpha + \\beta_1 \\texttt{eduyrs} + \\epsilon\n\\end{align*}\\]\nThe multivariate model then included also age (agea) and gender (gndr) as additional independent variables: \\[\\begin{align*}\n  \\texttt{trstplt\\_num} = \\alpha + \\beta_1 \\texttt{eduyrs} + \\beta_2 \\texttt{gndr} + \\beta_3 \\texttt{agea} + \\epsilon\n\\end{align*}\\]\nThis last model assumes that all the independent variables work separately, that each has its own unique effect and this effect does not depend on the other variables. But the guiding hypothesis for this tutorial is of course that things depend: Specifically, that the effect of age depends on gender. This means we need to extend the multivariate model to let the effect of age vary by gender, which we do by including an interaction term.\nTo interact two variables, you multiply them. Specifically, you keep the two main variables in the model (plus any other additional controls) but then you also add a new term that is the product of the two constituent variables. In this case here, this would be the product of product of age and gender. If we add that interactive term, the model becomes an interactive regression model:\n\\[\\begin{align*}\n  \\texttt{trstplt\\_num} = \\alpha + \\beta_1 \\texttt{eduyrs} + \\beta_2 \\texttt{gndr} + \\beta_3 \\texttt{agea} + \\textcolor{blue}{\\beta_4 (\\texttt{gndr} \\times \\texttt{agea}}) + \\epsilon\n\\end{align*}\\]\nThe results of this model now become much more difficult to interpret:\n\nThe coefficient for gndr, \\(\\beta_2\\) is now not the unique effect of gender but the effect of gender when agea is exactly 0 (and, by implication, the interaction term is also 0).\nSimilarly, the coefficient for agea, \\(\\beta_3\\) is no longer the unique effect of age but instead the effect of age when the gender-dummy is 0 (and, again, the interaction term is 0). Depending on how the gender-dummy is specified, this can be the case for men or women.\nThe coefficient for the interaction term, \\(\\beta_4\\) shows you how much the effects of age and gender vary: How much the effect of age differs between genders, but also how much the effect of gender differs by age.\nThe coefficient for education, \\(\\beta_1\\) is not part of the interaction. Therefore, you interpret it as before: The unique effect of an additional year of education on trust in politicians.\n\nYou probably notice that this is quite hard to wrap your head around — but this gets easier when we instead look at marginal effects, which comes below. First, however, we look at how you specify an interactive model in R.",
    "crumbs": [
      "Tutorial 10: *It depends!* Interactive regression models"
    ]
  },
  {
    "objectID": "tutorial_10.html#model-specification-in-r",
    "href": "tutorial_10.html#model-specification-in-r",
    "title": "Tutorial 10: It depends! Interactive regression models",
    "section": "4.2 Model specification in R",
    "text": "4.2 Model specification in R\nAs you know, the formula for the multivariate model (with all control variables) would be written like this:\n\ntrstplt_num ~ eduyrs + gndr + agea\n\nThere are now two ways to make the model interactive. The first option is to extend the previous equation with the interactive term that multiplies agea with gndr (for which you use the colon-symbol):\n\ntrstplt_num ~ eduyrs + gndr + agea + gndr:agea\n\nA simpler (but more indirect) way is to directly interact gndr with agea with the multiplication symbol. R will then automatically add individual terms for agea and gndr plus the interactive term when it runs the calculation:\n\ntrstplt_num ~ eduyrs + gndr*agea",
    "crumbs": [
      "Tutorial 10: *It depends!* Interactive regression models"
    ]
  },
  {
    "objectID": "tutorial_10.html#estimating-interactive-models-with-lm",
    "href": "tutorial_10.html#estimating-interactive-models-with-lm",
    "title": "Tutorial 10: It depends! Interactive regression models",
    "section": "4.3 Estimating interactive models with lm()",
    "text": "4.3 Estimating interactive models with lm()\nTo start, we quickly re-estimate the bi- and multivariate models from the previous tutorials so that we can compare these results to the interactive model’s results (and because you should anyways proceed in this stepwise fashion):3\n\n# bivariate model\nmodel1 &lt;- lm(trstplt_num ~ eduyrs, \n             data = ess7)\n\n# multivariate model\nmodel2 &lt;- lm(trstplt_num ~ eduyrs + gndr + agea, \n             data = ess7)\n\nNow that we have these models as a baseline comparison, we estimate the interactive model and print a quick summary of the results to the Console:\n\n# interactive model\nmodel3 &lt;- lm(trstplt_num ~ eduyrs + gndr*agea, \n             data = ess7)\nsummary(model3)\n\nCall:\nlm(formula = trstplt_num ~ eduyrs + gndr * agea, data = ess7)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-5.867 -1.192  0.206  1.435  5.064 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      4.813994   0.271636  17.722  &lt; 2e-16 ***\neduyrs           0.070857   0.013885   5.103 3.79e-07 ***\ngndrFemale      -0.389786   0.277113  -1.407  0.15977    \nagea            -0.012221   0.003797  -3.219  0.00132 ** \ngndrFemale:agea  0.009818   0.005488   1.789  0.07382 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.927 on 1422 degrees of freedom\n  (9 observations deleted due to missingness)\nMultiple R-squared:  0.02745,   Adjusted R-squared:  0.02471 \nF-statistic: 10.03 on 4 and 1422 DF,  p-value: 5.234e-08\n\nWhen you look at the summary, you can directly see that R has automatically “populated” the model with the correct terms: An individual term for eduyrs, then a dummy for women using gndr, another term for agea, and finally the interaction term between agea and gndrFemale (gndrFemale:agea).\nYou can see already here that the interaction term is at least significant at the 10% level (indicated by the single dot in the last line). So there might be something there. But to really know, we need to look at marginal effects.\nBefore that, we first print all the results in a proper table:\n\nscreenreg(list(model1,model2,model3),\n          custom.coef.names = c(\"Intercept\",\n                                \"Years of educ. completed\",\n                                \"Female\",\n                                \"Age\",\n                                \"Female x Age\"),\n          custom.model.names = c(\"Bivariate\",\n                                 \"Multivariate\",\n                                 \"Interactive\"),\n          stars = 0.05)\n\n==============================================================\n                          Bivariate  Multivariate  Interactive\n--------------------------------------------------------------\nIntercept                    4.23 *     4.62 *        4.81 *  \n                            (0.20)     (0.25)        (0.27)   \nYears of educ. completed     0.07 *     0.07 *        0.07 *  \n                            (0.01)     (0.01)        (0.01)   \nFemale                                  0.07         -0.39    \n                                       (0.10)        (0.28)   \nAge                                    -0.01 *       -0.01 *  \n                                       (0.00)        (0.00)   \nFemale x Age                                          0.01    \n                                                     (0.01)   \n--------------------------------------------------------------\nR^2                          0.02       0.03          0.03    \nAdj. R^2                     0.02       0.02          0.02    \nNum. obs.                 1427       1427          1427       \n==============================================================\n* p &lt; 0.05",
    "crumbs": [
      "Tutorial 10: *It depends!* Interactive regression models"
    ]
  },
  {
    "objectID": "tutorial_10.html#basic-use",
    "href": "tutorial_10.html#basic-use",
    "title": "Tutorial 10: It depends! Interactive regression models",
    "section": "5.1 Basic use",
    "text": "5.1 Basic use\nNow to the interesting part where we really make sense of the model results by looking at marginal effects. Marginal effects are more meaningful and easy to understand because they tell us the overall effect of one variable at different levels of another one: E.g., what is the effect of age for women and then for men?\nR, or more specifically the margins-package can calculate marginal effects from the ‘raw’ model results via the margins() function. This function works like this:\n\nYou need to specify the model that you want the calculation to be based on. In this case, this would be model3.\nYou need to specify for which variable you want marginal effects calculated. In this case, this would be agea.\nYou need to specify over which other variable these marginal effects should vary. Here, this would be gndr.\n\nPutting it all together, the complete margins() call would look like this:\n\nmargins(model = model3,\n        variables = \"agea\",\n        at = list(gndr = c(\"Female\",\"Male\")))\n\n at(gndr)      agea\n     Male -0.012221\n   Female -0.002403\n\n\nThese numbers may seem cryptic at first sight, but they are actually relatively easy to read. The first line gives you the effect of age for women. You read this as: If someone is a woman, every additional year of age decreases trust in politicians by a very tiny -0.002 points.\nThe second line gives you the corresponding result for men: If someone is a man, every additional year of age decreases trust in politicians by -0.012 points.\nIn both cases, the effects are surely not very large — but you also see that the effect of age for men, however small it may be, is still about five times as large as the tiny effect for women (-0.012221/-0.002403 = 5.085726). So, it does look like the effect of age differs by gender!",
    "crumbs": [
      "Tutorial 10: *It depends!* Interactive regression models"
    ]
  },
  {
    "objectID": "tutorial_10.html#getting-p-values-and-confidence-intervals",
    "href": "tutorial_10.html#getting-p-values-and-confidence-intervals",
    "title": "Tutorial 10: It depends! Interactive regression models",
    "section": "5.2 Getting p-values and confidence intervals",
    "text": "5.2 Getting p-values and confidence intervals\nThe results we have so far are interesting, but they are only half of the story — we also need to account for the fact that these numbers are estimates based on sample data and that we cannot just take them at face value. We also need to look if they really are statistically significant, meaning if we can say with sufficient confidence that these effects also exist in the general population for which we want to make inferences. As you know, we look at p-values and confidence intervals to figure this out.\nTo get these additional statistics, we use margins()’s sister-function, margins_nummary():\n\nmargins_summary(model = model3, \n                variables = \"agea\", \n                at = list(gndr = c(\"Male\",\"Female\")))\n factor   gndr     AME     SE       z      p   lower   upper\n   agea 1.0000 -0.0122 0.0038 -3.2186 0.0013 -0.0197 -0.0048\n   agea 2.0000 -0.0024 0.0040 -0.6039 0.5459 -0.0102  0.0054\n\nThis result now tells us the complete story, only the values of the gndr variable are presented in a less informative way. But we can simply use bst::visfactor() to figure out what 1 and 2 correspond to:\n\nbst290::visfactor(variable = \"gndr\", dataset = ess7)\n values labels\n      1   Male\n      2 Female\n\nIf we now look at the margins_summary() result and specifically at the p-values (under p), we see that only the marginal effect of age for men (see AME) is really statistically significant (p = 0.0013 &lt; 0.05). The marginal effect of age for women, on the other hand, is not (p = 0.5459).\nThis means: There (probably) is a small negative effect of age on trust in politicians in the general Norwegian population — but this effect exists only among Norwegian men. In the case of women, we cannot say with sufficient confidence if the true effect is really different from 0 or not, and so we assume that age has no effect on political trust in the case of Norwegian women.\nYou can also see this if you look at the last two columns of the result, which give you the upper and lower limits of the confidence intervals of the marginal effect estimates. The confidence interval for men does not include 0, while the one for women does overlap with 0.",
    "crumbs": [
      "Tutorial 10: *It depends!* Interactive regression models"
    ]
  },
  {
    "objectID": "tutorial_10.html#visualizing-marginal-effect-estimates",
    "href": "tutorial_10.html#visualizing-marginal-effect-estimates",
    "title": "Tutorial 10: It depends! Interactive regression models",
    "section": "5.3 Visualizing marginal effect estimates",
    "text": "5.3 Visualizing marginal effect estimates\nTo make the interpretation of the results still more intuitive for yourself (and especially for your readers!), you can and should visualize the result. Luckily, the output you get from margins_summary() is a data.frame, which means you can directly “pipe” it into a ggplot() graph.\nA raw, unpolished graph (here a bar graph; geom_point() is an alternative) would look like this:\n\nmargins_summary(model = model3, \n                variables = \"agea\", \n                at = list(gndr = c(\"Male\",\"Female\"))) %&gt;% \n  ggplot(aes(x = gndr, y = AME, ymin = lower, ymax = upper)) +\n    geom_bar(stat = \"identity\") + # draws marginal effect estimates as bars\n    geom_linerange() # draws confidence intervals as lines\n\n\n\n\n\n\n\n\nNote that geom_linerange(), which draws the confidence intervals, needs to know the upper and lower values of the lines it is supposed to draw. You need to specify these with ymin and ymax within aes().\nWhat you see here are the marginal effect estimates of age separately for men and women. You can directly see that the effect of age is more strongly negative for men than for women. If you look carefully, you can also see that the confidence interval overlaps with 0 in the case of women — indicating again the lack of statistical significance.\n\nTo make the interpretation even easier, we would polish the graph a bit more by adding proper labels to the axes, using a better-looking scheme, and also by adding a horizontal reference line at the 0-point on the y-axis with geom_hline().\n\nmargins_summary(model = model3, \n                variables = \"agea\", \n                at = list(gndr = c(\"Male\",\"Female\"))) %&gt;% \n  ggplot(aes(x = gndr, y = AME, ymin = lower, ymax = upper)) +\n    geom_bar(stat = \"identity\") +\n    geom_linerange() +\n    geom_hline(yintercept = 0, linetype = \"dashed\", color = \"gray\") +\n    labs(x = \"Gender\",\n         y = \"Effect of age on trust in politicians\",\n         caption = \"95% confidence intervals.\") +\n    theme_classic()\n\n\n\n\n\n\n\n\nThis graph should make the overall result fairly apparent: Among men, every additional year of age decreases trust in politicians by around -0.012 points. Among women, on the other hand, the effect of age is not significantly different from 0. In other words, we have shown that the effect of age on trust in politicians depends on gender! Only men trust politicians less as they get older, while age makes no difference for women.",
    "crumbs": [
      "Tutorial 10: *It depends!* Interactive regression models"
    ]
  },
  {
    "objectID": "tutorial_10.html#footnotes",
    "href": "tutorial_10.html#footnotes",
    "title": "Tutorial 10: It depends! Interactive regression models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor example, Liechti et al. (Liechti, F., Fossati, F., Bonoli, G., and Auer, D. (2017). The signalling value of labour market programmes. European Sociological Review, 33(2):257–274.) show that labor market re-integration programs for unemployed workers have different effects depending on whether the unemployed worker is an immigrant or not.↩︎\nA widely-cited studied that established this (and which you should cite if you estimate interactive models) is Brambor, T., Clark, W. R., and Golder, M. (2006). Understanding interaction models: Improving empirical analyses. Political Analysis, 14(1):63–82.↩︎\nSee Lenz, G. S. and Sahn, A. (2021). Achieving statistical significance with control variables and without transparency. Political Analysis, 29(3):356–369.↩︎",
    "crumbs": [
      "Tutorial 10: *It depends!* Interactive regression models"
    ]
  },
  {
    "objectID": "tutorial_6.html",
    "href": "tutorial_6.html",
    "title": "Tutorial 6: Tabular Analysis & the \\(\\chi^2\\) test",
    "section": "",
    "text": "As (social) data analysts, we are very often interested in relationships between variables or causal effects: Does one variable (e.g., gender) have an impact on another variable (e.g., political attitudes)?\nWe can look for evidence of relationships in two ways:\n\nA descriptive analysis will show first evidence of whether there are any interesting patterns or differences in our dataset;\nA formal statistical test will then tell us if whatever patterns we found earlier are statistically significant: If we can generalize the finding from our sample dataset to the general population.\n\nIn this tutorial, you learn how you can study relationships between two categorical variables with a descriptive tabular analysis and a formal statistical test, the \\(\\chi^2\\) (“chi-squared”) test. More specifically, we will study if men and women in Norway differ in their patterns of political participation when it comes to being a member of a trade union.\nUnlike in the previous tutorials, we will start right away with a real research dataset (data from the 2014 round of the European Social Survey).\n\n\n\n\n\n\nTip\n\n\n\nHvis du ønsker å lese en norsk tekst i tillegg: “Lær deg R”, Kapittel 5.3.",
    "crumbs": [
      "Tutorial 6: Tabular Analysis & the $\\chi^2$ test"
    ]
  },
  {
    "objectID": "tutorial_6.html#loading-packages",
    "href": "tutorial_6.html#loading-packages",
    "title": "Tutorial 6: Tabular Analysis & the \\(\\chi^2\\) test",
    "section": "3.1 Loading packages",
    "text": "3.1 Loading packages\nIn this tutorial, you will need two packages: tidyverse for the data management and bst290 for a special function to make cross tables. As you know, you load these with the library() function:\n\nlibrary(tidyverse)\nlibrary(bst290)\n\nThen you use the read_dta() function from the haven package to import the dataset — make sure that you store (“assign”) the dataset with the assignment operator (&lt;-):\n\ness7 &lt;- haven::read_dta(\"ess7.dta\") # change the file name if necessary",
    "crumbs": [
      "Tutorial 6: Tabular Analysis & the $\\chi^2$ test"
    ]
  },
  {
    "objectID": "tutorial_6.html#data-cleaning",
    "href": "tutorial_6.html#data-cleaning",
    "title": "Tutorial 6: Tabular Analysis & the \\(\\chi^2\\) test",
    "section": "3.2 Data cleaning",
    "text": "3.2 Data cleaning\nNow that the dataset is loaded, you can clean and trim it.\nFirst, you need to convert how the dataset is stored within R back to the familiar format with labelled::unlabelled():\n\ness7 &lt;- labelled::unlabelled(ess7)\n\nThen you trim the data down to the variables and observations you really need. We will only work with the data for Norway, and only the following variables:\n\nessround, idno, and cntry;\ngndr: The respondent’s gender;\nmbtru: Whether the respondent is (or was) a member of a trade union;\n\nYou can do both “trimming” steps in one go with the pipe operator (%&gt;%):\n\ness7 %&gt;% \n  filter(cntry==\"NO\") %&gt;% \n  select(essround,cntry,idno,gndr,mbtru) -&gt; ess7\n\nAlways make sure that you store the result, the new “trimmed” dataset. At the very end of the code chunk, we use the “reversed” assignment operator to store the trimmed dataset under the same name (-&gt; ess7) in the Environment!\nIf you like, you can also create a “dictionary” of your dataset with labelled::generate_dictionary():\n\ndict &lt;- labelled::generate_dictionary(ess7)",
    "crumbs": [
      "Tutorial 6: Tabular Analysis & the $\\chi^2$ test"
    ]
  },
  {
    "objectID": "tutorial_6.html#initial-exploratory-data-analysis-eda",
    "href": "tutorial_6.html#initial-exploratory-data-analysis-eda",
    "title": "Tutorial 6: Tabular Analysis & the \\(\\chi^2\\) test",
    "section": "3.3 Initial exploratory data analysis (EDA)",
    "text": "3.3 Initial exploratory data analysis (EDA)\nNow that you have your dataset in order, you can take a closer look at your main variables. As mentioned in the Introduction, we will look at the relationship between gender and trade union membership in the main part of this tutorial. This means the two variables we need to work with are gender (gndr) and trade union membership (mbtru).\nYou can use attributes() to get a first overview over the two variables and how they are stored.\n\nattributes(ess7$gndr)\n## $levels\n## [1] \"Male\"   \"Female\"\n## \n## $label\n## [1] \"Gender\"\n## \n## $class\n## [1] \"factor\"\nattributes(ess7$mbtru)\n## $levels\n## [1] \"Yes, currently\"  \"Yes, previously\" \"No\"             \n## \n## $label\n## [1] \"Member of trade union or similar organisation\"\n## \n## $class\n## [1] \"factor\"\n\nBoth variables are categorical variables and should therefore be stored as factors — and that is indeed the case.\n\nIn a next step, you should check how many observations you have per category of each variable, and if there are some empty categories. You do this with the table() function.\nIn the case of the gndr-variable, this looks as follows:\n\ntable(ess7$gndr)\n## \n##   Male Female \n##    764    672\n\nIt turns out that there a few more men than women in the dataset, and there are no “empty” categories.\nThe situation is similar in the case of the mbtru variable:\n\ntable(ess7$mbtru)\n## \n##  Yes, currently Yes, previously              No \n##             663             277             493",
    "crumbs": [
      "Tutorial 6: Tabular Analysis & the $\\chi^2$ test"
    ]
  },
  {
    "objectID": "tutorial_6.html#descriptive-analysis",
    "href": "tutorial_6.html#descriptive-analysis",
    "title": "Tutorial 6: Tabular Analysis & the \\(\\chi^2\\) test",
    "section": "4.1 Descriptive analysis",
    "text": "4.1 Descriptive analysis\nAs explained in Kellstedt & Whitten (2018, Chapter 8), the standard way to starting looking for relationships between categorical variables is to cross-tabulate them. This table would be organized like this:\n\nThe independent variable would appear in the columns;\nThe dependent variable would appear in the rows;\n\nIn this analysis, we expect that gender (gndr) has an effect on trade union membership (mbtru). This means,\n\ngndr is the independent variable;\nmbtru is the dependent variable;\n\n\n\n4.1.1 A cross-table showing frequencies\nTo cross-tabulate the two variables, you can use the table() function, which you already know from before. The variable that should be shown in the rows is named first, then the variable that should be shown in the columns:\n\ntable(ess7$mbtru,ess7$gndr)\n##                  \n##                   Male Female\n##   Yes, currently   328    335\n##   Yes, previously  164    113\n##   No               271    222\n\nThis table shows you the “raw” numbers of respondents that fall into each category — the frequencies. As it is now, this table is a bit difficult to read because it is not easy to see if there is an over- or underrepresentation of women or men among trade union members.\n\n\n\n4.1.2 A cross-table showing percentages\nA better way to present the data is in the form of a cross-table that shows column percentages (see again Kellstedt and Whitten 2018, Chapter 8). Creating such a table is possible in R (see Hermansen 2019, Lær deg R Chapter 5.3.1), but that is a multi-step process with lots of potential for error messages and frustration.\nTo make this easier and quicker, you can use a special function from the bst290 package called krysstabell(). When using this function, you just need to specify a) the dataset; b) the variable that should be shown in the rows; and c) the variable to be shown in the columns:\n\nkrysstabell(dataset = ess7,\n            rowvar = \"mbtru\",\n            colvar = \"gndr\")\n##                  gndr\n## mbtru               Male Female    Sum\n##   Yes, currently   42.99  50.00  46.27\n##   Yes, previously  21.49  16.87  19.33\n##   No               35.52  33.13  34.40\n##   Sum             100.00 100.00 100.00\n\nThis table is a bit easier to read: You see that around 43% of men in the sample are members in a trade union, while the share of trade union members among women is 50%. Conversely, fewer women than men used to be members previously or are not members. This indicates that there might be a relationship between gender and trade union membership in Norway — but here, women are more, not less likely to be members than men.\nThe krysstabell() function has an export-option (export=TRUE) that you can use to get output that you can copy/paste into your Word document and style there (see also Tutorial 2).",
    "crumbs": [
      "Tutorial 6: Tabular Analysis & the $\\chi^2$ test"
    ]
  },
  {
    "objectID": "tutorial_6.html#running-a-chi2-test",
    "href": "tutorial_6.html#running-a-chi2-test",
    "title": "Tutorial 6: Tabular Analysis & the \\(\\chi^2\\) test",
    "section": "4.2 Running a \\(\\chi^2\\) test",
    "text": "4.2 Running a \\(\\chi^2\\) test\nThe previous descriptive analysis suggests that there might be a relationship between gender and trade union membership. But: It is possible that this reflects only sampling variation: After all, we are working with a sample of the entire Norwegian population. Maybe we were really unlucky and ended up by mere chance with a survey sample in which women are more likely to be trade union members, even though this is not true for the wider Norwegian population?\nTo find out if our relationship is systematic or statistically significant, we can use the \\(\\chi^2\\) test. As you know, this is a formal statistical test for relationships between two categorical variables.\nRunning this test in R is easy because there is a built-in function for it: chisq.test(). Usually, you run this function directly on the cross table.3\nThis means: You first cross-tabulate your two variables and save the result:\n\ncrosstab &lt;- table(ess7$mbtru,ess7$gndr)\n\nThen you use the table that you just saved in the chisq.test() function:\n\nchisq.test(crosstab)\n## \n##  Pearson's Chi-squared test\n## \n## data:  crosstab\n## X-squared = 8.3335, df = 2, p-value = 0.0155",
    "crumbs": [
      "Tutorial 6: Tabular Analysis & the $\\chi^2$ test"
    ]
  },
  {
    "objectID": "tutorial_6.html#interpretation",
    "href": "tutorial_6.html#interpretation",
    "title": "Tutorial 6: Tabular Analysis & the \\(\\chi^2\\) test",
    "section": "4.3 Interpretation",
    "text": "4.3 Interpretation\n\nchisq.test(crosstab)\n## \n##  Pearson's Chi-squared test\n## \n## data:  crosstab\n## X-squared = 8.3335, df = 2, p-value = 0.0155\n\nThe output should be possible to understand if you read Kellstedt and Whitten (2018, see Chapter 8):\n\ndf refers to the degrees of freedom. You calculate these as \\(df = (r - 1)(c - 1)\\) where \\(r\\) is the number of rows of the cross table, and \\(c\\) is the number of columns. In this case, this is \\((3 - 1)(2 - 1) = 2\\times1 = 2\\).\nX-squared is the \\(\\chi^2\\) statistic. This is the result of a longer calculation that is explained in Kellstedt and Whitten (and also the interactive dashboard included in the bst290 package; see bst290::practiceStatistics());\np-value is, obviously, the associated p-value.\n\nCan you make sense of the result?",
    "crumbs": [
      "Tutorial 6: Tabular Analysis & the $\\chi^2$ test"
    ]
  },
  {
    "objectID": "tutorial_6.html#footnotes",
    "href": "tutorial_6.html#footnotes",
    "title": "Tutorial 6: Tabular Analysis & the \\(\\chi^2\\) test",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee Gallego (2007), “Unequal Participation in Europe”, International Journal of Sociology 37(4):10-25.↩︎\nSee e.g., Iversen and Rosenbluth (2010), Women, Work, and Politics, Yale University Press.↩︎\nYou can technically also enter two factor-variables like this: chisq.test(ess7$mbtru,ess7$gndr). However, creating the table first helps to remind you that you always run the actual test on the table, not on the underlying data.↩︎",
    "crumbs": [
      "Tutorial 6: Tabular Analysis & the $\\chi^2$ test"
    ]
  },
  {
    "objectID": "tutorial_5.html",
    "href": "tutorial_5.html",
    "title": "Tutorial 5: Confidence intervals",
    "section": "",
    "text": "You have now made it through the software- and code-heavy parts of the course and learned the basics of data handling and visualization with R. Now, beginning with this tutorial, the focus will shift away from software and code and instead focus more on statistics. This means that the tutorials will say less about how to write code and more about how you make sense of your results.\nThe reason for this is the following: Running statistical tests and estimations in R is not really difficult as far as coding is concerned. As you will see, most statistical estimations require only a few brief lines of code. The tricky thing is knowing how to write the necessary bits of code correctly. But the most difficult part is how to make sense of the results. Here you need to understand what the different statistical concepts such as p-values or standard errors are and how you identify them in the output that R gives you. And this is why we focus mostly on the latter part, the interpretation.\nThis being said, we will still use these tutorials to repeat and practice the data management and visualization skills that you learned previously.\nWe start in this tutorial with the first building block of inferential statistics: confidence intervals. You will learn how to use R to calculate a confidence interval for a sample mean and how to interpret the result — as was explained in Kellstedt & Whitten (2018, Chapter 7). As before, we start by working with the small ess practice dataset included in the bst290 package. You will then apply what you learned on real ESS data in the in-class exercises.",
    "crumbs": [
      "Tutorial 5: Confidence intervals"
    ]
  },
  {
    "objectID": "tutorial_5.html#calculating-the-sample-mean",
    "href": "tutorial_5.html#calculating-the-sample-mean",
    "title": "Tutorial 5: Confidence intervals",
    "section": "5.1 Calculating the sample mean",
    "text": "5.1 Calculating the sample mean\nStep 1 is to calculate the mean of the new stflife_num variable — after all, we want to have a confidence interval for the sample mean, which means we have to first figure out what that sample mean is!\nCalculating the sample mean is, of course, easy:\n\nmean(ess$stflife_num)\n## [1] 7.86014\n\nThe high value of almost 8 on the 0-10 scale again reflects the overall high level of life satisfaction in Norway.",
    "crumbs": [
      "Tutorial 5: Confidence intervals"
    ]
  },
  {
    "objectID": "tutorial_5.html#estimating-the-confidence-interval-using-a-built-in-function",
    "href": "tutorial_5.html#estimating-the-confidence-interval-using-a-built-in-function",
    "title": "Tutorial 5: Confidence intervals",
    "section": "5.2 Estimating the confidence interval using a built-in function",
    "text": "5.2 Estimating the confidence interval using a built-in function\nTo calculate a confidence interval for a sample mean in R, you use the t.test() function. This may seem confusing: Why is that function not called conf.int() or similar? Does that function really use the same formula that was explained in Kellstedt & Whitten?\nThe reason behind this is that the t-test (which you will learn about in two weeks) and the confidence interval are related, and R uses the t.test() function for both procedures. The difference between the “by-hand” approach in Kellstedt & Whitten and the t.test() function is the following:\n\nThe formula by Kellstedt & Whitten simply assumes that you are working with a large sample (a few hundred observations or more). This is often reasonable, but not always.\nThe t.test() function uses a slightly different calculation that makes an adjustment for the sample size you actually have. This will not really make a difference if your sample size is large (&gt;1.000), and usually only a minor difference for smaller samples.\n\nGenerally speaking, you are safe if you use the t.test() function (and you work with a random sample, of course!).\n\n\n5.2.1 Using t.test()\nThe code to calculate the confidence interval with t.test() is simple:\n\nt.test(x = ess$stflife_num)\n## \n##  One Sample t-test\n## \n## data:  ess$stflife_num\n## t = 52.383, df = 142, p-value &lt; 2.2e-16\n## alternative hypothesis: true mean is not equal to 0\n## 95 percent confidence interval:\n##  7.563515 8.156765\n## sample estimates:\n## mean of x \n##   7.86014\n\nAll you do is call the t.test() function and, within it, name the variable for which you want a confidence interval — similar to the mean() or sd() functions! The t.test() function uses the 95% level of confidence by default, but you can adjust this as you will see a bit further below.\nThe important part of the output is of course the line below the one saying 95 percent confidence interval: Here you see the lower and upper limit of the 95% confidence interval: 7.564 and 8.157.\nCan you interpret the result here (see Kellstedt/Whitten 2018, Chapter 7)?\nYou also see the sample mean of 7.86014 mentioned again at the bottom of the output.\n\n\n\n5.2.2 Adjusting the level of confidence\nIt is easy to get a different level of confidence: You just change the level with the conf.level option (“argument”). For example, to get a 90% confidence interval, you run:\n\nt.test(x = ess$stflife_num, conf.level = 0.90)\n## \n##  One Sample t-test\n## \n## data:  ess$stflife_num\n## t = 52.383, df = 142, p-value &lt; 2.2e-16\n## alternative hypothesis: true mean is not equal to 0\n## 90 percent confidence interval:\n##  7.611705 8.108574\n## sample estimates:\n## mean of x \n##   7.86014\n\nAnd to get a 99% confidence interval, you run:\n\nt.test(x = ess$stflife_num, conf.level = 0.99)\n## \n##  One Sample t-test\n## \n## data:  ess$stflife_num\n## t = 52.383, df = 142, p-value &lt; 2.2e-16\n## alternative hypothesis: true mean is not equal to 0\n## 99 percent confidence interval:\n##  7.468369 8.251910\n## sample estimates:\n## mean of x \n##   7.86014\n\nAnd that is it! You now know how to calculate a confidence interval for a sample mean in R. You may have noticed that we spent quite a bit of time on data cleaning at the beginning while the “meat part” — the actual estimation – was short. This reflects the central aspect of data analysis in R: The easy things are difficult, but the difficult things are easy.",
    "crumbs": [
      "Tutorial 5: Confidence intervals"
    ]
  },
  {
    "objectID": "tutorial_5.html#the-math",
    "href": "tutorial_5.html#the-math",
    "title": "Tutorial 5: Confidence intervals",
    "section": "7.1 The math",
    "text": "7.1 The math\nAs explained in Kellstedt & Whitten, the formula to get the 95% confidence interval for the sample mean of a variable \\(Y\\) is the following: \\[CI_{\\bar{Y}} = \\bar{Y} \\pm 1.96 \\times \\sigma_{\\bar{Y}}\\]\nwhere \\(\\bar{Y}\\) is the sample mean of the variable, \\(\\sigma_{\\bar{Y}}\\) is the standard error of that mean, and 1.96 (no, not 2!) is the critical value for a 95% confidence level.\nThe standard error of the mean \\(\\sigma_{\\bar{Y}}\\) is calculated as follows: \\[\\sigma_{\\bar{Y}} = \\frac{s_Y}{\\sqrt{n}}\\]\nwhere \\(s_Y\\) is the sample standard deviation of the \\(Y\\) variable, and \\(n\\) is the sample size.",
    "crumbs": [
      "Tutorial 5: Confidence intervals"
    ]
  },
  {
    "objectID": "tutorial_5.html#the-calculation-in-r",
    "href": "tutorial_5.html#the-calculation-in-r",
    "title": "Tutorial 5: Confidence intervals",
    "section": "7.2 The calculation in R",
    "text": "7.2 The calculation in R\nAs before, step 1 is get the mean with the mean() function. Important: We need to store the result so we can use it in our later calculations:\n\nY_bar &lt;- mean(ess$stflife_num, na.rm = T)\nY_bar\n## [1] 7.86014\n\nNow we have to calculate the standard error of this mean value. For that, we first need the standard deviation, which we also store:\n\nY_sd &lt;- sd(ess$stflife_num, na.rm = T)\nY_sd\n## [1] 1.794363\n\nAnd then we need the sample size. Here we need to pay attention that we do not accidentally count missing observations (NAs). We do this by letting R calculate the sum of non-missing (!is.na()) observations of our variable:\n\nY_n &lt;- sum(!is.na(ess$stflife_num))\nY_n\n## [1] 143\n\nWe can then use the sample size and the standard deviation to calculate the standard error:\n\nY_se &lt;- (Y_sd/sqrt(Y_n))\nY_se\n## [1] 0.1500522\n\nNow we have everything we need. All we have to do is to plug the values into the main formula. First, we calculate the upper limit of the confidence interval and store the result:\n\nupper &lt;- Y_bar + 1.96 * Y_se\n\nThen we calculate the lower limit while storing the result:\n\nlower &lt;- Y_bar - 1.96 * Y_se\n\nAnd then we print out the result:\n\nlower\n## [1] 7.566038\nupper\n## [1] 8.154242\n\nIf you pay close attention, you may notice that the interval we get here is almost identical to the one we got when we used the t.test() function.",
    "crumbs": [
      "Tutorial 5: Confidence intervals"
    ]
  },
  {
    "objectID": "tutorial_5.html#footnotes",
    "href": "tutorial_5.html#footnotes",
    "title": "Tutorial 5: Confidence intervals",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee e.g., https://worldhappiness.report/ed/2020/the-nordic-exceptionalism-what-explains-why-the-nordic-countries-are-constantly-among-the-happiest-in-the-world/↩︎",
    "crumbs": [
      "Tutorial 5: Confidence intervals"
    ]
  },
  {
    "objectID": "tutorial_4.html",
    "href": "tutorial_4.html",
    "title": "Tutorial 4: Data visualization with ggplot2",
    "section": "",
    "text": "You made it through the most tedious parts, the descriptive tables and data transformation procedures. Data visualization is where the fun begins.\nIf you are able to visualize your data, you can turn boring and dull rows of numbers and symbols into nice and informative graphs. In other words, you can do or produce something you and other people can see and use directly in reports, on websites, etc. Obviously, that is a skill that you can directly use at many workplaces where (social) data analysts work. And, if you keep working on it, you will eventually be able to produce more and more advanced and beautiful graphs.\nIn this tutorial, you will learn the basics of visualizing data with ggplot2, which is one of several ways to visualize data in R.1\nYou learn ggplot2 here for a few reasons:\n\nIt is arguably the most widely used package for data visualization in R (and generally one of the most frequently downloaded packages overall). The BBC Data Team, for example, uses ggplot2 to make graphics.\nIt is extremely flexible. You can generate a wide range of different types of graphs, and each graph can be customized down to the finest detail (as long as you know what you have to do).\nIt can be extended. One extension called ggiraph allows you to make ggplot2 graphs interactive.\n\nObviously, this tutorial will only cover the basics — but you can take a look at some examples of what you will eventually be able to do with ggplot2 and R:\n\nhttps://bbc.github.io/rcookbook/\nhttps://www.r-graph-gallery.com/\nhttps://www.data-to-art.com/\n\n(The first two websites are also generally highly recommended if you want to look for example code for a particular type of graph!)\nFor a more thorough introduction to data visualization with R see https://ggplot2-book.org/ or https://r-graphics.org/index.html.\n\n\n\n\n\n\nTip\n\n\n\nHvis du ønsker å lese en norsk tekst i tillegg: “Lær deg R”, Kapittel 3 & 5. OBS: Boken bruker en annen ‘dialekt’ (base R) enn den vi bruker her (tidyverse/ggplot2).",
    "crumbs": [
      "Tutorial 4: Data visualization with `ggplot2`"
    ]
  },
  {
    "objectID": "tutorial_4.html#scatterplots",
    "href": "tutorial_4.html#scatterplots",
    "title": "Tutorial 4: Data visualization with ggplot2",
    "section": "3.1 Scatterplots",
    "text": "3.1 Scatterplots\nScatterplots show the relationship between two numeric or continuous variables, and they are widely used in both academic research but also the “real world” (maybe you have seen Hans Rosling’s famous “200 countries, 400 years, 4 minutes” presentation that featured a scatterplot?).2 Therefore, knowing how to make a scatterplot is obviously handy!\nScatterplots are also a great way to show you the basic logic of ggplot2. For our scatterplot, you use two numeric variables from the small ess practice-dataset:\n\nheight: The respondent’s height in cm;\nweight: The respondent’s weight in kg;\n\nFeel free to familiarize yourself with these variables first, using the tools you learned in the previous tutorials such as summary().\n\nOnce you you know what you are working with, take a look at the scatterplot below and the code that generated it:\n\nggplot(data = ess,\n       aes(x = height, y = weight)) +\n        geom_point()\n\n\n\n\n\n\n\n\n\n\nggplot(data = ess,\n       aes(x = height, y = weight)) +\n        geom_point()\n\nLet’s go over the code and graph step-by-step so you understand what is going on here:\n\nThe first function is the ggplot() function — the ggplot() function is the main function in ggplot2. You use it to tell R that you want to make a graph with ggplot2, and it includes the main information needed for the graph:\n\nWhich dataset you want to use; in this case, you use data = ess to say that you want to want to use the ess dataset;\nYou define the main ‘aesthetics’ (aes()) of your graph — which variable goes on the horizontal x-axis (height) and which goes on the vertical y-axis (weight).\n\nThen notice the + symbol at the end of the second line; this tells R that you want to add a layer to the graph;\nThis layer is defined with geom_point() — you say that you want to graph the individual data points, which together form a scatterplot.\n\n\n\n\n\n\n\n\n\n\nIf this was a bit much, it might help to see what happens when you construct the graph one step at a time, which comes on the next page. \n\n3.1.1 The graph window\nConsider what happens when you only run the ggplot() function without anything else added:\n\nggplot()\n\n\n\n\n\n\n\n\nAll that happens is that you get an empty graph window with nothing in it — which is obvious because you have not told R what exactly you want to put into your graph.\n\n\n\n3.1.2 Defining the dataset\nNext, you add information about which dataset you want to use:\n\nggplot(data = ess)\n\n\n\n\n\n\n\n\nNo real improvement — but, then again, what is R supposed to do if all it knows is which dataset it should work with, but not the variables and where in the graph they go?\n\n\n\n3.1.3 Defining the variables and their roles (‘aesthetics’)\nNow you add information about the main dimensions or ‘aesthetics’ of the graph. In this case, the relevant bits of information are which variables you want to graph and what roles they play in your graph:\n\nggplot(data = ess,\n       aes(x = height, y = weight))\n\n\n\n\n\n\n\n\nIn this case, you tell R that you want to use the height and weight variables, and that they are supposed to form the horizontal x-axis (height) and the vertical y-axis (weight). You see the effect in the form of labels on the axes, and a grid system inside the graph.\nBut still: No contents! Where are the data points the graph is supposed to show?\n\n\n\n3.1.4 Defining the contents (‘geometric objects’)\nIn ggplot2, you add the actual contents of your graph — the points in a scatterplot, the lines in a line graph, or the bars in a bar graph — as additional layers of ‘geometric objects’ or ‘geoms’ (which makes sense: points, lines, and bars are geometric objects).\nIn the current case, you add a layer of points with geom_point():\n\nggplot(data = ess,\n       aes(x = height, y = weight)) +\n        geom_point()\n\n\n\n\n\n\n\n\nAnd there it is: A rough but complete scatterplot.\nImportant: Notice again the plus-symbol (+) at the end of the line before geom_point(). You always use this symbol in ggplot2 to add additional graph layers.\nYou will see that functionality clearer when we add more layers like nice labels, which comes now.\n\n\n\n3.1.5 Informative labels\nThe scatterplot you have now is acceptable if you would only like to visualize the data for yourself, as a part of getting to know your data. But the graph is not in a shape in which you could include it in a course paper, thesis, or report — it is not a “publication quality” graph.\nOne thing that is missing are informative labels for the two axes. You can add these as additional layers to your graph with labs():\n\nggplot(data = ess,\n       aes(x = height, y = weight)) +\n        geom_point() +\n        labs(x = \"Body height (cm)\",\n             y = \"Body weight (kg)\")\n## Warning: Removed 5 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n\n\n\n\n\n\n\n\nNotice that the text that forms the axis labels is put within quotation marks. This is important: Text always goes within quotation marks!\nIf you want, you can also add other labels such as a title, a subtitle, or a note (“caption”). See ?ggplot2::labs() for details.\n\n\n\n3.1.6 Changing the overall look of the graph (‘theme’)\nAlmost there — but the gray background makes the graph look a bit dull.3\nIn ggplot2, you can overwrite the default theme by adding a theme layer. For example, to use a simple black and white theme, you would add theme_bw():\n\nggplot(data = ess,\n       aes(x = height, y = weight)) +\n        geom_point() +\n        labs(x = \"Body height (cm)\",\n             y = \"Body weight (kg)\") +\n        theme_bw()\n\n\n\n\n\n\n\n\nAnd this is now a graph that you could include in a course paper, etc. Of course, there are many, many more ways to tweak this graph even further, but what you have is fine for now.\nIf you want to see what other themes you can choose from, visit https://ggplot2.tidyverse.org/reference/ggtheme.html",
    "crumbs": [
      "Tutorial 4: Data visualization with `ggplot2`"
    ]
  },
  {
    "objectID": "tutorial_4.html#bivariate-boxplots",
    "href": "tutorial_4.html#bivariate-boxplots",
    "title": "Tutorial 4: Data visualization with ggplot2",
    "section": "3.2 Bivariate boxplots",
    "text": "3.2 Bivariate boxplots\nA scatterplot like the one you just created is a great way to visualize the relationship between two numeric (or metric) variables. But there may be situations where only one of your variables is numeric and the other is categorical or ordinal. In that case, a scatterplot will not be helpful.\nBut you can visualize the relationship between a categorical and a numeric variable with a boxplot (or “box-whisker plot”). You should remember that a boxplot is a type of graph that allows you to visualize the distribution of a numeric variable (see Kellstedt & Whitten 2018, 135). A boxplot shows you the median (or “middle value”) but also the 1st and 3rd quartile and any outliers in the data.\nBy drawing boxplots of a numeric variable (such as age) over the groups of a categorical variable (such as gender), you can visualize any relationships between these variables.\n\nFor example, let’s assume you are interested in the relationship between gender (a categorical variable) and body height (a numeric variable): Are men typically taller than women?\nThe code below shows how you can visualize this relationship using a boxplot:\n\nggplot(data = ess,\n       aes(x = gndr, y = height)) +\n    geom_boxplot()\n\n\n\n\n\n\n\n\nThis piece of code should now not be difficult to understand:\n\nAs before, you need to specify a variable for the x- and one for the y-axis (just like in the scatterplot above). Here, you put gndr on the x-axis and height on the y-axis;\nThen just you let R draw the boxplots with geom_boxplot();\n\n\nWith some more polishing, you get a neat and informative graph:\n\nggplot(data = ess,\n       aes(x = gndr, y = height)) +\n    geom_boxplot() +\n    labs(x = \"Gender\",\n         y = \"Body height (cm)\") +\n    theme_bw()\n\n\n\n\n\n\n\n\nWhat do you learn from the graph about the differences in body height between men and women (see Kellstedt & Whitten 2018, 135 for how to read a boxplot)?",
    "crumbs": [
      "Tutorial 4: Data visualization with `ggplot2`"
    ]
  },
  {
    "objectID": "tutorial_4.html#univariate-graphs-histograms",
    "href": "tutorial_4.html#univariate-graphs-histograms",
    "title": "Tutorial 4: Data visualization with ggplot2",
    "section": "3.3 Univariate graphs: Histograms",
    "text": "3.3 Univariate graphs: Histograms\nThe graphs you created so far were types of bivariate graphs: They show the relationship between two different variables.\nYou can of course also create graphs that describe single variables: univariate graphs such as histograms or bar graphs.\n\n\n3.3.1 Drawing a histogram\nA histogram is a type of graph that is very useful to visualize the distribution of a single continuous or metric variable.\nSince you have seen earlier how a ggplot-graph is built, you should now be able to look at the code below and understand how you can produce a histogram:\n\nggplot(data = ess,\n       aes(x = weight)) +\n        geom_histogram()\n\n\n\n\n\n\n\n\nMuch of the code is still the same (data = ess) as above, but there are some differences:\n\nOnly one aesthetic (aes()) is defined: x = weight. This is because the graph is univariate, it shows only the distribution of a single variable;\nThe geometric object is now a histogram (geom_histogram());\n\n\n\n\n3.3.2 Improving appearance\nThe graph on the previous page is quite dull and, more importantly, the individual “bins” are difficult to distinguish from each other. This is easy to fix, however.\nAll we need to do is to change the color of the bins’ outlines with the color argument (“option”). White is a good option here since it contrasts with the dark gray bins but does not add extra complexity to the graph:\n\nggplot(data = ess,\n       aes(x = weight)) +\n        geom_histogram(color = \"white\")\n\n\n\n\n\n\n\n\n\n\n\n3.3.3 Changing the number of “bins”\nAs you know from Kellstedt & Whitten (2018, 138–39), the number of “blocks” or “bins” into which we group the data is an important choice that can really affect our impression of the variable we look at.\nBy default, geom_histogram() uses 30 bins, but this might obviously not be ideal in all situations. Fortunately, you can change the number of bins easily with the bins option.4\nFor example, we could set the number of bins to 20 instead of 30:\n\nggplot(data = ess,\n       aes(x = weight)) +\n        geom_histogram(color = \"white\", bins = 20)\n\n\n\n\n\n\n\n\nFeel free to play around with different numbers and see how the graph changes!\n\n\n\n3.3.4 Final polishing\nFinally, the histogram can also still be improved by changing the background theme and by adding axis labels:\n\nggplot(data = ess,\n       aes(x = weight)) +\n        geom_histogram(color = \"white\", bins = 30) +\n        labs(x = \"Body height (cm)\",\n             y = \"Number of observations\") +\n        theme_bw()\n\n\n\n\n\n\n\n\nCan you see what was changed?\n\nThe graph now has informative labels and a simple black and white theme;\nThe number of bins is changed back to 30.\n\nAs before, there are many more ways to make this graph prettier, but this is good enough for now.",
    "crumbs": [
      "Tutorial 4: Data visualization with `ggplot2`"
    ]
  },
  {
    "objectID": "tutorial_4.html#univariate-graphs-bar-graphs",
    "href": "tutorial_4.html#univariate-graphs-bar-graphs",
    "title": "Tutorial 4: Data visualization with ggplot2",
    "section": "3.4 Univariate graphs: Bar graphs",
    "text": "3.4 Univariate graphs: Bar graphs\nNext to histograms, bar graphs are another type of univariate graph to show the distribution of a single variable. Bar graphs are suited for categorical or ordinal variables — variables with a few distinct categories.\nThe code below produces a bar graph of the gender variable (gndr) — and it shows the number of men and women in the dataset:\n\nggplot(data = ess,\n       aes(x = gndr)) +\n        geom_bar()\n\n\n\n\n\n\n\n\nYou should be able to understand most of the code by now, but the geom_bar() part might require a bit of explanation.\n\nObviously, to produce a bar graph, you would use the geom_bar() layer;\nBy default, geom_bar() shows the number of observations (‘counts’) in each of the two groups. One could also be more explicit and specify geom_bar(stat = \"count\") to tell R very clearly that it is supposed to show the number of observations (‘counts’) in each category of the gender variable. To see what else you can do with geom_bar(), use the help file (?geom_bar).\n\n\nAnd, once more, there are ways to make this graph ready for publication:\n\nggplot(data = ess,\n       aes(x = gndr)) + \n        geom_bar(stat = \"count\") +\n        labs(x = \"Gender\",\n             y = \"Number of observations\") +\n        theme_bw()",
    "crumbs": [
      "Tutorial 4: Data visualization with `ggplot2`"
    ]
  },
  {
    "objectID": "tutorial_4.html#calculating-summary-statistics",
    "href": "tutorial_4.html#calculating-summary-statistics",
    "title": "Tutorial 4: Data visualization with ggplot2",
    "section": "4.1 Calculating summary statistics",
    "text": "4.1 Calculating summary statistics\nYou may remember that you can use group_by() and summarize() to get summary statistics such as the average over groups of a categorical variable, but just to refresh your memory here is how you calculate the average body height for men and women in the ess dataset:\n\ness %&gt;% # 1.\n  group_by(gndr) %&gt;% # 2.\n  summarize(avg_height = mean(height, na.rm = TRUE)) # 3.\n## # A tibble: 2 × 2\n##   gndr   avg_height\n##   &lt;fct&gt;       &lt;dbl&gt;\n## 1 Male         179.\n## 2 Female       168.\n\nTo put this into human language, you tell R to:\n\n“Take the ess dataset,…”\n“…group the data by gender (gndr),…”\n“…calculate the average body height for each gender, and store the result as a new variable (called avg_height).”\n\nIf you look carefully at the output, you can see that the result of the calculation is a so-called tibble. A tibble is a type of data.frame-object in R — i.e., a dataset.5 This means that the result you are looking at is, basically, a mini dataset that includes two observations (Male and Female) and two variables (gndr and avg_height). avg_height is here the average body height for men and women as identified by gndr. Important: Because the result is essentially a dataset, you can directly visualize it with ggplot!",
    "crumbs": [
      "Tutorial 4: Data visualization with `ggplot2`"
    ]
  },
  {
    "objectID": "tutorial_4.html#visualizing-the-result",
    "href": "tutorial_4.html#visualizing-the-result",
    "title": "Tutorial 4: Data visualization with ggplot2",
    "section": "4.2 Visualizing the result",
    "text": "4.2 Visualizing the result\nNow that you have the numbers you want to show in your graph, all you need to do is to use the %&gt;% operator to directly feed the numbers – the ‘mini dataset’ – into a graph:\n\ness %&gt;% \n  group_by(gndr) %&gt;% \n  summarize(avg_height = mean(height, na.rm = TRUE)) %&gt;% # here is the link!\n  ggplot(aes(x = gndr, y = avg_height)) +\n    geom_bar(stat = \"identity\")\n\n\n\n\n\n\n\n\nYou see that the first part is just the code for the calculation from above: group_by(), then summarize(). We then add another pipe at the end to feed the result directly into a ggplot graph. In this graph, gndr is on the x-axis and the y-axis shows the average height for each gender. You then visualize the height per gender with a simple bar graph using geom_bar().\nOne thing is worth noticing: In this case, we need to tell R that geom_bar() should only show the pure numbers that we got from the previous step (the content of the new avg_height variable in the mini dataset), and not the number of observations or other statistics. We do this by specifying stat = \"identity\" within geom_bar().\nLinking data cleaning and visualization can be very handy when want to visualize specific summary statistics (the average, the standard deviation,…), but especially when you want to create some more complicated graphs such as grouped bar graphs showing percentages.6",
    "crumbs": [
      "Tutorial 4: Data visualization with `ggplot2`"
    ]
  },
  {
    "objectID": "tutorial_4.html#footnotes",
    "href": "tutorial_4.html#footnotes",
    "title": "Tutorial 4: Data visualization with ggplot2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOthers are the basic graphing tools that are built into R or the lattice package.↩︎\nSee https://youtu.be/jbkSRLYSojo↩︎\nIt is also generally not ideal to have a gray background because it may make it unnecessarily difficult to read finer details in more complex graphs (see Schneider & Jacoby 2018, “Graphical displays for public opinion research”, in Atkinson & Alvarez, The Oxford Handbook of Polling and Survey Methods, pages 439–479. Oxford: Oxford University Press.).↩︎\nAlternatively, you can also change how wide the bins are with the binwidth option.↩︎\nSee https://tibble.tidyverse.org/ in case you want to learn more.↩︎\nSee for example here: https://sebastiansauer.github.io/percentage_plot_ggplot2_V2/.↩︎",
    "crumbs": [
      "Tutorial 4: Data visualization with `ggplot2`"
    ]
  },
  {
    "objectID": "tutorial_1.html",
    "href": "tutorial_1.html",
    "title": "Tutorial 1: Your first steps in R",
    "section": "",
    "text": "In the previous tutorial, you have learned how you can install R and RStudio and how you install add-on packages.\nBut even if you managed to get everything to work, it might still have felt strange to work with computer code.\nThe aim of this tutorial is to help you get familiar with R and more used to writing code. This means that you will do a few simply warm-up exercises and learn about the main logic of R and R code.\n\n\n\n\n\n\nTip\n\n\n\nHvis du ønsker å lese en norsk tekst i tillegg: “Lær deg R”, Kapittel 2 & 4.1.1",
    "crumbs": [
      "Tutorial 1: Your first steps in `R`"
    ]
  },
  {
    "objectID": "tutorial_1.html#calculating-with-r",
    "href": "tutorial_1.html#calculating-with-r",
    "title": "Tutorial 1: Your first steps in R",
    "section": "2.1 Calculating with R",
    "text": "2.1 Calculating with R\nOne thing that can help you get familiar with R is to simply use it as a calculator — which you can do easily because, in essence, R is just a very fancy, very powerful pocket calculator.\nTo do that, click into the Console window so that a small vertical blinking line (“cursor”) appears behind the &gt; symbol. Then type 1 + 1 and hit the Enter-key.\nThe result should look like this:\n\n1+1\n## [1] 2\n\nYou can of course also do much more complicated calculations such as:\n\n(12*78)/(0.5-7000)+42.541-8*98\n## [1] -741.5927\n\nYou can do all common mathematical operations that you know from (high) school in R:\n\n+ adds two numbers\n- subtracts one number from another\n* multiplies two numbers\n/ divides one number by another\n( and ) are parentheses (to separate out a specific part of the calculation)\n^ means “raise to the power of” — 2^2 means 2*2 (or “two squared”), 2^3 means 2*2*2, and so on",
    "crumbs": [
      "Tutorial 1: Your first steps in `R`"
    ]
  },
  {
    "objectID": "tutorial_1.html#working-with-text-and-your-first-error-message",
    "href": "tutorial_1.html#working-with-text-and-your-first-error-message",
    "title": "Tutorial 1: Your first steps in R",
    "section": "2.2 Working with text (and your first error message)",
    "text": "2.2 Working with text (and your first error message)\nClearly, R can handle numbers quite well — which is not surprising, given that R is a statistical programming language. But R can also handle text, as long as the text is entered correctly.\nTo see what this means, let’s have a look at what happens when you handle text incorrectly: type the following into the Console and hit “Enter”:\n\nHello world!\n\nYou should now receive your first error message (the first of many…sorry!): Error: unexpected symbol in \"Hello world\"\nHere is what happened: When you enter letters or words into the R Console, R thinks that you are entering a command (“function”). It then looks into its internal library of commands and, if it does not find a command that corresponds to the word you entered, it returns an error message. Unexpected symbol basically means “I don’t understand what you want from me.”\nYou can tell R to treat something as text and not as a command by placing it within quotation marks. See what happens when you enter the following into your Console:\n\n\"Hello world!\"",
    "crumbs": [
      "Tutorial 1: Your first steps in `R`"
    ]
  },
  {
    "objectID": "tutorial_1.html#the-assignment-operator--",
    "href": "tutorial_1.html#the-assignment-operator--",
    "title": "Tutorial 1: Your first steps in R",
    "section": "3.1 The assignment operator (<-)",
    "text": "3.1 The assignment operator (&lt;-)\nTo save something in an object, you use the assignment operator: &lt;- (the “smaller than” sign, &lt;, plus a dash, -)\nFor example, x &lt;- 5 tells R to “store 5 into the object x” (or, put differently, “assign the value of 5 to x”).\nSee for yourself what happens when you type the following into your Console and hit enter:\n\nx &lt;- 5\n\nYou will now not see any output in the Console — instead, you should see an item appear in your work environment (on the upper right side of your screen). You see that the value of 5 has indeed been stored as the x object.\nTry storing another number as y, for example:\n\ny &lt;- 7\n\nAs mentioned before, you can also store text. Run the following in your Console:\n\nz &lt;- \"statistics is pointless\"\n\nYou should now see the z object containing the phrase “statistics is pointless” appear in your work environment.",
    "crumbs": [
      "Tutorial 1: Your first steps in `R`"
    ]
  },
  {
    "objectID": "tutorial_1.html#shortcutting-the-assignment-operator",
    "href": "tutorial_1.html#shortcutting-the-assignment-operator",
    "title": "Tutorial 1: Your first steps in R",
    "section": "3.2 Shortcutting the assignment operator",
    "text": "3.2 Shortcutting the assignment operator\nWhen working with R, you will use the assignment operator a lot. Basically all the time.\nIt therefore makes sense to add a keyboard shortcut specifically for the assignment operator so that you do not have to twist your fingers into a knot every time you want to store something in an object.\nThis is easy to do in RStudio: Go to “Tools” in the taskbar at the top and choose “Modify keyboard shortcuts”; use the little search/filter field to search for “assignment”. The entry “Insert assignment operator” should appear. Use the existing shortcut, or change it to one that suits you.",
    "crumbs": [
      "Tutorial 1: Your first steps in `R`"
    ]
  },
  {
    "objectID": "tutorial_1.html#using-stored-objects",
    "href": "tutorial_1.html#using-stored-objects",
    "title": "Tutorial 1: Your first steps in R",
    "section": "3.3 Using stored objects",
    "text": "3.3 Using stored objects\nWhat if you want to re-use the objects you stored? Easy. You just call them up by typing their name into the Console.\nFor example, to retrieve the number you stored in the x object, you just type x into the Console and hit “Enter”.\nTry retrieving all the stored values, x, y, and z (one at a time!).\nYou can also use these stored items, for example in calculations. Try it out, type the following into your Console and hit “Enter”:\n\nx + y\n\nObviously, mathematical operations only make sense if you work with stored numbers. This does not work with text. (Feel free to try it anyways! See what happens when you run y + z.)",
    "crumbs": [
      "Tutorial 1: Your first steps in `R`"
    ]
  },
  {
    "objectID": "tutorial_1.html#storing-multiple-numbers-or-words",
    "href": "tutorial_1.html#storing-multiple-numbers-or-words",
    "title": "Tutorial 1: Your first steps in R",
    "section": "3.4 Storing multiple numbers or words",
    "text": "3.4 Storing multiple numbers or words\nNow comes a very important step: We are moving from storing single pieces of information (one “datum”) to storing multiple pieces of information (“data”).\nTo store multiple items together you use a very important command or “function”: c()\nc() stands for “combine” — you use it to combine pieces of information (“data points”) in a single object. The c() function is easy to use. You just add the things you want to store within the parentheses, separated by commas.\nTo see how it works, run the following code to store the sequence of the numbers 1 through 4 into the object a (or, “assign” them to the object a)\n\na &lt;- c(1,2,3,4)\n\nYou can also store a sequence of words as long as you put them into quotation marks and separate them with commas. Give it a try by running the following code:\n\nb &lt;- c(\"statistics\",\"is\",\"pointless\")",
    "crumbs": [
      "Tutorial 1: Your first steps in `R`"
    ]
  },
  {
    "objectID": "tutorial_1.html#on-names-for-r-objects",
    "href": "tutorial_1.html#on-names-for-r-objects",
    "title": "Tutorial 1: Your first steps in R",
    "section": "3.5 On names for R objects",
    "text": "3.5 On names for R objects\nYou have just learned about objects — the little containers in which you can store different values. When working in R, you will usually work with many different objects: Your main dataset will be one object in your Environment, but you may also be working with more than one dataset at a time or have other things stored in different objects. This means it is important that you always know what each object is — you have to give them useful and informative names!\nYou can choose essentially freely how to name your objects — just like you can freely choose how you name any Microsoft Word document you are working on. For example, you could name a Word Document “BST290_CoursePaper_draft15.docx” or “CoursePaper_BST290_draft23.docx”, or “StupidStatsCourse_Paper_almostdone_Version56.docx”, or any other name you want. The same applies to your R objects: You can name them, in principle, in any way you like.\nBut: Do make sure you always pick names that are informative so you don’t get confused! For example, once you work with a full dataset, you should give it a meaningful name like my_dataset and not just x or y! You should also avoid naming objects like important commands — so, don’t call your dataset object mean because there is a command in R to calculate the mean (“average”) that is called mean(). You obviously don’t want any confusion there…",
    "crumbs": [
      "Tutorial 1: Your first steps in `R`"
    ]
  },
  {
    "objectID": "tutorial_1.html#briefly-on-commands-functions",
    "href": "tutorial_1.html#briefly-on-commands-functions",
    "title": "Tutorial 1: Your first steps in R",
    "section": "3.6 Briefly on commands (“functions”)",
    "text": "3.6 Briefly on commands (“functions”)\nA bit earlier, you used the c()-command or c()-function to combine sets of items into an object.\nFunctions are a central component of the R language. You can think of functions as the R language’s verbs — the “do stuff” words. You can tell that something is a function if it has parentheses: ().\nFor example,\n\nc() is the command to tell R: “combine the following items together”\nmean() is the command to tell R to calculate the mean (“average”) of a set of numbers\ndownload.file() is the command to tell R — you guessed it — to download a file from the internet\nrm() is the command to tell R to remove (“delete”) an object from the work environment\n\nFunctions work by the following principle:\n\nThe name of the function — the text before the parentheses (mean, download.file,…) — specifies what operation you want to be done.\nWithin the parentheses, you specify the object(s) that the operation should be done with (and, if available, any options). In the official R lingo, the content within the parentheses is called arguments.\n\nLet’s look at two functions in action (run these in your Console, one at a time):\n\nbodyheight_data &lt;- c(187,156,198,183,175,171)\n\nmean(bodyheight_data)\n\n[1] 178.3333\n\n\nHere, you use two different functions:\n\nYou first use c() to store a collection of values, measurements of the body heights of a fictional group of people, as bodyheight_data.\nThen you let R calculate the average body height in this group using the mean() function.\n\nThere are obviously many, many, many more functions in the R language. No reason to worry, you will learn them one at a time.",
    "crumbs": [
      "Tutorial 1: Your first steps in `R`"
    ]
  },
  {
    "objectID": "tutorial_1.html#from-single-commands-to-script-files",
    "href": "tutorial_1.html#from-single-commands-to-script-files",
    "title": "Tutorial 1: Your first steps in R",
    "section": "5.1 From single commands to script files",
    "text": "5.1 From single commands to script files\nSo far, you have just entered a few individual functions into the R Console and then looked at the results. This is fine for a very first introduction, but later on — when things get more serious — you will write many more and also more complex functions and you will want to run them directly after each other. Also, you will usually work on any single data analysis project for a longer period of time. For example, you might do all your data management and cleaning on one day and then do the actual data analysis on the next day. In that case, you obviously do not want to have to re-enter all the code from the first day when you start again on the following day.\nThis means you need some place where you can write longer sequences of code and then save everything so you do not have to start from the beginning every time you need to interrupt your work. This is why you use script files.\nScript files contain all the code that you write for a specific data analysis project.",
    "crumbs": [
      "Tutorial 1: Your first steps in `R`"
    ]
  },
  {
    "objectID": "tutorial_1.html#writing-and-running-a-script-file",
    "href": "tutorial_1.html#writing-and-running-a-script-file",
    "title": "Tutorial 1: Your first steps in R",
    "section": "5.2 Writing and running a script file",
    "text": "5.2 Writing and running a script file\nTo make this more concrete, let’s open a script file on your own computer: In the taskbar at the top of your screen, go to “File” -&gt; “New File” -&gt; “R Script” (alternatively, press “Ctrl+Shift+N” on Windows or “Command+Shift+N” on Mac).\nThe left-hand side of your screen should now split in two: The Console moves to the bottom half and an entirely empty file with a blinking cursor appears in the top half. The new file at the top is your script file.\nFirst, save the file (“File” -&gt; “Save as”) and give it an informative name (e.g., r_tutorial1.R).\nThen write the following code to generate two new vectors into your script file:\n\nbodyweight_data &lt;- c(85.3,75.6,114.2,56.9)\n\nfruit &lt;- c(\"banana\",\"apple\",\"orange\",\"cherry\")\n\nNow you have some code (which should like in the image below) — but you may wonder how you run it? You could of course copy-paste it line by line into the Console below, but that would be annoying.\nInstead, select the code either with your mouse or with the Shift + arrow keys — just like you would select text in a Word document. The result should look like in the screenshot below:\n\n\n\nSelected code in a script file\n\n\nTo run this code, move your mouse to the top right corner of the script file (not the entire window!) — there is a little button with a white square and green arrow that says “Run”. Click on that button to run your code.\nYou will see that the commands you selected to run get “transferred” to the Console and then executed. This is the normal way to work in R: You write (and save!) code in your scriptfile, and you also run it from there. The Console is only used for small, less important things (e.g., when you quickly want to look at a stored object or want to try out something).\nYou can also create a keyboard shortcut to run code. This is strongly recommended because it will make your life easier. To find out what the current shortcut to run selected code is, navigate again to the relevant menu (“Tools” -&gt; “Modify Keyboard Shortcuts”; search for “Run current line or selection”; use the existing shortcut or modify it as desired).",
    "crumbs": [
      "Tutorial 1: Your first steps in `R`"
    ]
  },
  {
    "objectID": "tutorial_1.html#comments-in-a-script-file",
    "href": "tutorial_1.html#comments-in-a-script-file",
    "title": "Tutorial 1: Your first steps in R",
    "section": "5.3 Comments in a script file",
    "text": "5.3 Comments in a script file\nSometimes you might want to add notes or comments to your code, for example to explain to others (or your future self) what a particularly complicated piece of code does. Obviously, you then need to tell R to ignore these comments or you will get an error message.\nYou can add comments in a script file with the hash symbol (#). R will then ignore everything that comes after #. For example:\n\n# mean(bodyweight_data) &lt;- R ignores all of this\n\n# Some comment &lt;- R ignores this too\n\nmean(bodyweight_data) # &lt;- this code is run, but everything after the hash is ignored",
    "crumbs": [
      "Tutorial 1: Your first steps in `R`"
    ]
  },
  {
    "objectID": "tutorial_1.html#keeping-your-code-neat-and-tidy",
    "href": "tutorial_1.html#keeping-your-code-neat-and-tidy",
    "title": "Tutorial 1: Your first steps in R",
    "section": "5.4 Keeping your code neat and tidy",
    "text": "5.4 Keeping your code neat and tidy\nIt is important that you keep your script files organized and easy to read — especially for yourself. You should be able to open the scriptfile for an analysis that you did two years ago and understand, at least after a bit of reading, what you did and how you did it. This means:\n\nUse comments to provide a title for each file plus relevant information and explanations for each bit of code\nLoosen it up: Add empty lines between code ‘chunks’ that belong together\n\nFor example, this is how you could organize a simple scriptfile (this code will not make sense to you now, but you should be able to understand most of it by the end of this course):\n\n#########################\n# Example analysis script\n#########################\n\n# Carlo Knotz (March 17, 2022)\n\nlibrary(tidyverse) # add-on package for data management & visualization\n\n# Data management\n#################\n\n# Downloading data\ncpds &lt;- haven::read_dta(\"https://www.cpds-data.org/images/Update2021/CPDS_1960-2019_Update_2021.dta\")\n\n# Data cleaning (selecting relevant variables, changing name of one)\ncpds %&gt;%\n  select(country,year,openc,realgdpgr,debt_hist,gov_left1,gov_right1) %&gt;%\n  rename(growth = realgdpgr) -&gt; cpds\n\n\n# Descriptive graph\n###################\n\n# Average growth rate, by country\ncpds %&gt;%\n  group_by(country) %&gt;%\n  summarise(growth = mean(growth, na.rm = T)) %&gt;%\n  ggplot(aes(y = reorder(country,growth), x = growth)) +\n    geom_bar(stat = \"identity\") +\n    labs(x = \"Average economic growth rate (%)\",\n         y = \"\") +\n    theme_bw()\n\n# ...and so on...",
    "crumbs": [
      "Tutorial 1: Your first steps in `R`"
    ]
  },
  {
    "objectID": "tutorial_1.html#vectors-variables",
    "href": "tutorial_1.html#vectors-variables",
    "title": "Tutorial 1: Your first steps in R",
    "section": "6.1 Vectors & variables",
    "text": "6.1 Vectors & variables\nYou have previously learned that R stores collections of data points as vectors. But you may also remember from the lecture and Kellstedt & Whitten that we, as social scientists, have also another name for collections of data points: variables.\nMeasurements of a variable are really nothing else than collections of data points (usually numbers, but sometimes also text). For example, the (fictional) set of body height measurements we created earlier could be measurements of a variable (“body height”) that comes from a research project.\n\nbodyheight_data\n## [1] 187 156 198 183 175 171\n\n\nThe important point to remember: In R, variables are represented as vectors — as collections of data points.",
    "crumbs": [
      "Tutorial 1: Your first steps in `R`"
    ]
  },
  {
    "objectID": "tutorial_1.html#datasets",
    "href": "tutorial_1.html#datasets",
    "title": "Tutorial 1: Your first steps in R",
    "section": "6.2 Datasets",
    "text": "6.2 Datasets\nWhen you do a statistical analysis, you usually work with more than one variable (i.e., vector) because you usually want to know how one or more variables are related to another variable (as explained in Chapter 1 in Kellstedt & Whitten). To be able to work with two or more variables at the same time, they need to be “tied together” — and this is, in essence, what a dataset is: Several variables/vectors tied together.\nTo see what a dataset in R looks like, you can open one of the many “toy” datasets that are built into R: the “Violent Crime Rates by US State” (USArrests) dataset.1\nTo view this dataset, you just have to type USArrests into the Console and hit Enter. The result should look like this:\n\nUSArrests\n##                Murder Assault UrbanPop Rape\n## Alabama          13.2     236       58 21.2\n## Alaska           10.0     263       48 44.5\n## Arizona           8.1     294       80 31.0\n## Arkansas          8.8     190       50 19.5\n## California        9.0     276       91 40.6\n## Colorado          7.9     204       78 38.7\n## Connecticut       3.3     110       77 11.1\n## Delaware          5.9     238       72 15.8\n## Florida          15.4     335       80 31.9\n## Georgia          17.4     211       60 25.8\n## Hawaii            5.3      46       83 20.2\n## Idaho             2.6     120       54 14.2\n## Illinois         10.4     249       83 24.0\n## Indiana           7.2     113       65 21.0\n## Iowa              2.2      56       57 11.3\n## Kansas            6.0     115       66 18.0\n## Kentucky          9.7     109       52 16.3\n## Louisiana        15.4     249       66 22.2\n## Maine             2.1      83       51  7.8\n## Maryland         11.3     300       67 27.8\n## Massachusetts     4.4     149       85 16.3\n## Michigan         12.1     255       74 35.1\n## Minnesota         2.7      72       66 14.9\n## Mississippi      16.1     259       44 17.1\n## Missouri          9.0     178       70 28.2\n## Montana           6.0     109       53 16.4\n## Nebraska          4.3     102       62 16.5\n## Nevada           12.2     252       81 46.0\n## New Hampshire     2.1      57       56  9.5\n## New Jersey        7.4     159       89 18.8\n## New Mexico       11.4     285       70 32.1\n## New York         11.1     254       86 26.1\n## North Carolina   13.0     337       45 16.1\n## North Dakota      0.8      45       44  7.3\n## Ohio              7.3     120       75 21.4\n## Oklahoma          6.6     151       68 20.0\n## Oregon            4.9     159       67 29.3\n## Pennsylvania      6.3     106       72 14.9\n## Rhode Island      3.4     174       87  8.3\n## South Carolina   14.4     279       48 22.5\n## South Dakota      3.8      86       45 12.8\n## Tennessee        13.2     188       59 26.9\n## Texas            12.7     201       80 25.5\n## Utah              3.2     120       80 22.9\n## Vermont           2.2      48       32 11.2\n## Virginia          8.5     156       63 20.7\n## Washington        4.0     145       73 26.2\n## West Virginia     5.7      81       39  9.3\n## Wisconsin         2.6      53       66 10.8\n## Wyoming           6.8     161       60 15.6\n\nYou see that the dataset contains statistics about arrests for violent crimes in all U.S. States (in 1973).2\nNote that:\n\nEach row is a state (e.g., Alabama, Alaska, etc.);\nEach column is a variable that records the arrest rate for a given crime and the share of the population living in big cities (UrbanPop).\n\nImportantly, each of the variables by itself is just a sequence of numbers — or a vector — but, together, they form a dataset. And this is also how a dataset should ideally be organized: Each row is an observation, each column is a variable. A dataset that is organized like this is called a tidy dataset.3 In R, datasets are stored as so-called data.frames.4\nThe datasets you will work with during this course will be organized like this — but, in real life, datasets are often messy and need to be reorganized before you can really work with them. This is why this course includes a part on Data Management.",
    "crumbs": [
      "Tutorial 1: Your first steps in `R`"
    ]
  },
  {
    "objectID": "tutorial_1.html#creating-a-data.frame-yourself",
    "href": "tutorial_1.html#creating-a-data.frame-yourself",
    "title": "Tutorial 1: Your first steps in R",
    "section": "6.3 Creating a data.frame yourself",
    "text": "6.3 Creating a data.frame yourself\nWhen doing a data analysis in R, you will normally work with large to very large datasets that you import in one go — typing them in by in is obviously not an option then.\nBut you can create datasets by yourself, and doing so is a good way to understand better how to work with this type of object.\nYou can create your own data.frame with the data.frame() function. When you use this function, you go variable by variable – or column by column: You first type the name and value of one column, and then repeat that for all other columns.\nFor example, let’s assume you had collected a tiny dataset about some local students at UiS. This dataset has four observations (i.e., four rows) and three variables (columns). The rows are different four different students, and the variables include name, age, and hometown. The values are as shown below and we save the dataset as studata.\nThis is how you create a dataset of these four students (you should write this in your new scriptfile):\n\nstudata &lt;- data.frame(name = c(\"Janne\",\"Tore\",\"Siri\",\"Ola\"),\n                      age = c(23,21,19,22),\n                      hometown = c(\"Stavanger\",\"Oslo\",\"Tromso\",\"Bergen\"))\n\nWhen you take a closer look at the code, you see that each variable name (name, age,…) is followed by a set of values that together form the variables. Also, notice that values that are text (the names and hometowns) are put in quotation marks but not those that are true numbers (the students’ ages).\nWhen you now run this code (as described earlier), the studata object should appear under Data in the Work Environment. If you then click on the little blue circle with the white triangle in it (next to studata), you will also get an overview over the contents of the dataset: The variables and what types they are.\nYou can also see the structure of your new dataset more clearly if you print it out in the Console. To do that, you simply type the name of the stored dataset into the Console and press “Enter”:\n\nstudata\n##    name age  hometown\n## 1 Janne  23 Stavanger\n## 2  Tore  21      Oslo\n## 3  Siri  19    Tromso\n## 4   Ola  22    Bergen\n\nYou can directly see the structure of the dataset: Observations (students) in rows, variables (name, age,…) in columns.",
    "crumbs": [
      "Tutorial 1: Your first steps in `R`"
    ]
  },
  {
    "objectID": "tutorial_1.html#footnotes",
    "href": "tutorial_1.html#footnotes",
    "title": "Tutorial 1: Your first steps in R",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTo see what other “toy” datasets you have available, run data() in your Console.↩︎\nRun ?USArrests in the Console to open the documentation of the dataset, which will give you more detailed information.↩︎\nSee also Wickham, H. (2014). Tidy data. Journal of Statistical Software, 59(10):1–23.↩︎\nThere are also other types of dataset-objects such as the tibble (https://tibble.tidyverse.org/), but they all look and work more or less the same way.↩︎",
    "crumbs": [
      "Tutorial 1: Your first steps in `R`"
    ]
  },
  {
    "objectID": "tutorial_2.html",
    "href": "tutorial_2.html",
    "title": "Tutorial 2: Importing & exploring your data",
    "section": "",
    "text": "You have so far learned how to install and set up R and RStudio, how you can install and load packages, how data look like in R, and how you write and use code. All of this was essentially a warm-up.\nNow things get a bit more real: This week, you will learn how to open a real research dataset and how to explore it in R.\nBut we will take this one step at a time: You will first learn about data exploration with a small dataset that is already installed on your computer. Then you will import a real dataset (from the European Social Survey). This is to prepare you for the in-class exercises, where you will apply the data-exploration techniques you learned in the tutorial to the full-scale ESS dataset.\nImportant: As before, document your code in a dedicated scriptfile as you work your way through the tutorial – do not rely on the Console (unless you are just installing packages or quickly trying things out).\n\n\n\n\n\n\nTip\n\n\n\nHvis du ønsker å lese en norsk tekst i tillegg: “Lær deg R”, Kapittel 4",
    "crumbs": [
      "Tutorial 2: Importing & exploring your data"
    ]
  },
  {
    "objectID": "tutorial_2.html#your-project-folder",
    "href": "tutorial_2.html#your-project-folder",
    "title": "Tutorial 2: Importing & exploring your data",
    "section": "2.1 Your project folder",
    "text": "2.1 Your project folder\nThe first thing you need to do is to make sure that you are working in the Project (and the associated folder) that you created in the first seminar/lab in Week 1 of the course.1\nLook at the upper-right corner of the RStudio window and check that your project is active. It should not say: “Project: (None)”. Instead, you should see the name of the project you created. (If you do see “Project: (None)” written there, you can click on it to open a drop-down menu in which your project should be listed. You can open it there.)\nOnce you are done with make sure that you know where on your computer your project folder is; navigate there in the Windows File Explorer/Mac Finder.",
    "crumbs": [
      "Tutorial 2: Importing & exploring your data"
    ]
  },
  {
    "objectID": "tutorial_2.html#loading-the-bst290-package-and-the-practice-dataset",
    "href": "tutorial_2.html#loading-the-bst290-package-and-the-practice-dataset",
    "title": "Tutorial 2: Importing & exploring your data",
    "section": "2.2 Loading the bst290 package and the practice dataset",
    "text": "2.2 Loading the bst290 package and the practice dataset\nYou will remember that you installed a number of packages previously, one of which was the bst290 package. This package includes, among other things, a small practice dataset that you will use in this and the other tutorials to get familiar with the various operations in R before you move on to the “real-deal” research datasets.\nThe practice dataset in the bst290 package is a fragment of the European Social Survey data that were collected in Norway in 2014. In essence, this practice dataset is a mini-version of the full ESS dataset. Where the full ESS includes data for more than 1000 survey participants and hundreds of variables, the practice dataset includes only data for 143 Norwegian respondents and 22 variables.\nTo access the data, you first need to load the bst290 package with the library() function:\n\nlibrary(bst290)\n\nThen you can open the dataset (which is called ess) with the data() function:\n\ndata(ess)\n\nIf everything worked, then you should now see the ess dataset listed in the Environment panel (upper right of your screen). You will probably see &lt;Promise&gt; written where the dataset summary and the variables should appear — and you can take this literally: R promises you that the dataset will appear once you start using it. So, all you need to do is to call up the dataset in some way, for example by simply typing ess into the Console.\nOnce the dataset is properly loaded, you should see in the Environment panel that the dataset includes 143 observations and 22 variables.\nYou can also get the dataset directly with the “double-colon” method:\n\ness &lt;- bst290::ess\n\nTranslated into human language, this tells R to “get the ess dataset from the bst290 package and save it under the name ess in the Environment”.",
    "crumbs": [
      "Tutorial 2: Importing & exploring your data"
    ]
  },
  {
    "objectID": "tutorial_2.html#a-first-glimpse",
    "href": "tutorial_2.html#a-first-glimpse",
    "title": "Tutorial 2: Importing & exploring your data",
    "section": "3.1 A first glimpse",
    "text": "3.1 A first glimpse\nTake a look at the ess object in the Environment tab — can you see the tiny blue circle with the white triangle/arrow inside it that is directly to the left of ess?\nIf you click on it, you can get more information about the different variables that are included in the dataset.\n\nYou should now see a list of variable names (name, essround, idno,…). Each of these variables is a collection of data points — and therefore stored as a vector in R (you may remember from the previous tutorial). All these vectors are then combined into the ess dataset (or, in R lingo, data.frame).\nNext to these names, you also see chr or num written — as you probably remember, this tells you what type of information each variable contains.\nYou may also notice that some elements in the list are followed by the phrase Factor w/ XX levels... — these are so called factors and are a particular type of vector. You will learn about them further below.",
    "crumbs": [
      "Tutorial 2: Importing & exploring your data"
    ]
  },
  {
    "objectID": "tutorial_2.html#looking-at-the-data",
    "href": "tutorial_2.html#looking-at-the-data",
    "title": "Tutorial 2: Importing & exploring your data",
    "section": "3.2 Looking at the data",
    "text": "3.2 Looking at the data\nLet’s first get an idea of how the dataset really looks like, which you can do with the View() function. To do that, run the following in your Console:\n\nView(ess)\n\nA new tab should now open and you should see the entire dataset. This should look a bit like Microsoft Excel, a large table with lots of neat and orderly but boring rows and columns of data.",
    "crumbs": [
      "Tutorial 2: Importing & exploring your data"
    ]
  },
  {
    "objectID": "tutorial_2.html#printing-out-the-first-and-last-observations-with-head-and-tail",
    "href": "tutorial_2.html#printing-out-the-first-and-last-observations-with-head-and-tail",
    "title": "Tutorial 2: Importing & exploring your data",
    "section": "3.3 Printing out the first and last observations with head() and tail()",
    "text": "3.3 Printing out the first and last observations with head() and tail()\nLooking at the raw dataset is often quite helpful to get a first idea of what you are working with — but is impractical when you are working with very large datasets.\nAn alternative way to get a first glimpse of your dataset is to use the head() and tail() functions. These show you the first and last six rows (observations) of your dataset — in essence, they print out the top or bottom of the dataset.\n\n3.3.1 Default usage\nUsing them is simple, you just need to specify the name of your dataset within the function. For example, to display the first six observations in the ess dataset, you run:\n\nhead(ess) # This shows you the first 6 observations\n\nThe result should look like this:\n\n##   essround  idno cntry   gndr agea\n## 1        7 12414    NO   Male   22\n## 2        7  9438    NO Female   43\n## 3        7 19782    NO Female   58\n## 4        7 18876    NO Female   22\n## 5        7 20508    NO   Male   84\n## 6        7 19716    NO   Male   62\n##                                                                          edlvdno\n## 1    Fullført 3-4 årig utdanning fra høgskole (Bachelor-, cand.mag., lærerhøgsko\n## 2    Fullført 3-4 årig utdanning fra høgskole (Bachelor-, cand.mag., lærerhøgsko\n## 3                    Fullført 5-6 årig utdanning fra høgskole (master, hovedfag)\n## 4    Fullført 3-4 årig utdanning fra høgskole (Bachelor-, cand.mag., lærerhøgsko\n## 5    Universitet/høgskole, mindre enn 3 år, men minst 2 år (høgskolekandidat, 2-\n## 6 Fullført 5-6 årig utdanning fra universitet (master, hovedfag), lengre profesj\n##   mainact           mbtru        hinctnta                              tvtot\n## 1    &lt;NA&gt;              No H - 10th decile                     No time at all\n## 2    &lt;NA&gt;              No H - 10th decile  More than 1 hour, up to 1,5 hours\n## 3    &lt;NA&gt;  Yes, currently  K - 7th decile More than 2 hours, up to 2,5 hours\n## 4    &lt;NA&gt;              No  J - 1st decile More than 1,5 hours, up to 2 hours\n## 5 Retired Yes, previously            &lt;NA&gt;                     No time at all\n## 6    &lt;NA&gt;  Yes, currently H - 10th decile  More than 1 hour, up to 1,5 hours\n##   ppltrst vote             stflife                    gincdif\n## 1       7  Yes Extremely satisfied Neither agree nor disagree\n## 2       9  Yes                   8 Neither agree nor disagree\n## 3       9  Yes                   7             Agree strongly\n## 4       5   No                   7 Neither agree nor disagree\n## 5       7  Yes                   9 Neither agree nor disagree\n## 6       7  Yes                   8          Disagree strongly\n##                      freehms imwbcnt           happy    health ctzcntr brncntr\n## 1                      Agree       4               9      Good     Yes     Yes\n## 2             Agree strongly       4               9 Very good     Yes     Yes\n## 3             Agree strongly       5               8      Good     Yes     Yes\n## 4                      Agree       5 Extremely happy Very good     Yes     Yes\n## 5 Neither agree nor disagree       5               7 Very good     Yes     Yes\n## 6                      Agree       6               8      Fair     Yes     Yes\n##   height weight\n## 1    175     65\n## 2    175     71\n## 3    150     58\n## 4    173     63\n## 5    167     58\n## 6    174     58\n\n\n\n\n3.3.2 Looking at specific variables\nIf the result above seems pretty cluttered and not very informative: Correct. But there is a solution. You can specify that only the first observations of a single variable are shown when you run head() or tail(). This can help when the dataset contains a larger number of variables and the output therefore becomes cluttered – as was the case here.\nTake another quick look at the Environment window: You might have noticed that there are dollar symbols ($) before each of the variable names in the ess dataset. This is a hint to how you can select single variables from a dataset: With the dollar symbol.\nThe general syntax here is: dataset$variable. For example, to select the age-variable agea from the ess dataset, you would type: ess$agea\nYou can use this with the head() function to let R show you the first six observations of only the agea-variable:\n\nhead(ess$agea)\n## [1] 22 43 58 22 84 62\n\nOf course, you can do this also with any of the other variables — and this works also with many other functions such as tail(), mean(), or summarize(). More follows!\n\n\n\n3.3.3 Definining the number of observations (“rows”)\nYou can also tell R to show you more or fewer observations when you use the head() function. For example, the code below will print out the first 10 observations of the agea variable:\n\nhead(ess$agea, n = 10)\n\nYou can do the same with the tail() function.\n(A final note: As is often the case with R, there is more than one way to subset a dataset, and these allow you to select more than one variables at a time, or a specific set of observations. We will cover some of them in the next tutorial; for others see e.g.: https://www.statmethods.net/management/subset.html.)",
    "crumbs": [
      "Tutorial 2: Importing & exploring your data"
    ]
  },
  {
    "objectID": "tutorial_2.html#a-quick-summary-of-your-data-with-summary",
    "href": "tutorial_2.html#a-quick-summary-of-your-data-with-summary",
    "title": "Tutorial 2: Importing & exploring your data",
    "section": "3.4 A quick summary of your data with summary()",
    "text": "3.4 A quick summary of your data with summary()\nWith View(), head(), or tail(), you can look at the “raw” dataset. This can give you a first idea of what you are working with, but the problem is that you always only see a few data points at a time. Ideally, you would instead get a sense of how the entire dataset or single variables as a whole look like.\nThis is where you would use summary statistics like the mean (“average”), the median, or others (as explained in Kellstedt & Whitten).\nYou can get some important summary statistics with the summary() function.\nThis function is again easy to use: You just specify which object you want summarized within the parantheses. In this case, we use the function on the entire ess dataset:\n\nsummary(ess)\n\nIf you run this, you should get a list of summary statistics for all the variables in the ess dataset. For variables that contain numbers (‘numeric’ variables, or num), you get the minimum, the 1st quartile (a.k.a., the 25th percentile), the median, the mean (‘average’), the 3rd quartile (or 75th percentile) and the maximum. Where variables have missing observations (NA’s), you get these, too.\nFor non-numeric variables (like cntry, for example) you get their ‘length’ (how many observations they contain) and their type or ‘Class’.\nBut, as before, the output is again a bit cluttered (which is also why it is not shown here). It is therefore more useful to get summary statistics for a single variable by using the $ symbol. For example:\n\nsummary(ess$agea)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   16.00   33.50   46.00   47.91   62.00   90.00\n\nHere, Min. means “Minimum”, 1st Qu. means “First Quartile”, Median and Mean are obvious, 3rd Qu. means “Third Quartile”, and Max. means “Maximum”. If you read Kellstedt/Whitten (2018, Chapter 6), then you should know how to interpret these different statistics.",
    "crumbs": [
      "Tutorial 2: Importing & exploring your data"
    ]
  },
  {
    "objectID": "tutorial_2.html#specific-summary-statistics-for-numeric-variables",
    "href": "tutorial_2.html#specific-summary-statistics-for-numeric-variables",
    "title": "Tutorial 2: Importing & exploring your data",
    "section": "3.5 Specific summary statistics for numeric variables",
    "text": "3.5 Specific summary statistics for numeric variables\nWhile summary() provides you a whole list of summary statistics, you often want a specific measure of central tendency or spread for a given variable.\nThese are easy to get in R; all you need are four functions, all with quite intuitive names:\n\nmean() for the mean or “average”;\nmedian() for the median or “50th percentile”;\nvar() for the variance;\nsd() for the standard deviation;\n\nUsing these functions is straightforward — for example, to get the mean of the age-variable (agea) in the ess dataset, you just run:\n\nmean(ess$agea)\n## [1] 47.90909\n\nGetting the other summary statistics works the same way:\n\nmedian(ess$agea)\n## [1] 46\n\nsd(ess$agea)\n## [1] 18.5658\n\nvar(ess$agea)\n## [1] 344.6889\n\n\n\n3.5.1 When you have missing observations (NAs)\nIt is often the case that your variables contain missing information — indicated in R as NA. This happens for example when surveys include sensitive questions about people’s incomes or their sexual orientation, which many respondents refuse to disclose The result is then an NA (“not available”) for that particular respondent and variable.\nImportant: The mean(), median(), sd(), and var() functions (and many others) will not give you a proper result if there is even a single NA in your variable!\nFortunately, there is an easy solution: All four functions have an option to remove NAs from the data before calculating the respective summary statistic; this option is called na.rm (“NA remove”). You just have to set this option to TRUE (switch it on) to take care of missings, for example:\n\nmean(ess$agea, na.rm = TRUE)\n## [1] 47.90909\n\n(Make sure you always add a comma between different parts or “arguments” of a function!)",
    "crumbs": [
      "Tutorial 2: Importing & exploring your data"
    ]
  },
  {
    "objectID": "tutorial_2.html#working-with-categorical-or-ordinal-variables",
    "href": "tutorial_2.html#working-with-categorical-or-ordinal-variables",
    "title": "Tutorial 2: Importing & exploring your data",
    "section": "3.6 Working with categorical or ordinal variables",
    "text": "3.6 Working with categorical or ordinal variables\n\n3.6.1 Introducing factors\nThe variable you have been working with so far, agea, is a typical numeric variable: It measures a respondent’s age in years, and age is by nature a number. In this case, calculating statistics such as the mean makes sense.\nBut there are also other variables such as categorical or ordinal variables, where things are a bit different. Consider for example the variable that records the respondent’s gender, gndr. Obviously, gender is by nature a categorical variable: It has two or more distinct categories (e.g., male, female, diverse), and these categories are unordered, meaning ‘male’ is obviously not a ‘higher’ or ‘better’ category than ‘female’ or ‘diverse’. They are all simply different categories people can fall (or be put) into.\nOther times, you may be dealing with ordinal variables (e.g., a Likert-scale: “disagree completely”, “disagree”,“neither”, “agree”, “agree completely”). In these cases, there is an order — but you cannot give a precise number for how much higher “agree completely” is compared to “agree”. One is more than the other, but the difference between them is not clearly defined with a number.\nIn R, categorical or ordinal variables are usually stored as factors. Factors are a separate kind of variable or “vector” (next to numeric or num and character or chr variables). You can think of factors as “numbers with labels”.\nFor example, take another look at the Environment tab (upper right of your screen) and look for the gndr variable. You can see directly that it is designated as a “Factor” with 2 levels — but also that there is a row of numbers (1,2,2,...) behind the two levels “Male” and “Female”.\nThis means:\n\nEvery male respondent gets the number 1; that number then gets the label “Male” attached to it;\nEvery female respondent gets the number 2; that number is then labeled “Female”;\n\nThe same applies also to the (many) other factor variables in the ess dataset, or other datasets. Again: Factors are essentially just numbers with text labels.\n\n\n\n3.6.2 Identifying factor variables\nFirst, you should be able to identify that a given variable is indeed a factor variable. You can of course see this in the Environment, but this works only for a small dataset like the one you are using now. If you would work with the full ESS data, there would be many more variables and not all of them would be shown in the Environment.\nYou can use the class() function to let R tell you which type or class a specific variable is saved as. You use this like the other functions above (dataset$variable).\nLet’s check if the gender-variable (gndr) is really saved as a factor, as it should be:\n\nclass(ess$gndr)\n## [1] \"factor\"\n\nNow compare this to the age variable (agea):\n\nclass(ess$agea)\n## [1] \"numeric\"\n\nThis is by nature a numeric variable, and it turns out that R has stored it properly.\n\nImportant: You cannot rely on that this always works! It is often the case that one or more variables in your dataset are not stored properly, which then usually causes warnings and errors. In this case, you first need to identify the issue — and you now know how to do that — and then you need to fix it. You will learn how to do this in the next tutorial.\n\n\n\n\n3.6.3 Getting familiar with factor variables\nOnce you have identified a factor variable, you will usually want to learn more about it. But getting familiar with factors can be a bit tricky at first. Many summary statistics will not work here. For example, if you try to calculate the mean of a factor variable, R will refuse to do so:\n\nmean(ess$gndr)\n## Warning in mean.default(ess$gndr): argument is not numeric or logical:\n## returning NA\n## [1] NA\n\nThis does make sense: Many summary statistics are only appropriate if you are dealing with proper numbers, but here you have only categories. But this also means that you have to use different ways to learn how a factor variable in your dataset looks like.\n\n\n\n3.6.4 Getting the structure of a factor-type variable\nA first option is to let R print out the structure of the variable using str() (“structure”):\n\nstr(ess$gndr)\n##  Factor w/ 2 levels \"Male\",\"Female\": 1 2 2 2 1 1 1 1 1 1 ...\n\nThis tells you that gndr has two categories (“Male” & “Female”) and that these are encoded with the numbers 1 and 2 in the dataset.\nWhat is not fully clear from this output, however, is which number really corresponds to which label — are men now coded as 1 or as 2? And this is also generally one of the things that can make working with factors daunting: it is a bit difficult to see ‘under the hood’ of a factor: how its text labels correspond to the numerical values underneath.\nBut you do have a tool to figure this out!\n\n\n\n3.6.5 How numerical values and text labels correspond\nThe visfactor() function in the bst290 package allows you to see which number corresponds with which label in a given factor-type variable.\nFor example, to see the labels and numerical values of the gndr variable, you would run:\n\nvisfactor(variable = \"gndr\", dataset = ess)\n##  values labels\n##       1   Male\n##       2 Female\n\n\n\n\n3.6.6 Empty categories in factor-type variables\nAnother important thing to figure out is whether a particular factor variable in your dataset has empty categories. For example, you might be working with data from a survey in which respondents were asked whether they are working, in education, or unemployed — and it just so happened that none of the respondents were unemployed at the time. In this case, “unemployed” would be an empty category in the data.\nThe easiest way to see if there are empty categories in a factor variable is to let R show you how many observations you have for each of the categories of the variable. To do so, you use the table() function.\nThis is how you would do this with the gndr variable:\n\ntable(ess$gndr)\n## \n##   Male Female \n##     75     68\n\nYou see that there are 75 men and 68 women in the dataset — and there are no empty categories.\nBut now compare this to the case of the mainact variable, which tells you about the respondent’s main activity of the last seven days (whether they were working, unemployed, etc.):\n\ntable(ess$mainact)\n## \n##                                 Paid work \n##                                        15 \n##                                 Education \n##                                         7 \n##               Unemployed, looking for job \n##                                         0 \n##           Unemployed, not looking for job \n##                                         0 \n##              Permanently sick or disabled \n##                                         1 \n##                                   Retired \n##                                         7 \n##             Community or military service \n##                                         0 \n## Housework, looking after children, others \n##                                         3 \n##                                     Other \n##                                         0\n\nIt turns out that there are indeed some empty categories: There are no unemployed respondents in the dataset, and none of them was doing military or community services.\n\nAn alternative way to identify empty categories is to let R first print out which categories a factor variable can theoretically have and then compare that to what categories are actually represented in the dataset.\nTo see which categories your factor-variable can theoretically have, you use the levels() function:\n\nlevels(ess$mainact)\n## [1] \"Paid work\"                                \n## [2] \"Education\"                                \n## [3] \"Unemployed, looking for job\"              \n## [4] \"Unemployed, not looking for job\"          \n## [5] \"Permanently sick or disabled\"             \n## [6] \"Retired\"                                  \n## [7] \"Community or military service\"            \n## [8] \"Housework, looking after children, others\"\n## [9] \"Other\"\n\nYou see that the mainact variable has, in theory, nine categories in total, ranging from “Paid work” to “Other”.\nNow, to see which of these categories are really present in the data, you can use the unique() function:\n\nunique(ess$mainact)\n## [1] &lt;NA&gt;                                     \n## [2] Retired                                  \n## [3] Paid work                                \n## [4] Education                                \n## [5] Housework, looking after children, others\n## [6] Permanently sick or disabled             \n## 9 Levels: Paid work Education ... Other\n\nYou see that only five (plus the NAs) of the nine categories are listed — and being unemployed is not one of them.",
    "crumbs": [
      "Tutorial 2: Importing & exploring your data"
    ]
  },
  {
    "objectID": "tutorial_2.html#custom-functions-for-summary-tables",
    "href": "tutorial_2.html#custom-functions-for-summary-tables",
    "title": "Tutorial 2: Importing & exploring your data",
    "section": "3.7 Custom functions for summary tables",
    "text": "3.7 Custom functions for summary tables\nSince it is a statistical programming language, R can be used to generate pretty much any type of summary table for any kind of situation you could think of. In addition, there are special packages for more advanced tables, for instance:\n\ngtsummary (https://www.danieldsjoberg.com/gtsummary/index.html)\nxtable (https://cran.r-project.org/web/packages/xtable/vignettes/xtableGallery.pdf)\n\nBut: Learning how to use R functions to create tables takes a while, and using them can be tedious and prone to errors.\n\n3.7.1 Functions from the bst290 package\nTo make your life easier while you take this course, you can use special functions from the bst290 package to easily generate the most important descriptive tables you will need:\n\noppsumtabell: To generate univariate summary tables; this is helpful for numeric variables.\noppsum_grupp: To get a table with summary statistics for one variable, over categories of another variable; this is helpful when you have a a numeric and a categorical variable.\n\n\n\n\n\n3.7.2 Using oppsumtabell\noppsumtabell produces a table with the most important summary statistics of one or more numeric variables.2 All you need to do is specify the dataset that contains your variable(s) and the specific variables you want summary statistics for.\nFor example, to get summary statistics for the agea variable you just run:\n\noppsumtabell(dataset = ess, variables = \"agea\")\n\n Variable        agea  \n Observations    143.00\n Average          47.91\n 25th percentile  33.50\n Median           46.00\n 75th percentile  62.00\n Stand. Dev.      18.57\n Minimum          16.00\n Maximum          90.00\n Missing           0.00\n\n\nTo do the same for more than one variable, you run:\n\noppsumtabell(dataset = ess, variables = c(\"agea\",\"height\",\"weight\"))\n\n Variable        agea   height weight\n Observations    143.00 142.00 138.00\n Average          47.91 173.76  78.63\n 25th percentile  33.50 167.25  65.00\n Median           46.00 174.00  75.00\n 75th percentile  62.00 180.00  88.75\n Stand. Dev.      18.57   8.78  19.20\n Minimum          16.00 147.00  50.00\n Maximum          90.00 196.00 182.00\n Missing           0.00   1.00   5.00\n\n\nThis table shows summary statistics for age (agea) and the respondent’s body height and weight.\nCan you interpret each of the statistics shown (again, see Kellstedt/Whitten 2018, Chapter 6).\n\n\n\n3.7.3 Norwegian language support\nYou can choose to have the table labelled in Norwegian (NB), if you want. All you have to do is to activate the norsk-option of the oppsumtabell() function and set it to TRUE (or T):\n\noppsumtabell(dataset = ess, \n             variables = c(\"agea\",\"height\",\"weight\"),\n             norsk = TRUE)\n\n Variabel      agea   height weight\n Observasjoner 143.00 142.00 138.00\n Gjennomsnitt   47.91 173.76  78.63\n 25. persentil  33.50 167.25  65.00\n Median         46.00 174.00  75.00\n 75. persentil  62.00 180.00  88.75\n Standardavvik  18.57   8.78  19.20\n Minimum        16.00 147.00  50.00\n Maksimum       90.00 196.00 182.00\n Manglende       0.00   1.00   5.00\n\n\nIf you take a look at the new version of the table, you will see that all English labels (“standard deviation”, “observations”) are replaced with their Norwegian equivalents (“standardavvik”, “observasjoner”).\n\n\n\n3.7.4 Exporting the table to Word\noppsumtabell also has an export-functionality: You can switch on the export-function to get a result that you can directly copy and paste into a Word document and then transform into a nice, publication-quality table.\nFor example, to export the last table from above you simply add export=TRUE to your code:\n\noppsumtabell(dataset = ess, \n             variables = c(\"agea\",\"height\",\"weight\"),\n             norsk = TRUE,\n             export = TRUE)\n## Variabel,agea,height,weight\n## Observasjoner,143.00,142.00,138.00\n## Gjennomsnitt, 47.91,173.76, 78.63\n## 25. persentil, 33.50,167.25, 65.00\n## Median, 46.00,174.00, 75.00\n## 75. persentil, 62.00,180.00, 88.75\n## Standardavvik, 18.57,  8.78, 19.20\n## Minimum, 16.00,147.00, 50.00\n## Maksimum, 90.00,196.00,182.00\n## Manglende,  0.00,  1.00,  5.00\n\nThis result arguably looks even less presentable than the other one, but:\n\nCopy the result as it is displayed in the Console (see also the screenshot below);\nOpen a Word document;\nPaste the copied text into the document;\nSelect the copied text and, in Word, open the ‘Table’ menu in the menu bar at the top; there, select ‘Convert’ and then `Convert text to table…’;\nIn the menu, under “Separate text at” (“Skill tekst ved”), select “Other” (“Annet”) and enter a comma into the field next to that option. The number of columns at the top should then also automatically adjust. Then click ‘OK’;\nPolish the table using the familiar options in Word;\n\n\n\n\nSelecting & copying the results from the R console\n\n\n\n\n\n3.7.5 Using oppsum_grupp\nSometimes you want summary statistics for one variable, but separately for different categories of another variable. For example, assume you are interested in whether (and if yes, by how much) Norwegian men are on average taller than Norwegian women.\nThe oppsum_grupp() function produces a summary table that contains the same statistics as the ones you get from oppsumtabell(), but now broken down by categories of a second variable (which should ideally have only a few distinct categories!).\nTo get summary statistics for body height for men and women separately (i.e., over the categories of gndr) you run:\n\noppsum_grupp(dataset = ess, variable = \"height\", by.var = \"gndr\")\n##  gndr   Observations Average Stand. Dev. 25th percentile Median 75th percentile\n##  Female 67           167.87  6.66        164.00          168.00 173.00         \n##  Male   75           179.03  6.90        174.00          178.00 183.50         \n##  Minimum Maximum Missing\n##  147.00  180.00  1      \n##  165.00  196.00  0\n\nYou can see that men are, on average, around 11 centimeters taller than women, and that the smallest woman is smaller than the smallest man (and the same for the tallest individuals in the sample).\nLike oppsumtabell(), oppsum_grupp() also has an export function (export = TRUE) and Norwegian language support (norsk = TRUE).\n\n\n\n3.7.6 Further help\nYou now know how to get quick summary statistics for a dataset or specific variables in a dataset. Of course, this tutorial covered only the essentials and there are many other ways to summarize your data. But these essentials should help you when you do your first steps as a political or social data analyst.\nAlso, if you want to get more detailed help on any of the functions covered in this tutorial, you can always resort to the functions’ help files. For example, to get the help file for the mean() or oppsumtabell() functions, you just type the following into your Console tab and press Enter:\n\n?mean\n?oppsumtabell\n\nThe help files also contain examples that show you how to use the functions. Feel free to explore!",
    "crumbs": [
      "Tutorial 2: Importing & exploring your data"
    ]
  },
  {
    "objectID": "tutorial_2.html#the-european-social-survey",
    "href": "tutorial_2.html#the-european-social-survey",
    "title": "Tutorial 2: Importing & exploring your data",
    "section": "4.1 The European Social Survey",
    "text": "4.1 The European Social Survey\nThe European Social Survey (ESS) is a large survey project that is conducted in countries all over Europe, including in Norway, and which has been running for several years now. In each round, between several hundred to more than 2000 randomly selected persons in each participating country give information (anonymously, of course) about their political opinions and behavior, their views about society, and their income, jobs, work situation, and families. Their responses are then made machine-readable and stored in dataset files, which anyone can use for free.\nYou can use the ESS to study, for example, why people vote or participate otherwise politically (e.g., by joining demonstrations or protests), which parties they voted for, how people think about social inequality, climate change, sexuality, the welfare state, and many other topics. Political scientists and sociologists often use data from the ESS in their research.3\nSee https://www.europeansocialsurvey.org/ for more details.\n\n\n4.1.1 Accessing & downloading ESS data\nYou can access all data from the ESS via the SIKT Data Portal: https://ess.sikt.no/en/.4\nOnce you have the page open:\n\nScroll down and choose ESS Round 7 – 2014. Immigration, Social inequalities in health.\nChoose ESS7 - integrated file, edition 2.3. You will then be forwarded to another page.\nClick on the red “Download” button that is shown on the upper right of your screen.\nYou should then be forwarded a login page. Choose “Logg in med Feide” and use your UiS credentials to log in. (you may be able to jump over that step if you are already logged on to Feide, e.g., via Canvas).\nOnce you are logged in, you will be directed back to the ESS Data Portal – and you will now see three different download buttons (CSV, SPSS, Stata) on the upper right of your screen.\nClick on the Stata button.\nThe data and a few other files will be downloaded as part of a compressed ZIP file. Unpack and open that file. The folder that opens will contain one file that ends with .dta.5\nThe file ending with .dta is the dataset file. Copy/move this file into your project folder (in Windows File Explorer/Mac Finder). Ideally, give the file a shorter name that is easier to type (e.g., ess7.dta).\n\nOnce you have your data file stored within your project folder, you can go back to RStudio.\nHere, you can check if everything worked by opening the Files tab in the lower-right corner. The dataset file should be listed here (next to all the other files in this folder).",
    "crumbs": [
      "Tutorial 2: Importing & exploring your data"
    ]
  },
  {
    "objectID": "tutorial_2.html#importing-data-with-haven-and-labelled",
    "href": "tutorial_2.html#importing-data-with-haven-and-labelled",
    "title": "Tutorial 2: Importing & exploring your data",
    "section": "4.2 Importing data with haven and labelled",
    "text": "4.2 Importing data with haven and labelled\nR by itself can open some types of dataset files, but not all of them. Among the types of files that R itself cannot open are those that were created for other (commercial) data analysis programs:\n\n.sav, the file format for SPSS\n.dta, the file format for Stata\n.sas7bdat, the file format for SAS\n\nThe ESS dataset file you just downloaded is a .dta file — which means this dataset is saved in the Stata file format, and R by itself cannot open it.\nBut, luckily, there are a few packages that allow you to import these types of files into R. One of these is the haven package, and this is the one we will be using in this course.6 haven is a part of the tidyverse collection (see https://haven.tidyverse.org/), which means that you already installed it when you installed the tidyverse earlier.\n\n\n\n\n\n\nJust in case\n\n\n\nIf R gives you an error message (e.g., “Package labelled not found), you may have to quickly install the two packages with:\n\ninstall.packages(\"haven\")\ninstall.packages(\"labelled\")\n\n\n\nhaven includes three functions to import the three main “commercial” dataset file formats:\n\nread_sav() for .sav files\nread_dta() for .dta files\nread_sas() for .sas7bdat files\n\nTherefore, to import the ESS dataset that you just downloaded in .dta format, you would use read_dta().\nImportant: haven has a bit of a quirk in that it has its own way of organizing a dataset within R – called the labelled format – and that can take a bit to get used. To keep things simple, we convert the dataset to the “normal” format for R. To do that, we use the labelled::unlabelled() function.\n\nPutting all this together: To import the dataset file, you would use:\n\ness7 &lt;- labelled::unlabelled(haven::read_dta(\"ess7.dta\"))\n\nHere, haven::read_dta() uses the read_dta() function from haven to import the dataset – and then we directly convert it with labelled::unlabelled() and save the result as ess7.\n\n\n4.2.1 Generating a data dictionary\nIf you take a quick look at the ess7 data object in the Environment, you notice that it contains 601 variables. Such a large number of variables is typical for a real-life survey dataset, but it also means that it can be difficult to get an overview over all the variables and their values.\nFortunately, there is a function to easily create a data dictionary or codebook that is included in labelled: the generate_dictionary() function.\nUsing this function is easy — you just need to make sure to save the function’s output in a new object like dict_ess7:\n\ndict_ess7 &lt;- labelled::generate_dictionary(ess7)\n\nYou will now see a new object in your Environment called dict_ess7.7 If you now run View(dict_ess7), you get a neat table that shows you the name, label, and value labels of all the variables in your dataset.\nNow you know how you can get survey data for Norway and many other countries on a wide variety of topics from a highly trusted source! Take also a few minutes to explore the ESS website and their Data Portal to see which topics they cover and which variables they have in each survey round!",
    "crumbs": [
      "Tutorial 2: Importing & exploring your data"
    ]
  },
  {
    "objectID": "tutorial_2.html#footnotes",
    "href": "tutorial_2.html#footnotes",
    "title": "Tutorial 2: Importing & exploring your data",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDid you miss that session? You can read about Projects and how you create them here: https://support.posit.co/hc/en-us/articles/200526207-Using-RStudio-Projects or in Lær deg R, 4.1.1.3.↩︎\nIt does also work with factor variables, but you will get a warning message.↩︎\nExamples are: Rehm, P. (2009). Risks and redistribution: An individual-level analysis. Comparative Political Studies, 42(7):855–81; Giger, N. and Nelson, M. (2013). The welfare state or the economy? Preferences, constituencies, and strategies for retrenchment. European Sociological Review, 29(5):1083–94; Hooghe, M., Reeskens, T., Stolle, D., and Trappers, A. (2009). Ethnic diversity and generalized trust in Europe: A cross-national multilevel study. Comparative Political Studies, 42(2):198– 223; Gallego, A. (2007). Unequal political participation in Europe. International Journal of Sociology, 37(4):10–25; or Finseraas, H. (2008). Immigration and preferences for redistribution: An empirical analysis of European survey data. Comparative European Politics, 6(4):407–431.↩︎\nAlternatively, go to https://www.europeansocialsurvey.org/ and click on “Data” in the menu at the top. On the following page, click on “ESS Data Portal” button.↩︎\nIf you cannot see the file endings (“extensions”), you need to activate this in File Explorer/Finder. You should find instructions for your particular operating system if you google for example “show file extensions in Windows” or “show file extensions in Mac”.↩︎\nOther alternatives are foreign, memisc, or readstata13.↩︎\nTechnically, the dict_ess7 dictionary is itself a dataset-type object, which means you can also do some data exploration with it. This goes beyond the scope of this tutorial, but feel free to play around with it.↩︎\nIf there are no tutorials called “De-bugging exercises:…” shown, just restart R by clicking on “Session” in the menu at the top of your screen, and there on “Restart R”. You may also have to install the learnr package — in that case, RStudio will let you know and you only have to do this once.↩︎",
    "crumbs": [
      "Tutorial 2: Importing & exploring your data"
    ]
  },
  {
    "objectID": "tutorial_9.html",
    "href": "tutorial_9.html",
    "title": "Tutorial 9: Multivariate linear regression",
    "section": "",
    "text": "In the previous tutorial, you learned how to estimate a bivariate linear regression model in R. You used a linear regression model to see if people’s level of education influences their level of trust in politicians. The expectation was that higher education would lead to greater trust in politicians, and that is also what we found.\nIn this tutorial, we go one step further and test if this bivariate relationship stays the same (“is robust”) when we control for additional factors. We do so by estimating a multivariate regression model. In this type of model, we estimate the effects of several independent variables (predictors) on a dependent variable simultaneously.\n\n\n\n\n\n\nTip\n\n\n\nHvis du ønsker å lese en norsk tekst i tillegg: “Lær deg R”, Kapittel 8.",
    "crumbs": [
      "Tutorial 9: Multivariate linear regression"
    ]
  },
  {
    "objectID": "tutorial_9.html#packages-data-import-and-first-data-cleaning",
    "href": "tutorial_9.html#packages-data-import-and-first-data-cleaning",
    "title": "Tutorial 9: Multivariate linear regression",
    "section": "2.1 Packages, data import, and first data cleaning",
    "text": "2.1 Packages, data import, and first data cleaning\nAs before, we use the tidyverse to help with data management and visualization and texreg to make neat-looking regression tables.\n\nlibrary(tidyverse)\nlibrary(texreg)\n\nYou should now already know what to do next, and how to do it:\n\nUse haven::read_dta() to import the ESS round 7 (2014) dataset; save it as ess7;\nTransform the dataset into the familiar format using labelled::unlabelled();\nTrim the dataset:\n\nKeep only observations from Norway;\nSelect the following variables: essround, idno, cntry, trstplt, eduyrs — and also agea, and gndr;\nUse the pipe to link everything;\nSave the trimmed dataset as ess7;\n\nIf you like, create a data dictionary using labelled::generate_dictionary();\nTransform the trstplt variable from factor to numeric using as.numeric(); do not forget to adjust the scores; store the new variable as trstplt_num;",
    "crumbs": [
      "Tutorial 9: Multivariate linear regression"
    ]
  },
  {
    "objectID": "tutorial_9.html#new-data-management-additional-independent-variables",
    "href": "tutorial_9.html#new-data-management-additional-independent-variables",
    "title": "Tutorial 9: Multivariate linear regression",
    "section": "2.2 New data management: additional independent variables",
    "text": "2.2 New data management: additional independent variables\nIn addition to the main predictor from the previous tutorial (eduyrs, the respondent’s level of education), we want to add controls for two additional variables:\n\nThe respondent’s age in years, using agea\nThe respondent’s gender, using gndr\n\nWe take a quick look at how these variables are stored:\n\nclass(ess7$agea)\n[1] \"numeric\"\nclass(ess7$gndr)\n[1] \"factor\"\n\nLuckily, these variables are stored correctly: agea is numeric, and gndr is a factor — as they should be.",
    "crumbs": [
      "Tutorial 9: Multivariate linear regression"
    ]
  },
  {
    "objectID": "tutorial_9.html#the-lm-function",
    "href": "tutorial_9.html#the-lm-function",
    "title": "Tutorial 9: Multivariate linear regression",
    "section": "4.1 The lm() function",
    "text": "4.1 The lm() function\nJust to refresh your memory, we first estimate the bivariate regression model from the previous tutorial and store the result as model1:\n\nmodel1 &lt;- lm(trstplt_num ~ eduyrs, \n             data = ess7)\n\nIf you like, you can also print out the result with summary():\n\nsummary(model1)\n\nCall:\nlm(formula = trstplt_num ~ eduyrs, data = ess7)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7148 -1.1966  0.0996  1.4332  5.0256 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.23401    0.19737  21.452  &lt; 2e-16 ***\neduyrs       0.07404    0.01377   5.378 8.78e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.932 on 1425 degrees of freedom\n  (9 observations deleted due to missingness)\nMultiple R-squared:  0.0199,    Adjusted R-squared:  0.01921 \nF-statistic: 28.93 on 1 and 1425 DF,  p-value: 8.779e-08\n\n\nIn the next step, we test if the effect of eduyrs is robust to including the additional control variables: gender and age.\nThis means that we estimate our multivariate model, store the result as model2, and print out the results:\n\nmodel2 &lt;- lm(trstplt_num ~ eduyrs + gndr + agea, data = ess7)\nsummary(model2)\n\nCall:\nlm(formula = trstplt_num ~ eduyrs + gndr + agea, data = ess7)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7539 -1.1985  0.1983  1.4072  5.2348 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.621764   0.249674  18.511  &lt; 2e-16 ***\neduyrs       0.069118   0.013861   4.986 6.91e-07 ***\ngndrFemale   0.070814   0.102590   0.690  0.49014    \nagea        -0.007544   0.002756  -2.737  0.00627 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.928 on 1423 degrees of freedom\n  (9 observations deleted due to missingness)\nMultiple R-squared:  0.02526,   Adjusted R-squared:  0.0232 \nF-statistic: 12.29 on 3 and 1423 DF,  p-value: 6.121e-08\n\nYou see that the output looks the same as before, just with more coefficients listed. Notice also that R has automatically figured out that gndr is a factor variable and included a dummy only for women (gndrFemale) into the model (as also explained in Kellstedt & Whitten, Chapter 11.2).\nNow it gets tricky: How do you make sense of these results? Kellstedt & Whitten (2018, Chapters 10 & 11) or Solbakken (Statistikk for Nybeginnere, Chapters 6 & 8) explain this.",
    "crumbs": [
      "Tutorial 9: Multivariate linear regression"
    ]
  },
  {
    "objectID": "tutorial_9.html#presenting-the-regression-results-in-a-publication-quality-table",
    "href": "tutorial_9.html#presenting-the-regression-results-in-a-publication-quality-table",
    "title": "Tutorial 9: Multivariate linear regression",
    "section": "4.2 Presenting the regression results in a publication-quality table",
    "text": "4.2 Presenting the regression results in a publication-quality table\nTo present the results and export them as a nice-looking table, we use again the texreg package.\nFirst, we print out a preview of our table using the screenreg() function. Now we have two regression models to print, so we have to list both in the function:\n\nscreenreg(list(model1, model2))\n\n=====================================\n             Model 1      Model 2    \n-------------------------------------\n(Intercept)     4.23 ***     4.62 ***\n               (0.20)       (0.25)   \neduyrs          0.07 ***     0.07 ***\n               (0.01)       (0.01)   \ngndrFemale                   0.07    \n                            (0.10)   \nagea                        -0.01 ** \n                            (0.00)   \n-------------------------------------\nR^2             0.02         0.03    \nAdj. R^2        0.02         0.02    \nNum. obs.    1427         1427       \n=====================================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\nWhen you look at the table, you can directly see the differences between the two models. Model 1 included only the intercept and education (eduyrs), while model 2 also included age and gender as control variables. The table includes all the variables’ coefficients: You see, for instance, the coefficients for education and how they differ (or not) between the two models.\n\nNext, we add proper labels for the coefficients and trim the significance stars:\n\nscreenreg(list(model1,model2),\n          custom.coef.names = c(\"Intercept\",\n                                \"Years of educ. completed\",\n                                \"Female\",\n                                \"Age\"),\n          stars = 0.05)\n\n==============================================\n                          Model 1    Model 2  \n----------------------------------------------\nIntercept                    4.23 *     4.62 *\n                            (0.20)     (0.25) \nYears of educ. completed     0.07 *     0.07 *\n                            (0.01)     (0.01) \nFemale                                  0.07  \n                                       (0.10) \nAge                                    -0.01 *\n                                       (0.00) \n----------------------------------------------\nR^2                          0.02       0.03  \nAdj. R^2                     0.02       0.02  \nNum. obs.                 1427       1427     \n==============================================\n* p &lt; 0.05\n\nFinally, we use the wordreg() function to export the table to a Microsoft Word document:\n\nwordreg(list(model1,model2),\n          custom.coef.names = c(\"Intercept\",\n                                \"Years of educ. completed\",\n                                \"Female\",\n                                \"Age\"),\n          stars = 0.05,\n        file = \"ols_models.doc\")",
    "crumbs": [
      "Tutorial 9: Multivariate linear regression"
    ]
  },
  {
    "objectID": "tutorial_8.html",
    "href": "tutorial_8.html",
    "title": "Tutorial 8: Bivariate linear regression",
    "section": "",
    "text": "The ordinary least squares (OLS) or “linear” regression model is probably the workhorse statistical model in (social) data analysis. Countless analyses use it, or variations of it, and all other more advanced models such as logistic regression follow in essence the same logic.\nIt is therefore important that you know how to estimate and interpret a linear regression model in R, and how you can present the results in your course paper or also a thesis or report. This is what you learn in this tutorial.\nSpecifically, this tutorial will first walk you through the essential data preparation steps. Then you will see how you estimate a bviariate linear regression model with the lm() function. The last step is to show you how you can present the results in an informative and professional way in your paper.\nTo illustrate all this, we will study the relationship between how educated people are and how much trust they have in politicians and international institutions using data from the European Social Survey.\n\n\n\n\n\n\nTip\n\n\n\nHvis du ønsker å lese en norsk tekst i tillegg: “Lær deg R”, Kapittel 7.",
    "crumbs": [
      "Tutorial 8: Bivariate linear regression"
    ]
  },
  {
    "objectID": "tutorial_8.html#packages-data-download",
    "href": "tutorial_8.html#packages-data-download",
    "title": "Tutorial 8: Bivariate linear regression",
    "section": "3.1 Packages & data download",
    "text": "3.1 Packages & data download\nAs always, the first step in an analysis in R is to load all the packages we need (and install those that are not yet installed).\nIn this tutorial, you will directly work with a full dataset from the European Social Survey (ESS) — not the small practice dataset that you used before. Since the first step will again be to clean and prepare the data, you need the tidyverse package.\nTo load the package, you just use library() as before:\n\nlibrary(tidyverse)\n\nBut you will now also use a new package that you do not yet know: texreg. This is a package that allows you to easily create nice-looking tables that contain the results of your regression models.\nTo be able to use texreg, you obviously need to install and then load it. Therefore, use install.packages() to install the package — you can run this in your Console since you will do this only once:\n\ninstall.packages(\"texreg\")\n\nWhen the installation is complete, use library() to load the package and document this in your script file:\n\nlibrary(texreg)",
    "crumbs": [
      "Tutorial 8: Bivariate linear regression"
    ]
  },
  {
    "objectID": "tutorial_8.html#data-import",
    "href": "tutorial_8.html#data-import",
    "title": "Tutorial 8: Bivariate linear regression",
    "section": "3.2 Data import",
    "text": "3.2 Data import\nYou should by now know how to import and “trim” ESS data, so this part will be brief and without an example code chunk:\n\nUse haven::read_dta() to import the ESS round 7 (2014) dataset; save it as ess7 in RStudio.\nTransform the dataset into the familiar format using labelled::unlabelled();\nTrim the dataset:\n\nKeep only observations from Norway;\nSelect the following variables: essround, idno, cntry, trstplt (trust in politicians), and eduyrs;\nUse the pipe to link everything;\nSave the trimmed dataset as ess7;\n\nIf you like, create a data dictionary using labelled::generate_dictionary();\n\n\nThe two main variables for the main part of this tutorial are trstplt and eduyrs. trstplt measures how much trust people have in politicians in general. Specifically, respondents were asked to rate on a scale from 0 (“No trust at all”) to 10 (“Complete trust”) how much they trust different political institutions and actors — and politicians were one of them. This will be the dependent variable here.\nYou can learn more about the variable if you use the attributes() function:\n\nattributes(ess7$trstplt)\n\nWe will use the eduyrs variable to measure people’s level of education. eduyrs measures how many years of full-time education a given respondent has completed. Obviously, this is not a perfectly accurate measurement (some people might spend a lot of time in education, but without success), but it can be used as a proxy (i.e., “approximate measurement”) for education.\nUse the data dictionary and the attributes() and other relevant functions to get familiar with the eduyrs variable.",
    "crumbs": [
      "Tutorial 8: Bivariate linear regression"
    ]
  },
  {
    "objectID": "tutorial_8.html#data-preparation",
    "href": "tutorial_8.html#data-preparation",
    "title": "Tutorial 8: Bivariate linear regression",
    "section": "3.3 Data preparation",
    "text": "3.3 Data preparation\nYou might have noticed already that the dependent variable is not stored as a numeric or linear variable but as a factor. This means that you cannot do any calculations with it in this form — and that includes obviously also linear regression.\nTherefore, you first have to convert the trstplt variable to a numeric variable. As before, this involves using the as.numeric() function while subtracting 1 to account for the divergence between the text labels and the underlying numerical values:\n\ness7$trstplt_num &lt;- as.numeric(ess7$trstplt) - 1",
    "crumbs": [
      "Tutorial 8: Bivariate linear regression"
    ]
  },
  {
    "objectID": "tutorial_8.html#norwegians-trust-in-politicians",
    "href": "tutorial_8.html#norwegians-trust-in-politicians",
    "title": "Tutorial 8: Bivariate linear regression",
    "section": "4.1 Norwegians’ trust in politicians",
    "text": "4.1 Norwegians’ trust in politicians\nWith the new numeric variable in hand, you can create a graph that shows how it is distributed. You should always show this if you are writing a research paper or report:\n\ness7 %&gt;% \n  ggplot(aes(x = trstplt_num)) +\n    geom_bar() +\n    scale_x_continuous(breaks = seq(from = 0, to = 10, by = 1)) +\n    labs(x = \"''How much do you trust politicians?''\",\n         y = \"Observations\",\n         caption = \"(0 = ''No trust at all''; 10 = ''Complete trust'')\") +\n    theme_bw()\n\n\n\n\n\n\n\n\nThe distribution is concentrated around the mid-to-high values (5-7), which indicates that Norwegians have an overall moderately high level of trust in their politicians. But only very few respondents said they have “complete trust” (10), and there are at least a few respondents that have low levels of trust (0-4).",
    "crumbs": [
      "Tutorial 8: Bivariate linear regression"
    ]
  },
  {
    "objectID": "tutorial_8.html#descriptive-table",
    "href": "tutorial_8.html#descriptive-table",
    "title": "Tutorial 8: Bivariate linear regression",
    "section": "4.2 Descriptive table",
    "text": "4.2 Descriptive table\nIt is also a good idea to give your readers descriptive statistics that help them see how your variables look like. Since you have two numeric or linear variables, a simple table with summary statistics would be appropriate:\n\nbst290::oppsumtabell(dataset = ess7,\n                     variables = c(\"trstplt_num\",\"eduyrs\"))\n Variable        trstplt_num eduyrs \n Observations    1429.00     1434.00\n Average            5.26       13.85\n 25th percentile    4.00       11.00\n Median             5.00       14.00\n 75th percentile    7.00       17.00\n Stand. Dev.        1.95        3.72\n Minimum            0.00        0.00\n Maximum           10.00       30.00\n Missing            7.00        2.00\n\nYou would of course describe this table in more detail in your paper or report, but we will skip this now for the sake of brevity.",
    "crumbs": [
      "Tutorial 8: Bivariate linear regression"
    ]
  },
  {
    "objectID": "tutorial_8.html#bivariate-analysis",
    "href": "tutorial_8.html#bivariate-analysis",
    "title": "Tutorial 8: Bivariate linear regression",
    "section": "4.3 Bivariate analysis",
    "text": "4.3 Bivariate analysis\nIt usually makes sense to do some simpler bivariate tests before you do a (more complicated) regression analysis. Both the dependent and the independent variable are continuous, so the appropriate bivariate test would be to calculate the Pearson correlation coefficient for the two variables and to test if this coefficient is significantly different from 0 (as you know from the previous tutorial):\n\ncor.test(x = ess7$eduyrs,\n         y = ess7$trstplt_num,\n         method = \"pearson\")\n\n    Pearson's product-moment correlation\n\ndata:  ess7$eduyrs and ess7$trstplt_num\nt = 5.3783, df = 1425, p-value = 8.779e-08\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.08981487 0.19154010\nsample estimates:\n      cor \n0.1410498 \n\nThe test shows that the two variables are weakly positively correlated with a correlation coefficient of 0.14. Can you tell if this correlation is statistically significant (hint: check the p-value!)?",
    "crumbs": [
      "Tutorial 8: Bivariate linear regression"
    ]
  },
  {
    "objectID": "tutorial_8.html#models-formulas",
    "href": "tutorial_8.html#models-formulas",
    "title": "Tutorial 8: Bivariate linear regression",
    "section": "5.1 Models & formulas",
    "text": "5.1 Models & formulas\nWhenever you estimate any regression model in R — be it a simple linear regression or more advanced models – you always need to specify the formula for the model.\nThe formula (or equation) is, in essence, nothing else than a statement about what your dependent and independent variables are and how they are related to each other. In the case here, we expect that people’s trust in politicians (measured via the trstplt_num variable) increases with their level of education (measured with eduyrs).\nExpressed as a formula, this would look like this: \\[\\begin{align*}\n  \\texttt{trstplt\\_num} = \\alpha + \\beta_1 \\texttt{eduyrs} + \\epsilon\n\\end{align*}\\]\nAs you know from the chapters on linear regression in Kellstedt & Whitten (see alternatively Solbakken, Statistikk for Nybeginnere, Chapter 6):\n\n\\(\\alpha\\) is the intercept (konstantleddet in Norwegian)\n\\(\\beta_1\\) is the regression coefficient (or “slope”, “weight”, regresjonskoeffisient, or stigningstall).\n\\(\\epsilon\\) is the error term — this term captures the variation in the dependent variable that the model cannot explain (see also Solbakken, Statistikk for Nybeginnere, p. 177).\n\n\nIn R, this formula looks much simpler:\n\n\ntrstplt_num ~ eduyrs\n\nThere are two interesting things to notice:\n\nWe do not use the equal sign (=) but the tilde (~). This indicates that we do not know if there really is a relationship between the dependent variable trstplt_num and eduyrs — we are only estimating if there is one!\nAll you have to do is to specify the dependent and the independent variable(s). R takes care of the intercept and the error term automatically.",
    "crumbs": [
      "Tutorial 8: Bivariate linear regression"
    ]
  },
  {
    "objectID": "tutorial_8.html#estimating-linear-regression-models-the-lm-function",
    "href": "tutorial_8.html#estimating-linear-regression-models-the-lm-function",
    "title": "Tutorial 8: Bivariate linear regression",
    "section": "5.2 Estimating linear regression models: The lm() function",
    "text": "5.2 Estimating linear regression models: The lm() function\nNow that you have the formula for the regression model, you just need to plug it into the R function to estimate linear regression models: the lm() (“linear models”) function.\nThe lm() function needs as inputs:\n\nThe formula (as just described)\nThe dataset that should be used\n\nThe following code estimates the model and then saves the result as model1:\n\n\nmodel1 &lt;- lm(trstplt_num ~ eduyrs,\n             data = ess7)\n\nR will not automatically show you the results — to see them, you need to print out a summary with summary():\n\nsummary(model1)\n\n\nCall:\nlm(formula = trstplt_num ~ eduyrs, data = ess7)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.7148 -1.1966  0.0996  1.4332  5.0256 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.23401    0.19737  21.452  &lt; 2e-16 ***\neduyrs       0.07404    0.01377   5.378 8.78e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.932 on 1425 degrees of freedom\n  (9 observations deleted due to missingness)\nMultiple R-squared:  0.0199,    Adjusted R-squared:  0.01921 \nF-statistic: 28.93 on 1 and 1425 DF,  p-value: 8.779e-08\n\n\nThe output is very R-typical: technical and condensed. Focus on the following:\n\nThe Coefficients:-table. First, you see two coefficients listed, the (Intercept) and eduyrs. These show you what the model includes: an intercept and a single independent variable (eduyrs).\nIn the columns to the right, you see Estimate, Std. Error, t value, and Pr(&gt;|t|) printed:\n\nThe Estimates are the coefficients themselves — the \\(\\alpha\\) (for the intercept) and the \\(\\beta\\)s or regression coefficients;\nThe Std. Errors are, as the name suggests, the standard errors of the coefficients.\nThe t values are the ratios of the coefficients and their standard errors: \\(\\frac{\\texttt{Estimate}}{\\texttt{Std. Error}}\\).2 You can interpret the t values like those from a t-test, only the test here is whether a given coefficient is equal to 0 or not.\nPr(&gt;|t|) are the p-values.\nThe asterisks are an alternative way to indicate p-values. Three asterisks correspond to a p-value of 0.001 or less. This is also explained in the legend (Signif. codes)\n\n\nKellstedt & Whitten (2018, Chapter 9) or Solbakken (Statistikk for Nybeginnere, Chapters 6 & 8) explain how you make sense of these numbers (also: feel free to ask during class!).\n\n\n\n\nIf you find this still difficult to grasp then it might help to look at a visualization of the model’s main result, which is shown in the graph below:\n\n\n\n\n\n\n\n\n\nFirst, focus on the black line with the dots. If you start at the point where eduyrs is zero, you see that the predicted value of trstplt_num is just a bit over 4. If you now go back to the result, is there a similar number?\nYou can also clearly see that there is a positive relationship between the two variables: As eduyrs increases, trstplt_num increases as well. If you look more carefully, you also see that trstplt_num increases only by a small amount for every additional year of education completed. Can you find out in the detailed results above by how much exactly trstplt_num changes?\nYou should also notice the gray-shaded area around the black line. This area indicates the confidence intervals for the predicted values. These confidence intervals reflect the fact that we are working with estimates based on a sample. You interpret these the same way as you interpreted the confidence intervall for a sample mean a few weeks ago.",
    "crumbs": [
      "Tutorial 8: Bivariate linear regression"
    ]
  },
  {
    "objectID": "tutorial_8.html#presenting-the-regression-results-in-a-publication-quality-table",
    "href": "tutorial_8.html#presenting-the-regression-results-in-a-publication-quality-table",
    "title": "Tutorial 8: Bivariate linear regression",
    "section": "5.3 Presenting the regression results in a publication-quality table",
    "text": "5.3 Presenting the regression results in a publication-quality table\nMaking sense of the results for yourself is of course only a part of your job as a (social) data analyst. The next step is to present them to the readers of your course paper, thesis, or report.\nWhen you report results like these, you should never simply copy and paste the results from R into your document. Instead, you should always present them in a clean and organized table. And this is in fact quite easy to do with the texreg package that you installed and loaded earlier! This package is specifically designed to transform regression results from R into publication-quality tables.\nAs a first step, it makes sense to print a preview of the table to your Console. To do this, you use the screenreg() function from the texreg package. Also, the table is supposed to show the results of the model that we estimated earlier (model1), so we specify that within the list (list()) of results we want to have in the table):\n\nscreenreg(list(model1))\n\n========================\n             Model 1    \n------------------------\n(Intercept)     4.23 ***\n               (0.20)   \neduyrs          0.07 ***\n               (0.01)   \n------------------------\nR^2             0.02    \nAdj. R^2        0.02    \nNum. obs.    1427       \n========================\n*** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05\n\nThe table shows you in essence the same information as the R output above, but in a more organized fashion. You see the coefficient for eduyrs and the intercept plus their standard errors and stars that indicate the corresponding p-values. Further below, you also see summary statistics including the \\(R^2\\), the adjusted \\(R^2\\), and the number of observations. At the very bottom, you see a legend that tells you how different stars (asterisks) correspond to different p-values. For example, three asterisks would indicate a p-value of less than 0.001.\n\nYou can still make this table a bit prettier by adding proper labels for the coefficients (never report the “raw” coefficient names — your readers don’t know what they mean!)3 and by trimming the significance stars (one is enough):\n\n\nscreenreg(list(model1),\n          custom.coef.names = c(\"Intercept\",\n                                \"Years of educ. completed\"),\n          stars = 0.05)\n\n===================================\n                          Model 1  \n-----------------------------------\nIntercept                    4.23 *\n                            (0.20) \nYears of educ. completed     0.07 *\n                            (0.01) \n-----------------------------------\nR^2                          0.02  \nAdj. R^2                     0.02  \nNum. obs.                 1427     \n===================================\n* p &lt; 0.05\n\nThis version is O.K. for now. Next, you use the wordreg() function to export the table to a Microsoft Word document. The settings stay the same, but you now also need to specify a name for the Word document that will contain your table with the file-option:\n\n\nwordreg(list(model1),\n          custom.coef.names = c(\"Intercept\",\n                                \"Years of educ. completed\"),\n          stars = 0.05,\n        file = \"bivariate_model.doc\")\n\nAnd done! If you open that document, you should see the final table with the results (which you can polish more in Word).",
    "crumbs": [
      "Tutorial 8: Bivariate linear regression"
    ]
  },
  {
    "objectID": "tutorial_8.html#footnotes",
    "href": "tutorial_8.html#footnotes",
    "title": "Tutorial 8: Bivariate linear regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee e.g., Bourdieu, P. (1985). The forms of capital. In Richardson, J. G., editor, Handbook of Theory and Research for the Sociology of Education, pages 241–258. Greenwood, New York; Brady, H. E., Verba, S., and Schlozman, K. L. (1995). Beyond SES: A resource model of political participation. American Political Science Review, 89(2):271–294.↩︎\nSee for yourself: Divide a coefficient by its standard error and see what the result is.↩︎\nI’m only doing this here to help you make sense of the code and the results. I would not do this in a research article that I want to publish.↩︎",
    "crumbs": [
      "Tutorial 8: Bivariate linear regression"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hei!",
    "section": "",
    "text": "This website features all R tutorials for the BST290 Kvantitativ forskningsmetode course. Simply use the menu on the side to navigate to and between the different tutorials and use the search field (top right) to find tutorials with specific contents.\n\n\n\n\n\n\nImportant\n\n\n\nTo make reading the tutorials easier and quicker, download them as PDFs and print them out. There are download buttons at the beginning of each tutorial (under the table of contents on the right side). You can of course read all the material “on screen” if you really want to, but that is not recommended. Ideally, you read each of the tutorials first on paper and you use this website mainly to quickly look things up later.\n\n\nIf you have questions, notice something that is wrong or is not shown properly, please send an email to carlo.knotz@uis.no.\nThat’s it. Lykke til!\n\n\n\n Back to top",
    "crumbs": [
      "Hei!"
    ]
  },
  {
    "objectID": "tutorial_3.html",
    "href": "tutorial_3.html",
    "title": "Tutorial 3: Data cleaning & management",
    "section": "",
    "text": "In the previous tutorial, you learned how you can import a dataset and do some initial exploratory data analysis (EDA) to get familiar with it.\nIn a real data analysis project, these would of course only be the very first steps. Usually, you will discover some smaller and larger issues with your data during the initial EDA, for example that you have to trim and clean the dataset, and that you may need to recode some variables or create new ones. In more extreme cases, your dataset can be disorganized or contain irrelevant information, or the variables in them can be stored in the wrong way.\nThis process of cleaning and organizing a dataset, and of creating new variables is called data management (or ‘munging’, ‘wrangling’, or ‘data manipulation’). It is the process of turning a raw dataset, which usually contains irrelevant observations or variables and where some variables need to be transformed or newly constructed, into the neat and tidy dataset you use in your statistical analysis.\nLet’s be honest for a moment: Data cleaning is often not very entertaining, and is generally the thing that is most difficult to get through when you are just starting and you do not yet have a good intuition or “muscle memory” for working with data. In that case, data cleaning can be a major hurdle and source of frustration.\nFortunately, things have improved a lot with the arrival of the tidyverse (see https://www.tidyverse.org/). As mentioned in the first tutorial, the tidyverse collection includes several packages that make data cleaning, and even otherwise difficult operations much easier and quicker.\nStill: Expect to be struggling with these things at the beginning, and that this tutorial will likely be the one that feels most dull and confusing. Just hang in there, and ask if you really need help!\nIn this tutorial, you will first learn how to do basic data cleaning and preparation tasks with functions from the tidyverse (sections 4-6). In section 7, you will learn how to change how a particular variable is stored (e.g., from numeric to character). Here, you will use functions from ‘base R’.\nAs in the previous tutorial, you will first practice all these operations with the small ess dataset from the bst290 package. Later, when doing the exercises, you will apply what you have learned to real-life data from the ESS.\n\n\n\n\n\n\nTip\n\n\n\nHvis du ønsker å lese en norsk tekst i tillegg: “Lær deg R”, Kapittel 6. OBS: Boken bruker en annen ‘dialekt’ (base R) enn den vi bruker her (tidyverse).",
    "crumbs": [
      "Tutorial 3: Data cleaning & management"
    ]
  },
  {
    "objectID": "tutorial_3.html#select",
    "href": "tutorial_3.html#select",
    "title": "Tutorial 3: Data cleaning & management",
    "section": "3.1 select()",
    "text": "3.1 select()\nThe small ess practice dataset includes 22 variables. Let’s assume that you really need only the ID number (idno), age (agea), and gender (gndr) variables for an analysis. You therefore want to reduce your dataset to these three variables and get rid of all the others.2 The select() function allows you to do that.\n\n3.1.1 Using select()\nHere is how you keep specific variables with select():\n\nselect(.data = ess, idno, agea, gndr)\n\nHere you tell R:\n\nThat you want to select variables with the select() function;\nThat you want to select from the ess dataset (with .data = ess). It is important that you do not forget to add a dot before data;3\nThen you simply list the variables you want to select, separated by commas;\n\n\n\n3.1.2 Saving the result\nIf you only run the function, R will do the operation and then simply print the result out for you — and the result is then the “trimmed” dataset. This can be helpful if you just want to test if your code works, but you usually want to store the reduced dataset so you can use it in your analysis.\nYou can save the resulting “trimmed” dataset as a new object using the good old assignment operator:\n\ness_selected &lt;- select(.data = ess, idno, agea, gndr)\n\n\n\n3.1.3 Removing (de-selecting) variables\nNow you know how you can keep certain variables in a dataset and get rid of all others. But you can also use select() to remove specific variables but leave the rest of the dataset as it is. To do this, you simply add a minus symbol (-) before the variables you want to get rid off.\nFor example, to remove the agea and gndr variables — and keep all the others — you would run:\n\nselect(.data = ess,-agea,-gndr)",
    "crumbs": [
      "Tutorial 3: Data cleaning & management"
    ]
  },
  {
    "objectID": "tutorial_3.html#filter",
    "href": "tutorial_3.html#filter",
    "title": "Tutorial 3: Data cleaning & management",
    "section": "3.2 filter()",
    "text": "3.2 filter()\nFiltering observations works basically the same way, the only difference is that you have to specify how or by which criteria you want to select observations from the data.\nFor example, let’s assume you wanted to remove all those observations from the ess dataset where respondents were younger than 40 years. This is how you would do this with filter():\n\nfilter(.data = ess,agea&gt;=40)\n\nIn human language:\n\nYou tell R that you want to filter observations from the ess dataset\nYou specify a condition using mathematical symbols (&gt;=): Keep all those observations where the respondent’s age is equal to 40 or greater (agea&gt;=40)\n\nThe expression &gt;= stands, as you probably know, for “greater than or equal to”. It is one of several you can use to filter your data:\n\n&gt; “greater”\n&lt; “smaller”\n&lt;= “smaller or equal to”\n&gt;= “greater or equal to”\n== “must be equal to” (the double equal sign means we are extra sure here)\n!= “must not be equal to” (generally, ! stands for “is not”)\n%in% “is included in”, usually followed by a vector (e.g., cntry %in% c(\"Norway\",\"Sweden\",\"Denmark\"))4\n\nYou can also specify multiple conditions in filter(). For example, to limit the data to women who are older than 35 you would do the following:\n\nfilter(.data = ess, agea&gt;35 & gndr==\"Female\")\n\nIn human language:\n\nYou want respondents older than 35 (agea&gt;35)\nYou want only women (gndr==\"Female\")\nYou make clear that both conditions have to be fulfilled at the same time with the & (“ampersand”) symbol.\n\nYou can save the result with the assignment operator (&lt;-) as shown above with select().",
    "crumbs": [
      "Tutorial 3: Data cleaning & management"
    ]
  },
  {
    "objectID": "tutorial_3.html#storing-the-result-of-your-pipeline",
    "href": "tutorial_3.html#storing-the-result-of-your-pipeline",
    "title": "Tutorial 3: Data cleaning & management",
    "section": "4.1 Storing the result of your “pipeline”",
    "text": "4.1 Storing the result of your “pipeline”\nAs before, you can save the result of your “data cleaning pipeline” as a new dataset in your Environment. This is useful when you want to prepare a cleaned and trimmed version of the original “raw” dataset that you can then use in your statistical analysis.\nAlternative 1 is to use the standard assignment operator (&lt;-):\n\ness_clean &lt;- ess %&gt;% # saves the \"trimmed\" dataset as 'ess_clean'\n  select(idno, agea, gndr) %&gt;%\n  filter(gndr==\"Female\" & agea&gt;35)\n\nThere is also a second alternative, in which you use the “reversed” assignment operator (-&gt;):\n\ness %&gt;%\n  select(idno, agea, gndr) %&gt;%\n  filter(gndr==\"Female\" & agea&gt;35) -&gt; ess_clean # as above, but now at the end of the pipeline\n\nFinally, and as with the regular assingment operator, it can be a good idea to create a keyboard shortcut for the pipe operator to make typing easier (in RStudio, go to “Options”, then “Code”, and then “Modify keyboard shortcuts”).",
    "crumbs": [
      "Tutorial 3: Data cleaning & management"
    ]
  },
  {
    "objectID": "tutorial_3.html#the-new-pipe-operator",
    "href": "tutorial_3.html#the-new-pipe-operator",
    "title": "Tutorial 3: Data cleaning & management",
    "section": "4.2 The new pipe operator (|>)",
    "text": "4.2 The new pipe operator (|&gt;)\nThe “traditional” pipe operator (%&gt;%) that we used so far is a part of the tidyverse — and it will only work if you have the tidyverse or one of the central packages (dplyr, ggplot2) loaded.\nWith the release of R version 4.1.0, a new pipe operator was introduced: |&gt;. This one works like the old one, but is “native” to R — you shouldn’t need to load packages to be able to use it.\nWhile there are some differences in how they work, you can generally use either of them (or both) without running into any problems or getting wrong results.5\n\n\n\n\n\n\nImportant\n\n\n\nThe two pipe operators may not work with all functions in R. They should work fine with functions that come from the tidyverse package collection (select(), filter(), drop_na(),…) and they work also with some other functions, but this is not always the case. If you notice that your “pipeline” breaks when you add a particular function, then it is best if you just store the result after the last functioning step of your pipeline and then use that result with the “offending” function separately.6",
    "crumbs": [
      "Tutorial 3: Data cleaning & management"
    ]
  },
  {
    "objectID": "tutorial_3.html#simple-transformations-with-numeric-variables",
    "href": "tutorial_3.html#simple-transformations-with-numeric-variables",
    "title": "Tutorial 3: Data cleaning & management",
    "section": "5.1 Simple transformations with numeric variables",
    "text": "5.1 Simple transformations with numeric variables\nThe most basic way to transform a variable is to do a simple mathematical transformation. For example, let’s say you wanted to work with the height variable from the ess dataset. This variable records the respondents’ body heights in centimeters. When you run summary(ess$height), you see that this variable ranges from 147cm (the shortest person) to 196cm (the tallest person).\nBut you decide, for some reason, that you want that variable measured in meters, and not in centimeters. To get there, you have to divide the height variable by 100.\nHere is how you can do this with mutate() (plus again the pipe operator):\n\ness %&gt;% # 1.\n  mutate(height_meters = height/100) # 2.\n\nOnce more in human language:\n\n“Take the ess dataset…”\n“…and mutate height into a new variable, height_meters, by dividing height by 100.”\n\nObviously, this is just a very simple example and you can certainly take this further — for example by adding, subtracting, or multiplying two or more variables or doing more complex mathematical transformations. To get an overview over what you can do with mutate(), see the official help page: https://dplyr.tidyverse.org/reference/mutate.html#useful-mutate-functions",
    "crumbs": [
      "Tutorial 3: Data cleaning & management"
    ]
  },
  {
    "objectID": "tutorial_3.html#advanced-mutate-dummy-coding-variables",
    "href": "tutorial_3.html#advanced-mutate-dummy-coding-variables",
    "title": "Tutorial 3: Data cleaning & management",
    "section": "5.2 Advanced mutate(): “Dummy-coding” variables",
    "text": "5.2 Advanced mutate(): “Dummy-coding” variables\nNext to mathematical transformations, you also often have to “dichotomize” or “dummy-code” one or more of your variables. “Dummy-coding” means that you turn a more complex variable into a simple yes/no or (1/0) “dummy” variable. To dummy-code a variable, you use the if_else() function within mutate().7\nYou can do this with numeric variables (e.g., age or years spent in education) but also categorical or ordinal variables that have more than two categories (and which are stored as Factors). The process differs only a bit between the two scenarios.\n\n5.2.1 Dummy-coding a numeric variable\nThis first example shows you how you can dummy-code a variable that is numeric. In this case, we use the age-variable agea, which measures the respondents’ ages in years, and we dummy-code it into a categorical variable that identifies all those respondents in our practice dataset who are older than 65 years.\nIn other words, the task is to create a new variable that identifies older respondents. This new variable would have the value 1 whenever a respondent in the dataset is older than 65, and it would have the value 0 for all those respondents who are younger. You can create this variable on the basis of the agea variable with the handy if_else() function.\nIn practice, the code to do this would look like this:\n\ness %&gt;% # 1.\n  mutate(older = if_else(condition = agea&gt;65, # 2.\n                         true = 1, # 3.\n                         false = 0)) # 4.\n\nThe mutate-call in human language:\n\n“Take the ess dataset and…\n“…create (”mutate”) a new variable called older based on the condition that the respondent’s age is greater than 65 (agea&gt;65).”\n“If that condition is true, the new variable older gets the value of 1…”\n“…and if that condition is false, older gets the value of 0.”\n\n\nTo show you more clearly what a dummy-coded variable looks like and does, here is a cleaned-up result of the operation shown above:\n\n    idno agea older\n1  12414   22     0\n2   9438   43     0\n3  19782   58     0\n4  18876   22     0\n5  20508   84     1\n6  19716   62     0\n7  13476   68     1\n8   6762   81     1\n9  19518   59     0\n10 21336   57     0\n\nWhat you see here is a small part of the ess dataset with the first ten observations and only the idno and agea variables plus the new older variable. You should directly see how the older variable corresponds to the agea variable: Whenever a given respondent’s age is greater than 65 years, older has the value of 1; otherwise, older is 0.\nIf you wanted to use the new variable in your analysis, you would obviously have to store the result with either the regular assignment operator (&lt;-) or the reversed version (-&gt;) as shown earlier. Otherwise, R will only print out the entire ess dataset with all existing variables plus the new one (older) that the code creates.\n\n\n\n5.2.2 Dummy-coding an ordinal or categorical variable\nOften, you want to dummy-code a variable that is not numeric like agea but categorical or ordinal, and which is stored as a Factor in R. This is of course also possible, but the process is slightly different.\nTo show you how this works, we will use the health variable that is included in the ess practice dataset. This variable measures how respondents subjectively perceive their own health on an ordinal scale. The categories on that scale are “Very good”, “Good”, “Fair”, “Bad”, and “Very bad”.\nYou can also see this when you use the attributes() function:\n\nattributes(ess$health)\n$levels\n[1] \"Very good\" \"Good\"      \"Fair\"      \"Bad\"       \"Very bad\" \n\n$class\n[1] \"factor\"\n\nUnder $levels, you see the different categories. Under $class, you see that it is stored as a factor-type variable (as it should be!).\nAssume now that we want to create a new variable that is based on health, and the new variable should identify those respondents in our dataset that perceive their own health to be at least “good”. In other words, we want to dummy-code the health variable into a new variable that identifies respondents who have a good or very good subjective health.\nHere again, we use if_else() within mutate(), but we now need to specify the condition a bit differently:\n\ness %&gt;%  # 1.\n  mutate(health_dummy = if_else(condition = health %in% c(\"Very good\",\"Good\"), # 2. \n                               true = \"Good health\", # 3.\n                               false = \"Not good health\")) # 4.\n\nTranslated into “human”, the code tells R to:\n\n“Take the ess dataset and…”\n“…create a new variable called health_dummy based on the condition that the health-variable has either the value ‘Very good’ or the value ‘Good’. (Notice that we use the %in% operator here to indicate that health should be eiter”Very good” or “Good”.)”\n“If that condition is true, the new variable gets the value ‘Good health’,…”\n“…and if that condition is false, the variable gets the value ‘Not good health’.\n\nAnd, as before, if you wanted to use the new variable in your analysis, you would need to store the new version of the dataset with &lt;- or -&gt;. Otherwise, R will only print out the entire dataset with all the variables, old and new.\n\nAnd just to show you again what the new variable does in this case, here is a cleaned-up version of the result of the code shown above:\n\n    idno    health    health_dummy\n1  12414      Good     Good health\n2   9438 Very good     Good health\n3  19782      Good     Good health\n4  18876 Very good     Good health\n5  20508 Very good     Good health\n6  19716      Fair Not good health\n7  13476      Fair Not good health\n8   6762      Fair Not good health\n9  19518 Very good     Good health\n10 21336      Good     Good health\n\nYou see that all respondents who felt that their health was either “very good” or “good” got the value “Good health” on the new variable. The three respondents who judged their own health to be only “fair” got the value “Not good health”, and this would obviously be the same for all respondents that rated their own health as “bad” or “very bad”.\n\nGood to know: When you use if_else(), you can decide what type of variable the new dummy-coded variable will be. If you use numbers (like in the first example where we dummy-coded agea), the new variable will be numeric. If you use text (like in the second example), the new variable will be a character-type variable. You can then transform this variable to a factor with as.factor(), if you like (see also below for details).",
    "crumbs": [
      "Tutorial 3: Data cleaning & management"
    ]
  },
  {
    "objectID": "tutorial_3.html#simple-summary-statistics",
    "href": "tutorial_3.html#simple-summary-statistics",
    "title": "Tutorial 3: Data cleaning & management",
    "section": "6.1 Simple summary statistics",
    "text": "6.1 Simple summary statistics\nTo start with a simple example, let’s say you are interested in the average body height of your respondents. In the ess dataset, this is measured via the height variable.\nYou know, of course, that you can get the average of height with the mean() function:\n\nmean(ess$height,na.rm = T)\n[1] 173.7606\n\nHere is how you would do it the tidyverse-way:\n\ness %&gt;% \n  summarize(mean_height = mean(height, na.rm = TRUE))\n  mean_height\n1    173.7606\n\nWhat you do here is, in essence, the same as the above: You use the mean() function to calculate the average of the height variable. The only differences are that a) you save the result temporarily into a new variable (mean_height), and b) you do not have to use the $ sign to tell R where to take the height variable from because you already do that in the very first step (ess %&gt;%).\nIf you now think that the second option is really just a more complicated and cumbersome form of the first one: True, in principle.\nBut the second option has the great advantage that it can be extended — for example to calculate summary statistics over the categories of some other variable. How you do this comes below.",
    "crumbs": [
      "Tutorial 3: Data cleaning & management"
    ]
  },
  {
    "objectID": "tutorial_3.html#combining-summarize-group_by",
    "href": "tutorial_3.html#combining-summarize-group_by",
    "title": "Tutorial 3: Data cleaning & management",
    "section": "6.2 Combining summarize() & group_by()",
    "text": "6.2 Combining summarize() & group_by()\nYou can use the group_by() function to group your dataset by some other variable before you calculate any summary statistics.\nFor example, let’s say you wanted to calculate the average body height for men and women separately. This is how you would do this with group_by() and summarize():\n\ness %&gt;% # 1.\n  group_by(gndr) %&gt;% # 2.\n  summarize(mean_height = mean(height, na.rm = TRUE)) # 3.\n# A tibble: 2 × 2\n  gndr   mean_height\n  &lt;fct&gt;        &lt;dbl&gt;\n1 Male          179.\n2 Female        168.\n\nIn human language:\n\n“Take the ess dataset…”\n“…group the data by gender (gndr)…”\n“…and finally calculate the average height for each of the two groups. Save the result temporarily into a new variable called mean_height.”",
    "crumbs": [
      "Tutorial 3: Data cleaning & management"
    ]
  },
  {
    "objectID": "tutorial_3.html#other-statistics-multiple-statistics-in-one-operation",
    "href": "tutorial_3.html#other-statistics-multiple-statistics-in-one-operation",
    "title": "Tutorial 3: Data cleaning & management",
    "section": "6.3 Other statistics & multiple statistics in one operation",
    "text": "6.3 Other statistics & multiple statistics in one operation\n\n6.3.1 Other summary statistics\nThe previous examples showed you how to calculate the mean value of a variable — but you can of course also calculate other summary statistics such as the variance, median, sum, or standard deviation.\nFor example, to calculate the median age across genders you would run:\n\ness %&gt;% \n  group_by(gndr) %&gt;% \n  summarize(med_age = median(agea, na.rm = TRUE)) \n# A tibble: 2 × 2\n  gndr   med_age\n  &lt;fct&gt;    &lt;dbl&gt;\n1 Male      51  \n2 Female    42.5\n\nSimilarly, if you wanted to know the number (N) of men and women in the sample you would use the following code:\n\ness %&gt;% \n  group_by(gndr) %&gt;% \n  summarize(obs = n()) \n# A tibble: 2 × 2\n  gndr     obs\n  &lt;fct&gt;  &lt;int&gt;\n1 Male      75\n2 Female    68\n\nn() simply calculates the number of observations.\n\n\n6.3.2 Multiple summary statistics\nYou can also get multiple summary statistics at the same time. All you need to do is to add to the summarize() call:\n\ness %&gt;% \n  group_by(gndr) %&gt;% \n  summarize(obs = n(),\n            med_age = median(agea, na.rm = T),\n            mean_weight = mean(weight, na.rm = T))\n# A tibble: 2 × 4\n  gndr     obs med_age mean_weight\n  &lt;fct&gt;  &lt;int&gt;   &lt;dbl&gt;       &lt;dbl&gt;\n1 Male      75    51          86.8\n2 Female    68    42.5        68.9\n\nLast but not least, a very useful way to extend these operations is to directly visualize the results in a graph using ggplot2. You will learn how to do this in the next tutorial.\n\nA heads-up: This last part is a bit technical and you have already done quite a lot, so maybe take a quick break before doing this.",
    "crumbs": [
      "Tutorial 3: Data cleaning & management"
    ]
  },
  {
    "objectID": "tutorial_3.html#identifying-variable-types",
    "href": "tutorial_3.html#identifying-variable-types",
    "title": "Tutorial 3: Data cleaning & management",
    "section": "7.1 Identifying variable types",
    "text": "7.1 Identifying variable types\nYou also know already how to recognize different types, for example by looking at the description in the Environment tab.\nIn addition to the information in the Environment tab, you can also use specific functions to identify the type of a variable in a dataset (or, really, any other object in your workspace) with the class() function — you may remember this from the previous tutorial.\nFor example, running class(ess$cntry) will tell you that the cntry variable is of type ‘character’ (chr).",
    "crumbs": [
      "Tutorial 3: Data cleaning & management"
    ]
  },
  {
    "objectID": "tutorial_3.html#changing-storage-types",
    "href": "tutorial_3.html#changing-storage-types",
    "title": "Tutorial 3: Data cleaning & management",
    "section": "7.2 Changing storage types",
    "text": "7.2 Changing storage types\nAs mentioned earlier, it can happen that one or more of the variables in your dataset are not stored correctly. For example, a variable that really consists of pure numbers was somehow converted to a text variable during the data import process. In that case, you need to be able to transform your variable into its proper storage type.\nWhat this means in practice is again easiest to see by looking at an example. Let’s say that, because you are feeling silly today, you want the age variable (agea) not stored as numbers but as text. In other words, you want to convert this variable from type ‘numeric’ to type ‘character’.\nTo do this, you would use the as.character() transformation function:\n\nas.character(ess$agea)\n  [1] \"22\" \"43\" \"58\" \"22\" \"84\" \"62\" \"68\" \"81\" \"59\" \"57\" \"85\" \"24\" \"43\" \"35\" \"59\"\n [16] \"52\" \"56\" \"69\" \"32\" \"63\" \"18\" \"56\" \"53\" \"53\" \"57\" \"40\" \"40\" \"81\" \"19\" \"21\"\n [31] \"43\" \"67\" \"66\" \"43\" \"26\" \"56\" \"82\" \"18\" \"35\" \"73\" \"29\" \"56\" \"65\" \"56\" \"17\"\n [46] \"67\" \"23\" \"50\" \"62\" \"41\" \"46\" \"62\" \"27\" \"62\" \"32\" \"69\" \"64\" \"31\" \"68\" \"58\"\n [61] \"31\" \"71\" \"79\" \"55\" \"35\" \"34\" \"62\" \"53\" \"51\" \"62\" \"42\" \"44\" \"58\" \"46\" \"38\"\n [76] \"35\" \"53\" \"56\" \"66\" \"40\" \"44\" \"60\" \"60\" \"20\" \"71\" \"17\" \"21\" \"58\" \"90\" \"32\"\n [91] \"41\" \"54\" \"38\" \"56\" \"39\" \"61\" \"32\" \"32\" \"39\" \"33\" \"47\" \"41\" \"17\" \"42\" \"23\"\n[106] \"76\" \"36\" \"74\" \"23\" \"55\" \"18\" \"43\" \"28\" \"44\" \"44\" \"38\" \"48\" \"38\" \"48\" \"41\"\n[121] \"75\" \"78\" \"19\" \"24\" \"24\" \"40\" \"75\" \"50\" \"72\" \"40\" \"70\" \"34\" \"59\" \"67\" \"17\"\n[136] \"87\" \"65\" \"71\" \"39\" \"33\" \"32\" \"16\" \"25\"\n\nYou could of course also directly add this new variable to the ess dataset with the assignment operator:\n\ness$age_chr &lt;- as.character(ess$agea)\n\nTake a look at the result above: Do you notice the quotation marks around all of the numbers that R printed out? This indicate that you transformed agea into a character variable: The numbers are still there — but they are now stored as text. R will now refuse to do any calculations with this variable.\nFor example, if you try to calculate the mean of this new variable, you will get an error message:\n\nmean(ess$age_chr, na.rm = T)\nWarning in mean.default(ess$age_chr, na.rm = T): argument is not numeric or\nlogical: returning NA\n[1] NA\n\n\nNow you know what the problem often looks like — there is a variable that you know is supposed to be a numeric variable, but it somehow got stored as text. In that case, you have to tell R that it should treat this variable as a proper numeric variable. This works equivalently to the previous operation, but with a different function — as.numeric():\n\n# Transform the age_chr variable we just created into a new one and store in ess\ness$age_num &lt;- as.numeric(ess$age_chr) \n \n# This should work now\nmean(ess$age_num, na.rm = TRUE)\n[1] 47.90909\n\nThis might seem like much ado about nothing, but knowing this can really save you a lot of time and headaches. The important point: Be conscious of how your data are stored in R, and if how it is stored really makes sense. If it does not, convert your variables into an appropriate format.",
    "crumbs": [
      "Tutorial 3: Data cleaning & management"
    ]
  },
  {
    "objectID": "tutorial_3.html#data-cleaning-and-transformations-with-factors",
    "href": "tutorial_3.html#data-cleaning-and-transformations-with-factors",
    "title": "Tutorial 3: Data cleaning & management",
    "section": "7.3 Data cleaning and transformations with factors",
    "text": "7.3 Data cleaning and transformations with factors\nCategorical or ordinal variables that are stored as factors can cause headaches during the data cleaning and management phase, often simply because they are more complex than pure numeric or character variables. In this last part of the tutorial, you will learn a few tricks that can help you deal with factor variables.\n\n7.3.1 Factor to numeric\nAssume you were interested in people’s level of satisfaction with life and you therefore wanted to do a statistical analysis with the stflife variable from the ess dataset, which measures exactly this. You also see that this variable has 11 categories — from 0 (“Extremely dissatisfied”) to 10 (“Extremely satisified”) — which are enough to be used as a numeric or “metric” variable:\n\nvisfactor(variable = \"stflife\", dataset = ess)\n values                 labels\n      1 Extremely dissatisfied\n      2                      1\n      3                      2\n      4                      3\n      5                      4\n      6                      5\n      7                      6\n      8                      7\n      9                      8\n     10                      9\n     11    Extremely satisfied\n\nBut: R will not do any math with the variable in this form. If you would try to calculate the average level of life satisfaction you get an error message:\n\nmean(ess$stflife, na.rm = T)\nWarning in mean.default(ess$stflife, na.rm = T): argument is not numeric or\nlogical: returning NA\n[1] NA\n\nThe problem: stflife is stored as a factor, which you can see when you you check how R stored it:\n\nclass(ess$stflife)\n[1] \"factor\"\n\n\nFortunately, you can – in principle – directly extract the numerical scores from a factor-type variable like sftlife into a new numeric variable with as.numeric().\nBut there is one thing you need to be careful about: Take another look at how the labels and underlying numbers of stflife correspond:\n\nvisfactor(variable = \"stflife\", dataset = ess)\n values                 labels\n      1 Extremely dissatisfied\n      2                      1\n      3                      2\n      4                      3\n      5                      4\n      6                      5\n      7                      6\n      8                      7\n      9                      8\n     10                      9\n     11    Extremely satisfied\n\nIf you take a careful look at the result here you should notice that the values and labels are off by 1: “Extremely dissatisfied” – which corresponds to 0 – has the underlying value of 1, the label “1” has the underlying value of 2, and so on. In other words, the values are wrong.\nThis is a problem because when you use as.numeric(), R will extract the values from stflife – and because the values are wrong, all results based on them will also be wrong.\nBut: there is an easy way to fix this: You just subtract 1 from the result of as.numeric() and then save the result of this as a new variable:\n\ness$stflife_num &lt;- (as.numeric(ess$stflife) - 1)\n\nIf this worked, then the new numeric version of stflife should have a maximum value of 10 – corresponding to the 0-10 scale that the respondents saw when they participated in the survey. And that is now indeed the case:\n\nmax(ess$stflife_num)\n[1] 10\n\nThis also means that this numeric version of stflife will give you correct results, for example the average value:\n\nmean(ess$stflife_num, na.rm = T)\n[1] 7.86014\n\n\nThe lesson to be learned: You need to really pay attention when you convert factor-type variables to numeric ones! In general: Never run on autopilot, always remain aware of what you are doing to your data!\nHere is a simple and quick checklist you can use to make sure that you are converting factor-type variables correctly to numeric:\n\nUse bst290::visfactor() to let R show you how the labels and underlying values correspond.\nIf the labels and values directly correspond – a label of 0 has the value of 0, and so on – then you can just directly use as.numeric()\nIf the labels and values do not correspond – a label of 0 has the value of 1, and so on – then you need to adjust the values by hand, e.g., by subtracting or adding the necessary number so that the values are correct.\n\n\n\n\n7.3.2 Factor to character\nA related problem you might have is that you want not the numerical scores but the text labels of a factor variable. For example, say you wanted to extract the labels for the different educational degrees in Norway from the edlvdno variable into a new pure character variable.8\nTo create a new variable that contains only the text labels from edlvdno, you use the as.character() function:\n\ness$edlv_chr &lt;- as.character(ess$edlvdno)\n\nYou can verify that the new variable really is a character variable with:\n\nclass(ess$edlv_chr)\n[1] \"character\"\n\nAnd you can see the different levels with unique():\n\nunique(ess$edlv_chr)\n [1] \"Fullført 3-4 årig utdanning fra høgskole (Bachelor-, cand.mag., lærerhøgsko\"    \n [2] \"Fullført 5-6 årig utdanning fra høgskole (master, hovedfag)\"                    \n [3] \"Universitet/høgskole, mindre enn 3 år, men minst 2 år (høgskolekandidat, 2-\"    \n [4] \"Fullført 5-6 årig utdanning fra universitet (master, hovedfag), lengre profesj\" \n [5] \"Vitnemål fra påbygging til videregående utdanning (fagskoleutdanning, teknisk\"  \n [6] \"Videregående avsluttende utdanning, yrkesfaglige studieretninger/utdanningsprog\"\n [7] \"Ungdomsskole (grunnskole, 7-årig folkeskole, framhaldsskole, realskole)\"        \n [8] \"Fullført 3-4 årig utdanning fra universitet (Bachelor, cand.mag.)\"              \n [9] \"Forkurs til universitet/høgskole som ikke gir studiepoeng\"                      \n[10] \"Videregående avsluttende utdanning, allmennfaglige studieretninger/studieforber\"\n[11] \"Barneskole (første del av obligatorisk utdanning)\"                              \n[12] \"Vitnemål fra folkehøgskole\"                                                     \n\n\n\n\n7.3.3 More tools for working with factors\nYou probably see now that working with factors can be a bit tedious, simply because they are a bit more complex than other types of variables. But you hopefully also see their structure — numbers with text labels — more clearly now that you have seen how you can extract the different elements with as.numeric() and as.character().\nIf you find yourself working with factors a lot, you will probably want to use the forcats package. This package is specifically designed for data cleaning and management with factors and is also included in the tidyverse collection. See then also Hadley Wickham’s R for Data Science.",
    "crumbs": [
      "Tutorial 3: Data cleaning & management"
    ]
  },
  {
    "objectID": "tutorial_3.html#footnotes",
    "href": "tutorial_3.html#footnotes",
    "title": "Tutorial 3: Data cleaning & management",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThink back to last week: Maybe you remember that the full ESS dataset included more than 600 variables?↩︎\nThe idno variable does not really contain substantive information about respondents, but it is good practice to keep this variable because it can come in handy later on — for example, in case you want to add other variables.↩︎\n“Why?”, you may ask? This is how the function was designed by its author, Hadley Wickham, and the detailed answer for why he did this is quite technical, see: https://design.tidyverse.org/dots-prefix.html. Not to worry, you will get around this in just a bit!↩︎\nThis is a bit more advanced, but very useful in practice!↩︎\nSee also https://www.tidyverse.org/blog/2023/04/base-vs-magrittr-pipe/.↩︎\nAlternatively, see this RStudio Community board discussion for a solution: https://community.rstudio.com/t/pipe-operator-does-not-work/66377↩︎\nif_else() is a newer version of the similar ifelse() function that is built into R from the start. Both work essentially in the same way, but if_else() is specifically designed for mutate() and also a bit stricter — which helps you avoid errors.↩︎\nYou already know from the previous tutorial that edvldno is a factor, but feel free to check again with class(ess$edlvdno).↩︎",
    "crumbs": [
      "Tutorial 3: Data cleaning & management"
    ]
  },
  {
    "objectID": "tutorial_0.html#about-r",
    "href": "tutorial_0.html#about-r",
    "title": "Tutorial 0: Installation & Setup",
    "section": "1.1 About R",
    "text": "1.1 About R\nR is one of the most popular software tools for data analysis and data science. R can be used for “traditional” quantitative data analysis (e.g., regression analysis), but also much more: Quantitative text analysis, web scraping, interactive data visualization, Big Data analysis, and machine learning.\nR is what professional data analysts, data scientists, and researchers use in their work. It is used in academia, but also at government agencies, news channels (e.g., the BBC), and private companies (e.g., YouGov, Netflix, or Meta/Facebook). If you want to get an idea of who uses R and for what, you can check out the example job ads on Canvas (under “Why do I have to learn this?!”). Knowing how to work with R is a big advantage when you look for jobs — and this is one reason for why we want you to learn it.\nR is also a really good tool to have for your studies. The work for your BA and perhaps MA thesis will get considerably easier if you are able to quickly download and analyze a dataset — no hours of transcribing interviews or digging through dusty archives.\nLast but not least, R is open-source and free.\nWorking with R means writing computer code. You will not be able to run your analyses by clicking your way through menus or dragging and dropping things around, as you would do when working with Microsoft Word etc. Obviously, this may come as quite a change for you and may even feel strange.\nBut: R can be learned. Others have done it before, and there are many who start using R who have no prior experience with programming or computer languages. Still, it might take some time before you are really used to it.\nThink about learning R like learning a foreign language: The start is often very confusing, and things that are really simple feel really difficult. The trick to getting better is to just keep practicing and not giving up. If you do that, you will soon feel that you are making progress!",
    "crumbs": [
      "Tutorial 0: Installation & Setup"
    ]
  },
  {
    "objectID": "tutorial_0.html#about-rstudio",
    "href": "tutorial_0.html#about-rstudio",
    "title": "Tutorial 0: Installation & Setup",
    "section": "1.2 About RStudio",
    "text": "1.2 About RStudio\nWorking with R has become quite a bit easier with the arrival of RStudio a while ago. Where R by itself gives you only a bland and empty computer code Console and an output window to work with, RStudio provides you with a more comfortable interface (or “integrated development environment”, IDE, to use the official term) that shows you which datasets and variables you are currently working with, gives you direct access to help files, and helps you with many other tasks (but you will still have to write code!). The basic version of RStudio is also open-source and free to use.",
    "crumbs": [
      "Tutorial 0: Installation & Setup"
    ]
  },
  {
    "objectID": "tutorial_0.html#what-you-see",
    "href": "tutorial_0.html#what-you-see",
    "title": "Tutorial 0: Installation & Setup",
    "section": "3.1 What you see",
    "text": "3.1 What you see\nLet’s quickly go over what you are seeing:\n\nThe large window on the left is the Console. This is where you can enter your commands and this is also where your results (other than graphs) will be shown (you can ignore the other tabs for now).\nThe smaller window in the upper right corner shows you your current working environment. It is currently empty but will later show you a list of all the variables, datasets, and other “objects” that are loaded at any given point. (You may also see that this window has other tabs labeled “History”, “Connections”, and “Tutorial”. You can also ignore these for now.)\nThe smaller window in the lower right corner is where any graphs (“Plots”) you create later will be shown, and this is also where the “Help” files appear if you request them. (You can also access and open “Files” on your computer, and see a list of add-on packages that you can install. These tabs are also not that important right now.)\n\nYou can customize how RStudio looks via the “Options” menu (yes, there is a regular click-based menu!). You can open the “Options” menu via the taskbar at the top (“Tools” -&gt; “Global Options…”). When the menu has popped up, navigate to “Appearance”. You can select a different theme and adjust other settings there.",
    "crumbs": [
      "Tutorial 0: Installation & Setup"
    ]
  },
  {
    "objectID": "tutorial_0.html#where-is-r",
    "href": "tutorial_0.html#where-is-r",
    "title": "Tutorial 0: Installation & Setup",
    "section": "3.2 Where is R?",
    "text": "3.2 Where is R?\nYou may be wondering why you had to install R when you are now working in RStudio and not in R. Technically, you are already working with R. You can see that written in the output that appears in the Console window. It should read something like:\n\nR version 4.4.1 (2024-06-14) -- \"Race for Your Life\"\nCopyright (C) 2024 The R Foundation for Statistical Computing\nPlatform: x86_64-apple-darwin20\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\nWhen you run RStudio, R is running “under the hood”. You can think of RStudio as the instrument panel and steering wheel in a car, and R as the car’s engine.",
    "crumbs": [
      "Tutorial 0: Installation & Setup"
    ]
  },
  {
    "objectID": "tutorial_0.html#avoiding-chaos",
    "href": "tutorial_0.html#avoiding-chaos",
    "title": "Tutorial 0: Installation & Setup",
    "section": "4.1 Avoiding chaos",
    "text": "4.1 Avoiding chaos\nThere is one important bit of housekeeping you need to do now to reduce chaos and confusion when working with R.\n\nOpen the main options menu in RStudio (click on “Tools” in the taskbar at the top, and then open the “Global Options” menu at the bottom of the list).\nUnder “Workspace” (shown right in the middle of the first page of the menu):\n\nUntick/deactivate “Restore .RData into workspace at startup”.\nIn the dropdown menu next to “Save workspace to .RData on exit”, select “Never”.\n\n\nYour settings should now look like those in the screenshot below.\n\n\n\nCorrect menu settings\n\n\nWhen you are done, click “OK” to close the menu.",
    "crumbs": [
      "Tutorial 0: Installation & Setup"
    ]
  },
  {
    "objectID": "tutorial_0.html#keeping-everything-organized-projects-and-working-directories",
    "href": "tutorial_0.html#keeping-everything-organized-projects-and-working-directories",
    "title": "Tutorial 0: Installation & Setup",
    "section": "4.2 Keeping everything organized: Projects and working directories",
    "text": "4.2 Keeping everything organized: Projects and working directories\nOnce you really start working with R, you will use a lot of different files: Dataset files go into R, and all the different graphs and tables that present the results of your analysis come out again. In addition, you will have one or more code files that document your code (more on that comes below). Obviously, you need to keep an overview over the different files and where they are on your computer.\nRStudio offers you a great way to keep all these files organized: Projects. A project automatically organizes all the files you need to work with in a given context (e.g., this course) and makes sure that any outputs you create (e.g., graphs) are stored somewhere where you can find them.\n\n\n\n\n\n\nImportant\n\n\n\nYou should have one (and only one) project for this course in RStudio. This project includes all the files (dataset files, code files, output) that you work with during this course. All these files should be stored in one (and only one) folder on your computer. Your project will be associated with that folder.\n\n\nFollow these steps to create the project:\n\nOpen File Explorer (on Windows) or Finder (on Mac) and create a folder on your computer where you want to store all the R-related files for this course (for example as Documents/Uni/3rd_semester/BST290/R_files).\nThen go back to RStudio. If you look at the top right corner of your RStudio window, you should see a little blue box symbol and “Project: (None)” written next to it (see also the screenshot on the following page). This is where you can create and open projects.\nClick on that symbol to open a little dropdown menu. Choose New Project…\nIn the menu that pops up, choose Existing Directory.\nUse the Browse… button to select the new folder you just created.\nThen click on Create Project.\n\nRStudio will then restart. When it is done restarting, you should see your project name in the upper right of your screen, next to the blue box symbol. This means that you are now working inside your new project folder. Every time you create a new file (e.g., a graph), that file will be saved in this project folder. In other words, this folder is now your “Working Directory”.1\n\n\n\n\nFinding the Projects menu",
    "crumbs": [
      "Tutorial 0: Installation & Setup"
    ]
  },
  {
    "objectID": "tutorial_0.html#installing-the-tidyverse-package",
    "href": "tutorial_0.html#installing-the-tidyverse-package",
    "title": "Tutorial 0: Installation & Setup",
    "section": "5.1 Installing the tidyverse package",
    "text": "5.1 Installing the tidyverse package\nThe first package you install is called the tidyverse package. Strictly speaking, the tidyverse is an entire collection of different R packages, but all these packages fit neatly together and are therefore bundled in a “package of packages”.\nThe tidyverse packages were designed to make things that are often difficult (data import, data cleaning & management, visualization) a lot easier (see also tidyverse.org). We will use the tidyverse a lot in this course, so it is absolutely necessary that you install it!\nYou can install the tidyverse directly from the main R “app-store”: The Central R Archive Network or CRAN.\nTo install the tidyverse package from CRAN, you just type the following code into the Console and press Enter (see also the screenshot below):\n\ninstall.packages(\"tidyverse\")\n\n\n\n\nThe R Console with code to install a package\n\n\nIf you are connected to the internet, R will now download and install the package automatically. This might take a bit, and a lot of computer code gibberish will appear on your screen — no reason to worry, this is normal!\n\n\n\n\n\n\n\nWarning\n\n\n\nIn some cases, R might ask you questions during the installation process (this applies to all package installations, now and in the future):\n\nIf it asks you to update packages, choose “CRAN packages only” (press 2)\nIf it asks you whether you want to “install from sources that need compilation”, choose “No” (press N)\n\n(Did you by accident choose a wrong answer to one of the questions above? No worries, just re-run the install.packages(\"tidyverse\") command above to start the installation process again.)",
    "crumbs": [
      "Tutorial 0: Installation & Setup"
    ]
  },
  {
    "objectID": "tutorial_0.html#what-you-should-see",
    "href": "tutorial_0.html#what-you-should-see",
    "title": "Tutorial 0: Installation & Setup",
    "section": "5.2 What you should see",
    "text": "5.2 What you should see\nIf all worked out, your Console should say that “the downloaded binary packages are in…”, followed by some gibberish (see also the example screenshot below).\n\n\n\nA successfull package installation (on Mac)\n\n\nThe result will look similar on a Windows computer (but with a bit more gibberish).",
    "crumbs": [
      "Tutorial 0: Installation & Setup"
    ]
  },
  {
    "objectID": "tutorial_0.html#installing-the-bst290-package",
    "href": "tutorial_0.html#installing-the-bst290-package",
    "title": "Tutorial 0: Installation & Setup",
    "section": "5.3 Installing the bst290 package",
    "text": "5.3 Installing the bst290 package\nNext to the tidyverse package, we will also use a package that was specifically designed for this course: The bst290 package (see also github.com/cknotz/bst290). This package includes a small practice dataset and a few tools to easily create descriptive tables for your course assignments. It also includes an interactive dashboard that can help you understand the really tricky statistics-concepts we will cover in this course (the Central Limit Theorem, confidence intervals, statistical tests) better.\nThe bst290 package is not (yet) available from CRAN, but you can install it directly from GitHub. GitHub is a popular platform for computer programmers and researchers to share their code with others, and many R packages are distributed via GitHub.\nInstalling the bst290 package from GitHub involves two steps:\nFirst, you need to install the remotes package — this is used to install packages from “remote” repositories like GitHub. You can install the remotes package directly from CRAN, just like with the tidyverse package earlier.\nYou type the following into the Console and press Enter:\n\ninstall.packages(\"remotes\")\n\nYou will now see a lot more computer code gibberish running over your screen — again, nothing to worry about as long as you get the same result as earlier.\n\n\n\n\n\n\nFor Windows users\n\n\n\nR might give you a warning that you should have something called RTools installed — this is not necessary for now! Just move on. If you want to install RTools anyways, you can do this in a few more steps. You can find an instruction here: https://cran.r-project.org/bin/windows/Rtools/rtools40.html.\n\n\n\nNext, you use the remotes package to download and install the bst290 package from GitHub. To do so, type the following code into the Console and press Enter:\n\nremotes::install_github(\"cknotz/bst290\")\n\nThis might again take a bit and you will probably again see a lot more computer gibberish appear — just hang in there. As before:\n\nIf R asks you to update packages, choose “CRAN packages only” (press 2)\nIf it asks you whether you want to “install from sources that need compilation”, choose “No” (press N)",
    "crumbs": [
      "Tutorial 0: Installation & Setup"
    ]
  },
  {
    "objectID": "tutorial_0.html#installing-the-tinytex-package",
    "href": "tutorial_0.html#installing-the-tinytex-package",
    "title": "Tutorial 0: Installation & Setup",
    "section": "5.4 Installing the tinytex package",
    "text": "5.4 Installing the tinytex package\nThe third add-on package you need to install is called tinytex. tinytex is really nothing more than a tool to install the TinyTex software automatically through RStudio. You need to have TinyTex to be able to create PDF code reports for your assignments.2\nYou install tinytex just like the other packages:\n\ninstall.packages(\"tinytex\")\n\nOnce that is complete, you run the following in your Console to install the TinyTex software:\n\ntinytex::install_tinytex()\n\nThe download and installation may take a few minutes – and there will be more code gibberish – so just hang in there!\nYou may get an error message that looks like this:\n\nFound '/Library/TeX/texbin/tlmgr', which indicates a LaTeX distribution may have existed in the system.\n\nIf that is the case, press N for No and cancel the installation. Most likely, you already have the necessary software installed to create PDF reports.",
    "crumbs": [
      "Tutorial 0: Installation & Setup"
    ]
  },
  {
    "objectID": "tutorial_0.html#loading-packages",
    "href": "tutorial_0.html#loading-packages",
    "title": "Tutorial 0: Installation & Setup",
    "section": "5.5 Loading packages",
    "text": "5.5 Loading packages\nWhen you install a new app on your smartphone, it only gets installed but it does (usually) not open and run automatically. You as the user need to open it yourself.\nThis works similarly in R: Installing a package is Step 1 — you as the R user have to get active if you then want to use the new package.\n\n5.5.1 Loading an entire package with library()\nTo load an installed package, you normally use the library() function. For example, to load the tidyverse package, you use:\n\nlibrary(tidyverse)\n\nWhen you run this code, you will get information about the packages that get loaded (“attached”). As mentioned, the tidyverse is a collection of packages. Therefore, if you load it, it loads a whole set of different packages: ggplot2 (for visualization), dplyr for data management, or stringr for working with text.\nYou will also be told about a number of conflicts. These conflicts are nothing to worry about, at least for now.3\nYou can load the bst290 package the same way:\n\nlibrary(bst290)\n\nThere should now not be any warnings, etc.\n\n\n\n\n\n\nGood to know: Loading only a specific part of a package with the ‘double-colon’ method\n\n\n\nYou can also directly “call” specific functions from a package using the “double-colon” method: package::function(). You can use this directly, without loading the entire package first.\nMaybe you notice that you already used that method earlier, when you installed the bst290 package: remotes::install_github(). In this case, you used just this one specific function from the remotes package.",
    "crumbs": [
      "Tutorial 0: Installation & Setup"
    ]
  },
  {
    "objectID": "tutorial_0.html#footnotes",
    "href": "tutorial_0.html#footnotes",
    "title": "Tutorial 0: Installation & Setup",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can also always check where exactly your working directory currently is by using the getwd() (“get working directory”) command. So, if you enter getwd() into the Console and press the Enter-key, you will be shown the file path to the folder that you are currently working in — where all your graphs, tables, etc. get put in when you export them from R. You can also manually change your working directory with setwd(). You just need to add a valid file path within the parentheses. For example, this would be file path that I could use on my own computer: setwd(\"\"/Users/carloknotz/Documents/Work/Teaching/BST290/R_Tutorials\").↩︎\nTinyTex is a small version of the LaTeX typesetting software on your computer, which RStudio then uses to create the code reports (see also https://yihui.org/tinytex/).↩︎\nWhat this tells you is that there are certain commands (“functions”) in the packages you just loaded that have the same name as some other functions that are in use. For example, dplyr::filter() masks stats::filter() means: “There is a function called filter() in the dplyr package that was just loaded. That function has the same name as another filter() function, which is included in the stats package that comes with R right out of the box. dplyr was loaded last, so if you now use the filter() function, the one from the dplyr package will be used.”↩︎",
    "crumbs": [
      "Tutorial 0: Installation & Setup"
    ]
  },
  {
    "objectID": "tutorial_7a.html",
    "href": "tutorial_7a.html",
    "title": "Tutorial 7a: t-test",
    "section": "",
    "text": "The previous tutorial showed you how you can analyze relationships between two categorical variables (e.g., gender and joining a trade union). But, obviously, sometimes we are interested in relationships where the independent variable is binomial (has two distinct categories) and the dependent variable is numeric. The gender wage gap (gender and income) is one example, but there are also many others.1\nThis tutorial will show you how you study such relationships between binomial and numerical variables using R. More specifically, we will see if gender is related to religiosity: If women are more religious than men (or vice versa).\nAs in the previous tutorial, we will start right away with the real ESS dataset, but you will use the small practice dataset in the de-bugging exercises.\n\n\n\n\n\n\nTip\n\n\n\nHvis du ønsker å lese en norsk tekst i tillegg: “Lær deg R”, Kapittel 5.3.2",
    "crumbs": [
      "Tutorial 7a: *t*-test"
    ]
  },
  {
    "objectID": "tutorial_7a.html#setup",
    "href": "tutorial_7a.html#setup",
    "title": "Tutorial 7a: t-test",
    "section": "3.1 Setup",
    "text": "3.1 Setup\nTo do this tutorial, you only need one package: the tidyverse for data management and visualization.\n\nlibrary(tidyverse)",
    "crumbs": [
      "Tutorial 7a: *t*-test"
    ]
  },
  {
    "objectID": "tutorial_7a.html#data-preparation",
    "href": "tutorial_7a.html#data-preparation",
    "title": "Tutorial 7a: t-test",
    "section": "3.2 Data preparation",
    "text": "3.2 Data preparation\nAs mentioned, we will be working with data from the ESS — as before, from round 7 (2014). And, also as previously, we will only use the data from Norway.\nWe will also work only with a few variables:\n\nessround, cntry, and idno;\ngndr, the respondent’s gender;\nrlgdgr, a variable measuring how religious each respondent feels;\n\n\n\n3.2.1 Importing the dataset\nImporting the dataset works just the same way as before: You import the dataset file (here called ess7.sav) into R using haven::read_sav() and save the dataset in your Environment as ess7:\n\ness7 &lt;- haven::read_dta(\"ess7.dta\")\n\n\n\n\n3.2.2 Conversion & trimming\nThen you convert the data to the familiar format with labelled::unlabelled():\n\ness7 &lt;- labelled::unlabelled(ess7)\n\nFinally you trim the dataset to the relevant observations and variables and save the new trimmed dataset under the same name:\n\ness7 %&gt;% \n  filter(cntry==\"NO\") %&gt;% \n  select(essround,cntry,idno,gndr,rlgdgr) -&gt; ess7\n\nIf you like, you can also create a dictionary for the final dataset:\n\ndict &lt;- labelled::generate_dictionary(ess7)",
    "crumbs": [
      "Tutorial 7a: *t*-test"
    ]
  },
  {
    "objectID": "tutorial_7a.html#initial-eda",
    "href": "tutorial_7a.html#initial-eda",
    "title": "Tutorial 7a: t-test",
    "section": "3.3 Initial EDA",
    "text": "3.3 Initial EDA\nAs before, it is important to take a look at the main variables and how they are stored so that you know if you may have to do further data transformations.\nA good start is to use the attributes() function, starting here with the gndr variable:\n\nattributes(ess7$gndr)\n## $levels\n## [1] \"Male\"   \"Female\"\n## \n## $label\n## [1] \"Gender\"\n## \n## $class\n## [1] \"factor\"\n\nThe gndr variable is stored as a factor, which is good because gender is a categorical variable.\nJust to confirm, you can use the table() function to let R show you how many observations you have per category:\n\ntable(ess7$gndr)\n## \n##   Male Female \n##    764    672\n\n\nNow that gndr is taken care off, you can repeat the exercise with the rlgdgr (“How religious are you?”) variable, the dependent variable here:\n\nattributes(ess7$rlgdgr)\n## $levels\n##  [1] \"Not at all religious\" \"1\"                    \"2\"                   \n##  [4] \"3\"                    \"4\"                    \"5\"                   \n##  [7] \"6\"                    \"7\"                    \"8\"                   \n## [10] \"9\"                    \"Very religious\"      \n## \n## $label\n## [1] \"How religious are you\"\n## \n## $class\n## [1] \"factor\"\n\nThe variable ranges from “Not at all religious” to “Very religious” on a scale from 0 to 10. Before proceeding, it makes sense to see how many observations you have per category:\n\ntable(ess7$rlgdgr)\n## \n## Not at all religious                    1                    2 \n##                  288                  128                  147 \n##                    3                    4                    5 \n##                  160                  116                  221 \n##                    6                    7                    8 \n##                  131                  104                   79 \n##                    9       Very religious \n##                   23                   34\n\nLuckily, the “missing” categories are empty!\nBecause the variable ranges from 0 to 10, you can treat it as a numeric variable. But: The output above tells you that the variable is stored as a factor, like the gndr variable. If you want to use in calculations or statistical tests, you first need to convert it to numeric — and here the problem is that the labels and underlying numerical values do not exactly correspond (you may recall the problem from Tutorial 3).\n\nYou can see this if you quickly use the visfactor() function from bst290 (loading the package is not needed):\n\nbst290::visfactor(variable = \"rlgdgr\", dataset = ess7)\n##  values               labels\n##       1 Not at all religious\n##       2                    1\n##       3                    2\n##       4                    3\n##       5                    4\n##       6                    5\n##       7                    6\n##       8                    7\n##       9                    8\n##      10                    9\n##      11       Very religious\n\nBut you might also remember how you can fix this: Convert to numeric with as.numeric(), but subtract 1 to account for the label-value divergence:\n\ness7$relig_num &lt;- as.numeric(ess7$rlgdgr) - 1\n\nThis is also a good chance to save the new numeric version with a decent name — who wants to twist their fingers into a knot every time they type a variable name?!",
    "crumbs": [
      "Tutorial 7a: *t*-test"
    ]
  },
  {
    "objectID": "tutorial_7a.html#option-1-bar-graph",
    "href": "tutorial_7a.html#option-1-bar-graph",
    "title": "Tutorial 7a: t-test",
    "section": "4.1 Option 1: Bar graph",
    "text": "4.1 Option 1: Bar graph\nYou might remember (from Tutorials 3 and 4) how you can calculate summary statistics of one variable over the categories of another — and then directly visualize the result in a bar graph: with group_by() and summarize(), plus the pipe (%&gt;%) to link all steps together!\nFirst, the calculation:\n\ness7 %&gt;% \n  group_by(gndr) %&gt;% \n  summarize(avgrel = mean(relig_num, na.rm = T))\n## # A tibble: 2 × 2\n##   gndr   avgrel\n##   &lt;fct&gt;   &lt;dbl&gt;\n## 1 Male     3.16\n## 2 Female   4.11\n\nIt turns out that women (in Norway) are indeed more religious, on average, than men.\n\nAs you know, you can then feed the result directly into a ggplot2 bar graph:\n\ness7 %&gt;% \n  group_by(gndr) %&gt;% \n  summarize(avgrel = mean(relig_num, na.rm = T)) %&gt;% \n  ggplot(aes(x = gndr, y = avgrel)) +\n    geom_bar(stat = \"identity\") +\n    labs(x = \"Gender\", y = \"''How religious are you?''\") +\n    theme_bw()\n\n\n\n\n\n\n\n\nThe difference between men and women when it comes to their religiosity is directly apparent. Also interesting: Despite the between-gender differences, the two means are both relatively low (3 and 4 on a 0-10 scale). Overall, Norwegians are tending toward not being very religious.",
    "crumbs": [
      "Tutorial 7a: *t*-test"
    ]
  },
  {
    "objectID": "tutorial_7a.html#option-2-boxplot",
    "href": "tutorial_7a.html#option-2-boxplot",
    "title": "Tutorial 7a: t-test",
    "section": "4.2 Option 2: Boxplot",
    "text": "4.2 Option 2: Boxplot\nWhile a bar graph is a good way to represent group differences, you can also use a boxplot (as also shown by Kellstedt and Whitten 2018, 175). As you know from the previous tutorial on Data Visualization, you can create a bivariate boxplot like this:\n\nYou specify the grouping variable (gndr) as the variable that goes on the x-axis, and…\n…the outcome variable (relig_num) as the variable for the y-axis;\nThen you just add a geom_boxplot() geometric object layer to draw the boxplots;\n\nThe code and result looks like this:\n\ness7 %&gt;% \n  ggplot(aes(y = relig_num, x = gndr)) +\n    geom_boxplot()\n## Warning: Removed 5 rows containing non-finite outside the scale range\n## (`stat_boxplot()`).\n\n\n\n\n\n\n\n\nThe difference between the two groups is quite easy to see.\n\nIf you like, you can then also polish the graph a bit more:\n\ness7 %&gt;% \n  ggplot(aes(y = relig_num, x = gndr)) +\n    geom_boxplot() +\n    labs(x = \"Gender\", y = \"''How religious are you?''\") +\n    theme_bw()\n## Warning: Removed 5 rows containing non-finite outside the scale range\n## (`stat_boxplot()`).",
    "crumbs": [
      "Tutorial 7a: *t*-test"
    ]
  },
  {
    "objectID": "tutorial_7a.html#the-t.test-function",
    "href": "tutorial_7a.html#the-t.test-function",
    "title": "Tutorial 7a: t-test",
    "section": "5.1 The t.test() function",
    "text": "5.1 The t.test() function\nYou already know the t.test() function from the tutorial on confidence intervals — and this is obviously also the function to use here.\nTo run a t-test with the t.test() function, you just have to enter your dependent and independent variables separated by a ~ (tilde) plus the dataset:\n\nt.test(relig_num ~ gndr, \n       data = ess7)\n\nThe most important part is the formula-part: relig_num ~ gndr. Here, we specify that we want to know if religiosity (relig_num) differs significantly between the genders (gndr). This is the part that you need to get right when you use this test! Generally: outcome ~ group (see also the help file under ?t.test()).",
    "crumbs": [
      "Tutorial 7a: *t*-test"
    ]
  },
  {
    "objectID": "tutorial_7a.html#interpretation",
    "href": "tutorial_7a.html#interpretation",
    "title": "Tutorial 7a: t-test",
    "section": "5.2 Interpretation",
    "text": "5.2 Interpretation\nLet’s look at the result:\n\nt.test(relig_num ~ gndr, \n       data = ess7)\n## \n##  Welch Two Sample t-test\n## \n## data:  relig_num by gndr\n## t = -6.5321, df = 1372.3, p-value = 9.11e-11\n## alternative hypothesis: true difference in means between group Male and group Female is not equal to 0\n## 95 percent confidence interval:\n##  -1.2378751 -0.6660886\n## sample estimates:\n##   mean in group Male mean in group Female \n##             3.162943             4.114925\n\nAs always in R, the output is not very polished but it just takes a bit of practice to read this:\n\nStart at the bottom, where it says sample estimates: This reports the simple group averages in the dependent variable — and you already know these numbers: The average of relig_num for men is around 3.16, while the average for women is around 4.11 (see also Section 4 above).\nUnder alternative hypothesis, R tells you what you are testing here: Is there a significant difference between the group averages for men and women?\nNow to the ‘meat part’, right under data:\n\nYou get the t-statistic: -6.532. In this case, this number reflects the ratio of the mean differences and their standard error — or the ‘signal-to-noise’ ratio (see Kellstedt and Whitten 2018, 176).\ndf are the degrees of freedom.\np-value is, obviously, the p-value. R gives you this number in the scientific notation. It corresponds to 0.00000000009109578 or a 0 followed by 10 more 0s and then 9.1… — a very small number.\n\n\nR has a little helper function called format.pval() that can be useful when you need to decipher a cryptic p-value shown in scientific notation. To use it, you just paste the p-value you get from t.test() (or any other test that returns a p-value) into format.pval(). It is also useful to specify the number of significant (non-zero) numbers after the comma you want shown and the level of precision you want. Here, we want up to three numbers after the comma (with digits=3) and that any p-value that is smaller than 0.01 should just be shown as &lt; 0.01 (with eps=0.01):\n\nformat.pval(9.11e-11, digits = 3, eps = 0.01)\n\n[1] \"&lt;0.01\"\n\n\nAs above, the p-value we got is very small – smaller than 0.01 in any case.\nNow to the interesting part: What does all of this mean? Try to interpret the results based on the explanation in Kellstedt/Whitten (2018, Chapter 8)!",
    "crumbs": [
      "Tutorial 7a: *t*-test"
    ]
  },
  {
    "objectID": "tutorial_7a.html#footnotes",
    "href": "tutorial_7a.html#footnotes",
    "title": "Tutorial 7a: t-test",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nOn the gender wage gap see e.g., Blau and Kahn (2000). Gender differences in pay. Journal of Economic Perspectives, 14(4):75–99.↩︎\nInglehart and Norris (2000). The developmental theory of the gender gap: Women’s and men’s voting behavior in global perspective. International Political Science Review, 21(4):441–463.↩︎\nIversen, T. and Rosenbluth, F. (2010). Women, Work, & Politics: The Political Economy of Gender Inequality. Yale University Press, New Haven and London.↩︎\nVoas, D., McAndrew, S., and Storm, I. (2013). Modernization and the gender gap in religiosity: Evidence from cross-national European surveys. KZfSS Kölner Zeitschrift für Soziologie und Sozialpsychologie, 65(1):259–283.↩︎",
    "crumbs": [
      "Tutorial 7a: *t*-test"
    ]
  },
  {
    "objectID": "tutorial_7b.html",
    "href": "tutorial_7b.html",
    "title": "Tutorial 7b: Correlation",
    "section": "",
    "text": "The last two tutorials showed you how you can study relationships between two categorical variables (with the \\(\\chi^2\\) test) and between one binomial variable and one metric or linear variable (with the t-test). What is left: How do you analyze relationships between two continuous variables?\nThe correct test is of course Pearson’s correlation coefficient (as you know from Kellstedt and Whitten 2018). In this tutorial, you will learn how you can implement this in R.\nThe research question for this tutorial is again about religiosity, but we will now look at the effect of age. In other words, we will see if there is a correlation between how old a person is and how religious she feels.\nAnd, as before, we will start directly with the full ESS dataset (round 7, 2014).\n\n\n\n\n\n\nTip\n\n\n\nHvis du ønsker å lese en norsk tekst i tillegg: “Lær deg R”, Kapittel 5.3.3",
    "crumbs": [
      "Tutorial 7b: Correlation"
    ]
  },
  {
    "objectID": "tutorial_7b.html#setup",
    "href": "tutorial_7b.html#setup",
    "title": "Tutorial 7b: Correlation",
    "section": "3.1 Setup",
    "text": "3.1 Setup\nAs before, you need the tidyverse for your data management and visualization:\n\nlibrary(tidyverse)",
    "crumbs": [
      "Tutorial 7b: Correlation"
    ]
  },
  {
    "objectID": "tutorial_7b.html#data-import-and-management",
    "href": "tutorial_7b.html#data-import-and-management",
    "title": "Tutorial 7b: Correlation",
    "section": "3.2 Data import and management",
    "text": "3.2 Data import and management\nYou import the dataset with read_dta() from haven:\n\ness7 &lt;- haven::read_dta(\"ess7.dta\")\n\nThen you need to covert it to the regular format with labelled::unlabelled():\n\ness7 &lt;- labelled::unlabelled(ess7)\n\nWe will again work only with the data from Norway, and the following variables:\n\nessround, cntry, and idno;\nagea, the respondent’s age in years;\nrlgdgr, the variable measuring how religious each respondent feels on a scale from 0 to 10;\n\nThis means, you apply the familiar “trimming” operations:\n\ness7 %&gt;% \n  filter(cntry==\"NO\") %&gt;% \n  select(essround,cntry,idno,agea,rlgdgr) -&gt; ess7",
    "crumbs": [
      "Tutorial 7b: Correlation"
    ]
  },
  {
    "objectID": "tutorial_7b.html#data-cleaning",
    "href": "tutorial_7b.html#data-cleaning",
    "title": "Tutorial 7b: Correlation",
    "section": "3.3 Data cleaning",
    "text": "3.3 Data cleaning\nYou probably also remember from the previous tutorial that the rlgdgr-variable is stored as a factor:\n\nattributes(ess7$rlgdgr)\n## $levels\n##  [1] \"Not at all religious\" \"1\"                    \"2\"                   \n##  [4] \"3\"                    \"4\"                    \"5\"                   \n##  [7] \"6\"                    \"7\"                    \"8\"                   \n## [10] \"9\"                    \"Very religious\"      \n## \n## $label\n## [1] \"How religious are you\"\n## \n## $class\n## [1] \"factor\"\n\nThis means you have to convert it to numeric with as.numeric() while subtracting 1 to account for the divergence between labels and numerical values:\n\ness7$relig_num &lt;- as.numeric(ess7$rlgdgr) - 1\n\nAnd again, you can use this opportunity to give the new numeric version a name that is easier to type.\n\nThe situation is easier when we look at agea:\n\nclass(ess7$agea)\n## [1] \"numeric\"\n\nIt is already stored as numeric — and therefore ready to go. Still, it makes sense to look at a quick summary to get a sense of how the variable is distributed:\n\nsummary(ess7$agea)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   15.00   32.00   47.00   46.77   61.25  104.00\n\nThe lowest age recorded is 15 years, while the oldest is 104 years. The average age in the sample is 46.7, and 75% of the respondents are 61.25 years old or younger (see 3rd Qu.).",
    "crumbs": [
      "Tutorial 7b: Correlation"
    ]
  },
  {
    "objectID": "tutorial_7b.html#the-cor-function",
    "href": "tutorial_7b.html#the-cor-function",
    "title": "Tutorial 7b: Correlation",
    "section": "4.1 The cor() function",
    "text": "4.1 The cor() function\nThere are two ways to let R calculate the correlation coefficient. The first option is to only get the correlation coefficient by itself, without any additional statistical tests. To do this, you use the cor() function.\nThe cor() function is fairly easy to use:\n\nYou need to state the two variables that you want to correlate (as x and y);\nYou also need to tell R what to do with missing observations; you do this by specifying the use option. In this case, we tell R to use only non-missing observations (complete.obs). This is equivalent to using na.rm in the mean() or sd() functions.\n\n\ncor(x = ess7$agea, y = ess7$relig_num, \n    use = \"complete.obs\")\n## [1] 0.219565\n\nWe get a correlation coefficient of 0.22. This means agea and relig_num are weakly positively correlated. This speaks for our hypothesis in general — but the relationship is not very strong.",
    "crumbs": [
      "Tutorial 7b: Correlation"
    ]
  },
  {
    "objectID": "tutorial_7b.html#significance-testing-with-cor.test",
    "href": "tutorial_7b.html#significance-testing-with-cor.test",
    "title": "Tutorial 7b: Correlation",
    "section": "4.2 Significance testing with cor.test()",
    "text": "4.2 Significance testing with cor.test()\nNow that we know that there is a positive association between age and religiosity, the next question is: is this association statistically significant? In other words, is our finding only the result of random sampling variation, or is there really a systematic relationship between age and religiosity in the underlying population?\nThe cor.test() function allows you to run the significance test that is explained in Kellstedt and Whitten (2018, 183-184). To use this function, you just specify the two variables you want to use. It will take care of the NAs by itself.\n\n\n\n\n\n\nThere are different ways in which you can use this function:\n\n\n\n\nBy default, the function tests if the correlation coefficient is different from 0 or not. In other words, it tests if the two variables are associated in some way, be it positive or negative. (This is also the procedure that is described in Kellstedt and Whitten).\nYou can also let the function test more specifically if the correlation coefficient is larger than 0—if there is a positive correlation. To do that, you set the alternative option to \"greater\".\nAnd you can test specifically if the correlation coefficient is smaller than 0—if there is a negative relationship. In that case, you set alternative=\"less\".\n\n\n\n\n\n4.2.1 Testing if the true r is equal to 0 or not (default setting)\nFor example, to test if there is any relationship, positive or negative, between agea and relig_num, you would just run the following:\n\ncor.test(x = ess7$agea, y = ess7$relig_num)\n## \n##  Pearson's product-moment correlation\n## \n## data:  ess7$agea and ess7$relig_num\n## t = 8.5076, df = 1429, p-value &lt; 2.2e-16\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  0.1696758 0.2683316\n## sample estimates:\n##      cor \n## 0.219565\n\nIf you take a quick look at the output, you see under alternative what R has been testing:\n\nThe alternative hypothesis is that the true r is not equal to 0.\nThe corresponding null hypothesis (which is not specifically mentioned in the output) is then that the true r is equal to 0.\n\nAnd, since you have now learned about two other statistical tests and their interpretation, you should know what to look for next: the p-value! Based on this result, can we reject the null hypothesis (see also Kellstedt/Whitten 2018, Chapter 8)? As in the previous tutorial on the t-test, you can use format.pval() to translate the p-value into something that is a bit easier to interpret:\n\nformat.pval(2.2e-16, digits = 3, eps = 0.01)\n\n[1] \"&lt;0.01\"\n\n\nAs in the previous tutorial, the result is a very small number – smaller than 0.01.\nYou can read the other results as follows:\n\nsample estimates: shows you the sample correlation coefficient, which you already know from earlier;\n95 percent confidence interval: is obviously the 95% confidence interval for the correlation coefficient.\nt is the t statistic. As explained in Kellstedt and Whitten (2018, 183), we use the t-test in this case to test if the correlation coefficient is equal to 0 or not;\ndf are the degrees of freedom (see Kellstedt and Whitten 2018, 183);\n\n\n\n\n4.2.2 Testing if the the true r is greater than 0\nBut we actually started from the more specific hypothesis that there is a positive relationship between age and relig_num—that r should be greater than 0.\nTo test if this is true, we need to run:\n\ncor.test(x = ess7$agea, y = ess7$relig_num,\n         alternative = \"greater\")\n## \n##  Pearson's product-moment correlation\n## \n## data:  ess7$agea and ess7$relig_num\n## t = 8.5076, df = 1429, p-value &lt; 2.2e-16\n## alternative hypothesis: true correlation is greater than 0\n## 95 percent confidence interval:\n##  0.1777628 1.0000000\n## sample estimates:\n##      cor \n## 0.219565\n\nAnd if you take another quick look at the output, you see that R has been testing a different hypothesis now:\n\nThe alternative hypothesis is now that the true r (in the population) is greater than 0.\nThe corresponding null hypothesis is that the true r is equal to or smaller than 0.\n\nAs before, can we reject the null hypothesis here – and what would that mean substantively?\n\n\n\n4.2.3 Testing if the true r is smaller than 0\nFinally, let’s see what happens if we test if the true r is smaller than 0. In this case, the hypotheses are as follows:\n\nThe alternative hypothesis is that the true r is smaller than 0.\nThe corresponding null hypothesis is that the true r is equal to or larger than 0.\n\nTo test this, we set alternative=\"less\":\n\ncor.test(x = ess7$agea, y = ess7$relig_num,\n         alternative = \"less\")\n## \n##  Pearson's product-moment correlation\n## \n## data:  ess7$agea and ess7$relig_num\n## t = 8.5076, df = 1429, p-value = 1\n## alternative hypothesis: true correlation is less than 0\n## 95 percent confidence interval:\n##  -1.0000000  0.2605762\n## sample estimates:\n##      cor \n## 0.219565\n\nDo you see the difference to the earlier results? Can you figure out what this means?",
    "crumbs": [
      "Tutorial 7b: Correlation"
    ]
  },
  {
    "objectID": "tutorial_7b.html#geom_jitter",
    "href": "tutorial_7b.html#geom_jitter",
    "title": "Tutorial 7b: Correlation",
    "section": "6.1 geom_jitter()",
    "text": "6.1 geom_jitter()\nOne thing you can do in this case is to “jitter” the data. “Jittering” means that you gently shake the data out of their discrete categories. In technical terms, you add a bit of random noise to the data.\nTo “jitter” the data in the graph, you just replace geom_point() with geom_jitter():\n\ness7 %&gt;% \n  ggplot(aes(x = agea, y= relig_num)) +\n    geom_jitter() +\n    labs(caption = \"The data were jittered.\")\n## Warning: Removed 5 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n\n\n\n\n\n\n\n\nNow it is a bit easier to see where most of the observations are. Important: If you jitter your data, you need to tell your readers (e.g., by including a note with the caption option).\nStill, the graph is hard to read and it is not clear what, if any, relationship there is between the data.\n\nYou can make it a bit easier to read still by changing the shape of the points with the shape option:\n\ness7 %&gt;% \n  ggplot(aes(x = agea, y = relig_num)) +\n    geom_jitter(shape = 1) +\n    labs(caption = \"The data were jittered.\")\n## Warning: Removed 5 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n\n\n\n\n\n\n\n\nshape = 1 means: “Use hollow black rings”.3 But again, if there is any pattern in the data then it is still difficult to see.",
    "crumbs": [
      "Tutorial 7b: Correlation"
    ]
  },
  {
    "objectID": "tutorial_7b.html#geom_smooth",
    "href": "tutorial_7b.html#geom_smooth",
    "title": "Tutorial 7b: Correlation",
    "section": "6.2 geom_smooth()",
    "text": "6.2 geom_smooth()\nWhat you can do now is to add a “fitted line” to the graph. This means that R estimates a linear (i.e., OLS) regression (which you will learn about in the next weeks) and plots the predicted or “fitted” values from this estimation to the graph. This can show you more clearly any patterns in the data.\nTo add such a fitted line, you use geom_smooth(). Importantly, you also need to specify that you want this line to be based on a linear model (method = \"lm\").4 In this case, we also turn off the confidence intervals (se = FALSE), but you can of course show these if you like!\n\ness7 %&gt;% \n  ggplot(aes(x = agea, y = relig_num)) +\n    geom_jitter(shape = 1) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    labs(caption = \"The data were jittered.\")\n## `geom_smooth()` using formula = 'y ~ x'\n## Warning: Removed 5 rows containing non-finite outside the scale range\n## (`stat_smooth()`).\n## Warning: Removed 5 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n\n\n\n\n\n\n\n\nAnd now you can clearly see the positive association between the two variables: As age (agea) increases, religiosity (relig_num) increases as well!\n\nWith some more polishing, we get a decent publication-quality graph:\n\ness7 %&gt;% \n  ggplot(aes(x = agea, y = relig_num)) +\n    geom_jitter(size = 2, shape = 1) +\n    geom_smooth(method = \"lm\", se = FALSE, color = \"red\", linetype = \"dashed\") +\n    labs(x = \"Age\", y = \"''How religious are you?''\",\n         caption = \"The data were jittered.\") +\n    theme_bw()\n## `geom_smooth()` using formula = 'y ~ x'\n## Warning: Removed 5 rows containing non-finite outside the scale range\n## (`stat_smooth()`).\n## Warning: Removed 5 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n\n\n\n\n\n\n\n\nNote that we made the points a bit larger (size = 2) and changed the color and type of the fitted line.",
    "crumbs": [
      "Tutorial 7b: Correlation"
    ]
  },
  {
    "objectID": "tutorial_7b.html#footnotes",
    "href": "tutorial_7b.html#footnotes",
    "title": "Tutorial 7b: Correlation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nHout, M. and Fischer, C. S. (2014). Explaining why more Americans have no religious preference: Political backlash and generational succession, 1987-2012. Sociological Science, 1:423–447; Marwell, G. and Demerath, N. (2003). “Secularization” by any other name. American Sociological Review, 68(2):314–316.↩︎\nSee also Inglehart, R. (1971). The silent revolution in Europe: Intergenerational change in post-industrial societies. American Political Science Review, 65(4):991–1017.↩︎\nSee also http://www.sthda.com/english/wiki/ggplot2-point-shapes↩︎\nOtherwise, R will use a fancy “smoothed” regression estimation procedure, which usually produces a very pretty result—but which is also difficult to understand. It is often better to keep it simple and just use the OLS method, which most people understand.↩︎",
    "crumbs": [
      "Tutorial 7b: Correlation"
    ]
  },
  {
    "objectID": "tutorial_11.html",
    "href": "tutorial_11.html",
    "title": "Tutorial 11: Logistic regression for binomial dependent variables",
    "section": "",
    "text": "When working as a (social) data analyst, you will often be interested in outcomes that have two distinct values: Did someone vote in the last election or not? Is someone a union member or not? Does a country go to war or experience a coup d’état in a given year or not? You will, in other words, work with dependent variables that are binary or dichotomous.\nIn addition, your theory for your outcome or dependent variable will often specify several different potential causes. For example, a person’s decision to vote in an election might depend their age (older people tend to be more likely to vote) but also their education or income. This means that you need to use multivariate regression to really test your theory.\nYou already know what you would use in this case if you had a linear dependent variable: Ordinary least squares (OLS) regression. But one of the central assumptions underlying the OLS model is that the dependent variable must be linear — and this assumption does not hold if you are interested in electoral participation, union membership, or coups.\nInstead, you need to use the logistic regression (a.k.a., “logit”) model. The logistic regression model is specifically designed to analyze the effects of multiple independent variables on a binary outcome or dependent variable. An alternative is the probit regression model, which works slightly differently but will usually give you almost identical results.1\nIn this tutorial, you will learn how you estimate and interpret a logistic regression model. Topic-wise, we continue with the example from tutorial 6 on tabular analysis: Political participation in the form of voting in a national election.\nBefore you continue here, make sure that you have read and understood the introduction to the logistic regression model in Kellstedt & Whitten (2018, Chapter 12). Otherwise, you may be able to run the code shown here but you will not understand what any of this really means.",
    "crumbs": [
      "Tutorial 11: Logistic regression for binomial dependent variables"
    ]
  },
  {
    "objectID": "tutorial_11.html#setup-and-data-management",
    "href": "tutorial_11.html#setup-and-data-management",
    "title": "Tutorial 11: Logistic regression for binomial dependent variables",
    "section": "2.1 Setup and data management",
    "text": "2.1 Setup and data management\nAs always, the start is to load the necessary packages, which are:\n\nlibrary(tidyverse) # for data management & visualization\nlibrary(prediction) # to calculate predicted probabilities\nlibrary(margins) # to calculate marginal effects\nlibrary(texreg) # to present regression results\n\nThe prediction and margins packages may be new to you (if you did not work through the extra part of Tutorial 9 on multivariate linear regression). These two packages are used to convert the results of regression analyses into more intuitive and useful “quantities of interest”, which is especially important when it comes to logistic regression, as you will see further below.",
    "crumbs": [
      "Tutorial 11: Logistic regression for binomial dependent variables"
    ]
  },
  {
    "objectID": "tutorial_11.html#data-import-and-initial-data-cleaning",
    "href": "tutorial_11.html#data-import-and-initial-data-cleaning",
    "title": "Tutorial 11: Logistic regression for binomial dependent variables",
    "section": "2.2 Data import and initial data cleaning",
    "text": "2.2 Data import and initial data cleaning\nThis part is exactly as before and you should now already know what to do here (see Tutorials 8 and 9 for details):\n\nUse haven::read_dta() to import the ESS round 7 (2014) dataset; save it as ess7;\nTransform the dataset into the familiar format using labelled:: unlabelled();\nTrim the dataset:\n\nKeep only observations from Norway;\nSelect the following variables: essround, idno, cntry, vote, eduyrs, agea, mbtru, and gndr;\nUse the pipe to link everything;\nSave the trimmed dataset as ess7;\n\nIf you like, create a data dictionary using labelled::generate_dictionary();",
    "crumbs": [
      "Tutorial 11: Logistic regression for binomial dependent variables"
    ]
  },
  {
    "objectID": "tutorial_11.html#preparing-the-dependent-variable",
    "href": "tutorial_11.html#preparing-the-dependent-variable",
    "title": "Tutorial 11: Logistic regression for binomial dependent variables",
    "section": "2.3 Preparing the dependent variable",
    "text": "2.3 Preparing the dependent variable\n\n2.3.1 Getting rid of extra categories\nThe previous part took care of most of the data cleaning and preparation, but the dependent variable (vote) needs a bit more work. We go over this in detail because the situation here is one that you may often encounter in practice, so it is good to know how to deal with it.\nYou will see more clearly what the problem is when you let R print out a frequency table of the variable with table():\n\ntable(ess7$vote)\n\n                 Yes                   No Not eligible to vote \n                1127                  158                  150 \n\nIt turns out that the vote variable has three categories: “Yes”, “No”, and “Not eligible to vote”. This is a problem because the type of logistic regression model we use here works only for dependent variables that have exactly two categories.2\nThis means that if we want to use the vote variable, we first have to get rid of one category. In this case, it makes sense to get rid of the “Not eligible” category because the people who are not eligible to vote (e.g., immigrants) have by definition a probability of 0 to vote, which means they are not that interesting to us here.\nTherefore, we recode the vote variable by setting all cases that fall into the “Not eligible” category to missing (NA). To do that, we use the na_if() function. This function simply takes a variable (specified under x in the function) and replaces all observations that have a particular value (specified under y) with NA. In this case, we create a new version of vote which is NA if the original version of vote is equal to “Not eligible” and then replace the old with the new version in the dataset:\n\ness7 %&gt;% \n  mutate(vote = na_if(x = vote,\n                      y = \"Not eligible to vote\")) -&gt; ess7\n\nImportant: You obviously need to save the result with the assignment operator. In this case, we use the “reversed” assignment operator and save the dataset with the new version of the vote variable under the old name (ess7).\n\nIf we now again tabulate the vote variable, the “Not eligible” category is empty:\n\ntable(ess7$vote)\n\n                 Yes                   No Not eligible to vote \n                1127                  158                    0 \n\nBut you will also notice another thing: The “Not eligible” category is now empty, and therefore no longer useful. This means we can get rid of it by using droplevels():\n\ness7$vote &lt;- droplevels(ess7$vote)\n\nAnother check reveals that the variable is now probably coded and has exactly two categories:\n\ntable(ess7$vote)\n\n Yes   No \n1127  158 \n\n\n\n2.3.2 Setting the reference category\nA last thing: When you run a logistic regression model, R will take one category of the dependent variable as the ‘baseline’ or ‘reference’ category and then estimate the probability of an observation falling into the other category. For example, if the baseline category of vote is “No”, then R will estimate the probability of falling into the “Yes” category. Importantly, R might also go the other direction and take “Yes” as the reference category. In this case, the model will still be correct — but the interpretation might be weird.\nTo be sure that R picks the correct reference category, it makes sense to set it explicitly. To do this, you use the relevel() function:\n\ness7$vote &lt;- relevel(ess7$vote,\n                     ref = \"No\")\n\nWith this function, we tell R that it should replace the vote variable with a new version of itself, in which “No” is the reference or baseline category.",
    "crumbs": [
      "Tutorial 11: Logistic regression for binomial dependent variables"
    ]
  },
  {
    "objectID": "tutorial_11.html#bivariate-model",
    "href": "tutorial_11.html#bivariate-model",
    "title": "Tutorial 11: Logistic regression for binomial dependent variables",
    "section": "4.1 Bivariate model",
    "text": "4.1 Bivariate model\nAs before, we start with a bivariate model that includes only the main independent variable, gndr:\n\nmod1 &lt;- glm(vote ~ gndr ,\n            data = ess7, family = \"binomial\")\n\nsummary(mod1)\n\nCall:\nglm(formula = vote ~ gndr, family = \"binomial\", data = ess7)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)   1.8891     0.1144  16.517   &lt;2e-16 ***\ngndrFemale    0.1632     0.1709   0.955     0.34    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 958.03  on 1284  degrees of freedom\nResidual deviance: 957.12  on 1283  degrees of freedom\n  (151 observations deleted due to missingness)\nAIC: 961.12\n\nNumber of Fisher Scoring iterations: 4\n\nAt first sight, the model results look similar to what you got from lm(): You get coefficient estimates under Estimate, standard errors, p-values, and significance starts in addition to some summary statistics.\nBut be careful: The coefficient estimates you get from a logistic regression model are logarithmic odds-ratios (or “log-odds”) and these very difficult to interpret by themselves. All you can really directly interpret are the p-values and the signs of the coefficients (are they positive or negative?). In other words, all you can see is if a variable has a positive or negative effect and if that effect is statistically different from 0 — but the coefficient estimates from a logit model cannot really be interpreted as measures of how strong or weak a given effect is exactly.\nIn this case, you can see that the dummy for females (which R automatically created) is positively associated with voting (i.e., women have a higher probability of voting than men), but also that the relationship is not statistically significant (and we conclude therefore that there is most likely no difference between men and women).",
    "crumbs": [
      "Tutorial 11: Logistic regression for binomial dependent variables"
    ]
  },
  {
    "objectID": "tutorial_11.html#multivariate-model",
    "href": "tutorial_11.html#multivariate-model",
    "title": "Tutorial 11: Logistic regression for binomial dependent variables",
    "section": "4.2 Multivariate model",
    "text": "4.2 Multivariate model\nNext, we see what happens when we include the other independent variables and make the model multivariate:\n\nmod2 &lt;- glm(vote ~ gndr + eduyrs + agea + mbtru,\n            data = ess7, family = \"binomial\")\n\nsummary(mod2)\n\nCall:\nglm(formula = vote ~ gndr + eduyrs + agea + mbtru, family = \"binomial\", \n    data = ess7)\n\nCoefficients:\n                      Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)          -0.923511   0.525497  -1.757   0.0788 .  \ngndrFemale            0.122934   0.177377   0.693   0.4883    \neduyrs                0.114889   0.027353   4.200 2.67e-05 ***\nagea                  0.034218   0.005432   6.300 2.98e-10 ***\nmbtruYes, previously -0.418963   0.245144  -1.709   0.0874 .  \nmbtruNo              -0.553805   0.206417  -2.683   0.0073 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 957.51  on 1282  degrees of freedom\nResidual deviance: 880.50  on 1277  degrees of freedom\n  (153 observations deleted due to missingness)\nAIC: 892.5\n\nNumber of Fisher Scoring iterations: 5\n\nThe effect of being a woman is still the same as before: The coefficient is positive, but not significant. Thus, there is probably no difference between men and women when it comes to voting in elections.\nBut you also see that the other variables have significant effects: Every additional year of education completed significantly raises one’s probability to participate in an election, all else held constant, and so does every additional year of age.\nThe trade union membership variable is a categorical variable, and thus has a different interpretation: Here the (omitted) baseline or reference category is “Being a union member”, and the displayed coefficients for “Not a member” and “Previously been a member” indicate the differences to that reference category. In this case, having previously been a union member and not being a union member corresponds to a lower chance of participating in an election, and the difference is significant at conventional levels in the case of the “No” category.",
    "crumbs": [
      "Tutorial 11: Logistic regression for binomial dependent variables"
    ]
  },
  {
    "objectID": "tutorial_11.html#presenting-the-results-as-a-table",
    "href": "tutorial_11.html#presenting-the-results-as-a-table",
    "title": "Tutorial 11: Logistic regression for binomial dependent variables",
    "section": "4.3 Presenting the results as a table",
    "text": "4.3 Presenting the results as a table\nAs in the case of a linear regression analysis before, it is important to present these results in a proper table. And here you can again use the texreg() package:\n\nscreenreg(list(mod1,mod2),\n          stars = 0.05,\n          custom.coef.names = c(\"Intercept\",\n                                \"Female\",\"Years of educ. completed\",\"Age\",\n                                \"Previous union member\",\"Not a union member\"))\n\n==============================================\n                          Model 1    Model 2  \n----------------------------------------------\nIntercept                    1.89 *    -0.92  \n                            (0.11)     (0.53) \nFemale                       0.16       0.12  \n                            (0.17)     (0.18) \nYears of educ. completed                0.11 *\n                                       (0.03) \nAge                                     0.03 *\n                                       (0.01) \nPrevious union member                  -0.42  \n                                       (0.25) \nNot a union member                     -0.55 *\n                                       (0.21) \n----------------------------------------------\nAIC                        961.12     892.50  \nBIC                        971.44     923.45  \nLog Likelihood            -478.56    -440.25  \nDeviance                   957.12     880.50  \nNum. obs.                 1285       1283     \n==============================================\n* p &lt; 0.05\n\nThis is also helpful to you because you get a few more helpful summary statistics like the BIC and AIC information criteria (lower values mean a better-fitting model) and the deviance (a measure that gives you an indication of how far off your model is, comparable to the RMSE in a linear regression model). The log-likelihood is a quite cryptic statistic and primarily used to evaluate the overall significance of your model.3\nStill: All you can say is whether a variable has a significant effect, and in which direction that effect goes — but the main problem remains: You cannot say by how large the effect of a variable really is! To do so, you need to calculate predicted probabilities or changes in predicted probabilities — and this comes next.",
    "crumbs": [
      "Tutorial 11: Logistic regression for binomial dependent variables"
    ]
  },
  {
    "objectID": "tutorial_11.html#predicted-probabilities",
    "href": "tutorial_11.html#predicted-probabilities",
    "title": "Tutorial 11: Logistic regression for binomial dependent variables",
    "section": "5.1 Predicted probabilities",
    "text": "5.1 Predicted probabilities\nThe best place to start is with overall predicted probabilities because these are the most intuitive to interpret. To get these, we use the prediction() function from the prediction package.\nWhen we use this function, we need to specify which model we want to base the prediction on and over which values of which variable. Since our main interest is in the effect of gender, we start by getting the predicted probabilities of voting for men and women based on the multivariate model:\n\nprediction(mod2,\n           at = list(gndr = c(\"Male\",\"Female\")))\n\nData frame with 2566 predictions from\n glm(formula = vote ~ gndr + eduyrs + agea + mbtru, family = \"binomial\", \n    data = ess7)\nwith average predictions:\n\n\n   gndr      x\n   Male 0.8711\n Female 0.8834\n\n\nThe numbers you get here are the predicted probabilities of voting in the usual 0-1 notation, where 1 corresponds to 100%. You can see that men have almost exactly the same probability of voting as women: 87% compared to 88%.\nTo also get measures of uncertainty and statistical significance (p-values & confidence intervals), we can use the prediction_summary() function:\n\nprediction_summary(mod2,\n           at = list(gndr = c(\"Male\",\"Female\")))\n\n at(gndr) Prediction      SE     z p  lower  upper\n     Male     0.8711 0.01240 70.23 0 0.8468 0.8954\n   Female     0.8834 0.01269 69.62 0 0.8585 0.9083\n\n\nHere, the p-values tell us that both predicted probabilities are significantly different from 0: We can say that both men and women in the overall Norwegian population have a probability of voting that is higher than 0 (which is arguably not very surprising).\nThe more interesting (but harder to see) thing is that the confidence interval for one gender include the predicted value of the other gender. For example, the confidence interval for the predicted probability of voting for men includes the predicted value for women. This reflects again the lack of a statistically significant effect of gender.\nAnd this lack of a statistically significant difference between men and women is probably easier to see when you visualize the result (as before, by “piping” the result into a `ggplot() graph):\n\nprediction_summary(mod2,\n           at = list(gndr = c(\"Male\",\"Female\"))) %&gt;% \n  ggplot(aes(x = `at(gndr)`, y = Prediction, ymin = lower, ymax = upper)) +\n    geom_point() +\n    geom_linerange() +\n    scale_y_continuous(breaks = seq(.85,.95,.01),\n                       labels = scales::percent) + # to get real percentages\n    labs(x = \"Gender\", y = \"Predicted probability of voting\",\n         caption = \"95% confidence intervals.\") +\n    theme_classic()\n\n\n\n\n\n\n\n\nThe conclusion to be drawn here is then obviously that our theory is wrong and we need to stick with the null hypothesis: Gender does not matter for electoral participation.\n\nYou can of course use the prediction() function also to get predicted probabilities for numeric independent variables. And, in this case, this also gets us a more interesting result.\nFor example, let’s look at how the effect of education (eduyrs) on voting when expressed in predicted probabilities. To get these (plus measures of uncertainty), we use the prediction_summary() function and specify a sequence of values of eduyrs over which these should be calculated:\n\nprediction_summary(mod2,\n           at = list(eduyrs = seq(from = 0, to = 25, by = 1)))\n\n at(eduyrs) Prediction       SE      z          p  lower  upper\n          0     0.6256 0.078460  7.973  1.547e-15 0.4718 0.7794\n          1     0.6496 0.071000  9.149  5.764e-20 0.5104 0.7887\n          2     0.6728 0.063614 10.577  3.811e-26 0.5482 0.7975\n          3     0.6954 0.056396 12.330  6.234e-35 0.5848 0.8059\n          4     0.7171 0.049430 14.507  1.099e-47 0.6202 0.8139\n          5     0.7378 0.042792 17.243  1.266e-66 0.6540 0.8217\n          6     0.7577 0.036551 20.730  1.857e-95 0.6861 0.8293\n          7     0.7766 0.030769 25.239 1.514e-140 0.7163 0.8369\n          8     0.7944 0.025505 31.148 5.406e-213 0.7444 0.8444\n          9     0.8113 0.020819 38.969  0.000e+00 0.7705 0.8521\n         10     0.8271 0.016781 49.290  0.000e+00 0.7942 0.8600\n         11     0.8420 0.013483 62.448  0.000e+00 0.8155 0.8684\n         12     0.8558 0.011041 77.513  0.000e+00 0.8341 0.8774\n         13     0.8686 0.009558 90.882  0.000e+00 0.8499 0.8874\n         14     0.8806 0.009012 97.708  0.000e+00 0.8629 0.8982\n         15     0.8916 0.009177 97.150  0.000e+00 0.8736 0.9096\n         16     0.9017 0.009731 92.664  0.000e+00 0.8827 0.9208\n         17     0.9111 0.010412 87.501  0.000e+00 0.8907 0.9315\n         18     0.9196 0.011063 83.127  0.000e+00 0.8979 0.9413\n         19     0.9274 0.011605 79.919  0.000e+00 0.9047 0.9502\n         20     0.9346 0.012005 77.845  0.000e+00 0.9110 0.9581\n         21     0.9411 0.012258 76.771  0.000e+00 0.9170 0.9651\n         22     0.9470 0.012369 76.563  0.000e+00 0.9227 0.9712\n         23     0.9523 0.012350 77.112  0.000e+00 0.9281 0.9765\n         24     0.9572 0.012218 78.340  0.000e+00 0.9332 0.9811\n         25     0.9616 0.011990 80.194  0.000e+00 0.9381 0.9851\n\n\nIf you now look at the numbers under Prediction, you can see how the probability of voting increases from around 62% for someone with no education to around 96% for someone with 25 years of education. That is an increase of more than 30 percentage points, which is not exactly small.\n\nA visualization should again make this even easier to see:\n\nprediction_summary(mod2,\n           at = list(eduyrs = seq(from = 0, to = 25, by = 1))) %&gt;% \n  ggplot(aes(x = `at(eduyrs)`, y = Prediction, ymin = lower, ymax = upper)) +\n    geom_point() +\n    geom_line(linetype = \"dashed\") +\n    geom_ribbon(alpha = .3) +\n    scale_y_continuous(labels = scales::percent) + # to get real percentages\n    labs(x = \"Years of education completed\",\n         y = \"Predicted probability of voting\") +\n    theme_classic()\n\n\n\n\n\n\n\n\nThe graph clearly shows the strong increase in the probability to vote as education increases. Also visible is that the estimates are more uncertain (wider confidence intervals) at low values of the eduyrs variable and get narrower for higher values. This most likely reflects that fact that there are only very few observations with less than 10 years of education in the dataset.",
    "crumbs": [
      "Tutorial 11: Logistic regression for binomial dependent variables"
    ]
  },
  {
    "objectID": "tutorial_11.html#marginal-effects",
    "href": "tutorial_11.html#marginal-effects",
    "title": "Tutorial 11: Logistic regression for binomial dependent variables",
    "section": "5.2 Marginal effects",
    "text": "5.2 Marginal effects\nIn addition to the overall predicted probabilities, we would usually also like to know by how much exactly a change in an independent variable changes these probabilities. These marginal effects (i.e., changes in predicted probabilities associated with a change in a given independent variable) are easier to interpret and more similar in nature to the regression coefficients you get from linear regression model.\nGetting these marginal effects (plus measures of uncertainty) is possible with the margins_summary() function from the margins package.\nYou might appreciate the fact that using this function is fairly simple (at least in its basic form):\n\nmargins_summary(model = mod2)\n               factor     AME     SE       z      p   lower   upper\n                 agea  0.0034 0.0005  6.3349 0.0000  0.0024  0.0045\n               eduyrs  0.0116 0.0028  4.2093 0.0000  0.0062  0.0170\n           gndrFemale  0.0124 0.0178  0.6951 0.4870 -0.0225  0.0472\n              mbtruNo -0.0563 0.0216 -2.6089 0.0091 -0.0986 -0.0140\n mbtruYes, previously -0.0406 0.0251 -1.6168 0.1059 -0.0897  0.0086\n\nAll we have to specify here is the model we want to base the calculations on, and the function automatically gives us the changes in the predicted probabilities associated with a given variable.\nAnd the result is also fairly easy to interpret: Every additional year of age increases the probability of voting by 0.34 percentage points (0.0034 \\(\\times\\) 100), while every additional year of education increases it by 1.16 percentage points. As the p-values indicate, these effects are significant.\nYou can also see that the predicted probability of voting for women is about 1.24 percentage points higher than that for men, but again that this is not statistically different from 0. And finally, those who are not a union member or who have previously been members have predicted probabilities of voting that are about 4 and 5.6 percentage points lower compared to current union members.\n\nThese results can of course also be visualized:\n\nmargins_summary(model = mod2) %&gt;% \n  ggplot(aes(y = factor, x = AME, xmin = lower, xmax = upper)) +\n    geom_point(shape = 1) + # shape = 1 gives hollow dots\n    geom_linerange() +\n    geom_vline(xintercept = 0, linetype = \"dashed\", color = \"gray\") +\n    scale_y_discrete(labels = c(\"Age\",\"Years of educ. compl.\",\n                                \"Female\",\"Not a union member\",\n                                \"Previous union member\")) + \n    scale_x_continuous(labels = scales::percent) + # to get real percentages\n    labs(x = \"Effect of variables on pred. probability of voting\",\n         y = \"\", caption = \"95% confidence intervals.\") +\n    theme_classic()\n\n\n\n\n\n\n\n\nAgain, you see that:\n\nGender makes no difference (the effect of being a woman is not significantly different from 0).\nAge and education have positive effects, and these are significantly different from 0.\nCompared to currently being a union member (the omitted baseline category of the union membership variable), having previously been part of a union does not significantly change one’s probability of voting.\nHowever, not being a union member does significantly reduce the probability of voting compared to the omitted baseline category, being a union member.",
    "crumbs": [
      "Tutorial 11: Logistic regression for binomial dependent variables"
    ]
  },
  {
    "objectID": "tutorial_11.html#footnotes",
    "href": "tutorial_11.html#footnotes",
    "title": "Tutorial 11: Logistic regression for binomial dependent variables",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you want a thorough introduction to the logit and probit models as well as a number of other regression models for non-linear dependent variables, you can read Long, J. S. (1997). Regression Models for Categorical and Limited Dependent Variables. Sage, London. To learn about the even more advanced conditional logit model and its extensions, which are often used in research on voter behavior, you can read Train, K. E. (1993). Qualitative Choice Analysis. Theory, Econometrics, and an Application to Automobile Demand. MIT Press, Cambridge, 3rd edition, and Train, K. E. (2009). Discrete Choice Methods with Simulation. Cambridge University Press, Cambridge, 2nd edition.↩︎\nThere are of course other models for more complex variables, see e.g., Long, J. S. (1997). Regression Models for Categorical and Limited Dependent Variables. Sage, London; or Train, K. E. (1993). Qualitative Choice Analysis. Theory, Econometrics, and an Application to Automobile Demand. MIT Press, Cambridge, 3rd edition.↩︎\nYou can see if your model as a whole is significantly better at predicting your outcome by using a likelihood-ratio test (see Long 1997, Chapter 4). In R, you get this by running an “empty” model (e.g., mod0 &lt;- glm(vote ~ 1 , data = ess7)) and then comparing the empty model with your main specification with the anova() function (e.g., anova(mod0,mod2)).↩︎",
    "crumbs": [
      "Tutorial 11: Logistic regression for binomial dependent variables"
    ]
  }
]