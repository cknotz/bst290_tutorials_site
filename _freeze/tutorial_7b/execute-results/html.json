{
  "hash": "e0c7ee679c52900a14d2c256baf99b59",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Tutorial 7b: Correlation\"\nauthor: \"Carlo Knotz\"\ntoc: true\nnumber-sections: true\nformat:\n  html: default\n  pdf: default\nlightbox: true\nlang: en\neditor_options: \n  chunk_output_type: console\n---\n\n\n\n\n\n\n\n\n\n# Introduction\n\nThe last two tutorials showed you how you can study relationships between two categorical variables (with the $\\chi^2$ test) and between one binomial variable and one metric or linear variable (with the *t*-test). What is left: How do you analyze relationships between *two continuous variables*?\n\nThe correct test is of course *Pearson's correlation coefficient* (as you know from Kellstedt and Whitten 2018). In this tutorial, you will learn how you can implement this in `R`.\n\nThe research question for this tutorial is again about religiosity, but we will now look at the effect of *age*. In other words, we will see if there is a correlation between how old a person is and how religious she feels.\n\nAnd, as before, we will start directly with the full *ESS* dataset (round 7, 2014).\n\n::: {.callout-tip}\n\n*Hvis du ønsker å lese en norsk tekst __i tillegg__: \"Lær deg R\", Kapittel 5.3.3*\n\n:::\n\n\\newpage\n# Hypotheses\n\nThere is a debate among sociologists about whether societies are becoming less religious (or \"secularized\"). Some argue that this indeed the case, while others are a bit more sceptical.^[Hout, M. and Fischer, C. S. (2014). Explaining why more Americans have no religious preference: Political backlash and generational succession, 1987-2012. *Sociological Science*, 1:423–447; Marwell, G. and Demerath, N. (2003). \"Secularization\" by any other name. *American Sociological Review*, 68(2):314–316.] Those who support the idea that religion becomes less important in people's lives point to *generational replacement* as a mechanism: Older generations are typically more religious, but this is less true for younger generations. As older generations die and newer are born, society overall becomes less religious.^[See also Inglehart, R. (1971). The silent revolution in Europe: Intergenerational change in post-industrial societies. *American Political Science Review*, 65(4):991–1017.]\n\nIf the generational-replacement hypothesis is true, then we should be able to see that younger people are less religious than older people --- or a *positive correlation* between religiosity and age: The higher one's age, the more religious they are. This is the hypothesis we will test here.\n\nThe corresponding null hypothesis is then: There is no relationship between age and religiosity, or the correlation between age and religiosity is zero.\n\n\\newpage\n# Setup, data import and management\n\nYou should at this point know quite well how you import data from the *ESS* and prepare them for analysis. \n\n## Setup\n\nAs before, you need the `tidyverse` for your data management and visualization:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n\n\n\n\n\n\\newpage\n## Data import and management\n\nYou import the dataset with `read_dta()` from `haven`:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ness7 <- haven::read_dta(\"ess7.dta\")\n```\n:::\n\n\n\n\n\n\nThen you need to covert it to the regular format with `labelled::unlabelled()`:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ness7 <- labelled::unlabelled(ess7)\n```\n:::\n\n\n\n\n\n\nWe will again work only with the data from Norway, and the following variables:\n\n* `essround`, `cntry`, and `idno`;\n* `agea`, the respondent's age in years;\n* `rlgdgr`, the variable measuring how religious each respondent feels on a scale from 0 to 10;\n\nThis means, you apply the familiar \"trimming\" operations:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ness7 %>% \n  filter(cntry==\"NO\") %>% \n  select(essround,cntry,idno,agea,rlgdgr) -> ess7\n```\n:::\n\n\n\n\n\n\n\\newpage\n\n## Data cleaning\nYou probably also remember from the previous tutorial that the `rlgdgr`-variable is stored as a *factor*:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nattributes(ess7$rlgdgr)\n## $levels\n##  [1] \"Not at all religious\" \"1\"                    \"2\"                   \n##  [4] \"3\"                    \"4\"                    \"5\"                   \n##  [7] \"6\"                    \"7\"                    \"8\"                   \n## [10] \"9\"                    \"Very religious\"      \n## \n## $label\n## [1] \"How religious are you\"\n## \n## $class\n## [1] \"factor\"\n```\n:::\n\n\n\n\n\n\nThis means you have to convert it to numeric with `as.numeric()` while subtracting 1 to account for the divergence between labels and numerical values:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ness7$relig_num <- as.numeric(ess7$rlgdgr) - 1\n```\n:::\n\n\n\n\n\n\nAnd again, you can use this opportunity to give the new numeric version a name that is easier to type.\n\n\\newpage\nThe situation is easier when we look at `agea`:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(ess7$agea)\n## [1] \"numeric\"\n```\n:::\n\n\n\n\n\n\nIt is already stored as `numeric` --- and therefore ready to go. Still, it makes sense to look at a quick summary to get a sense of how the variable is distributed:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(ess7$agea)\n##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n##   15.00   32.00   47.00   46.77   61.25  104.00\n```\n:::\n\n\n\n\n\n\nThe lowest age recorded is 15 years, while the oldest is 104 years. The average age in the sample is 46.7, and 75% of the respondents are 61.25 years old or younger (see `3rd Qu.`).\n\n\\newpage\n# Statistical analysis\n\nWe will jump right into the statistical analysis (but you can learn how to visualize the relationship between the two variables in the voluntary part below).\n\nAs mentioned in the Introduction, the appropriate measure for the strength of a relationship between two numeric variables is Pearson's correlation coefficient (*r*), which can range between -1 and 1. Negative values of *r* reflect *negative* relationships (more X, less Y) while positive values of *r* indicate *positive* relationships (more X, more Y). An *r* of 0 indicates that the two variables are not correlated---that they are unrelated to each other. This is also illustrated in the six graphs below.\n\n\n\n\n\n\n::: {.cell layout-ncol=\"2\"}\n::: {.cell-output-display}\n![](tutorial_7b_files/figure-html/corrgraphsr, figures-side-1.png){width=50%}\n:::\n\n::: {.cell-output-display}\n![](tutorial_7b_files/figure-html/corrgraphsr, figures-side-2.png){width=50%}\n:::\n\n::: {.cell-output-display}\n![](tutorial_7b_files/figure-html/corrgraphsr, figures-side-3.png){width=50%}\n:::\n\n::: {.cell-output-display}\n![](tutorial_7b_files/figure-html/corrgraphsr, figures-side-4.png){width=50%}\n:::\n\n::: {.cell-output-display}\n![](tutorial_7b_files/figure-html/corrgraphsr, figures-side-5.png){width=50%}\n:::\n\n::: {.cell-output-display}\n![](tutorial_7b_files/figure-html/corrgraphsr, figures-side-6.png){width=50%}\n:::\n:::\n\n\n\n\n\n\n\n\n\\newpage\n## The `cor()` function\nThere are two ways to let `R` calculate the correlation coefficient. The first option is to only get the correlation coefficient by itself, without any additional statistical tests. To do this, you use the `cor()` function.\n\nThe `cor()` function is fairly easy to use: \n\n1. You need to state the two variables that you want to correlate (as `x` and `y`);\n2. You also need to tell `R` what to do with missing observations; you do this by specifying the `use` option. In this case, we tell `R` to use only non-missing observations (`complete.obs`). This is equivalent to using `na.rm` in the `mean()` or `sd()` functions.\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(x = ess7$agea, y = ess7$relig_num, \n    use = \"complete.obs\")\n## [1] 0.219565\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n\n\nWe get a correlation coefficient of 0.22. This means `agea` and `relig_num` are *weakly positively* correlated. This speaks for our hypothesis in general --- but the relationship is not very strong.\n\n\\newpage\n## Significance testing with `cor.test()`\n\nNow that we know that there is a positive association between age and religiosity, the next question is: *is this association statistically significant?* In other words, is our finding only the result of random sampling variation, or is there really a systematic relationship between age and religiosity in the underlying population?\n\nThe `cor.test()` function allows you to run the significance test that is explained in Kellstedt and Whitten (2018, 183-184). To use this function, you just specify the two variables you want to use. It will take care of the `NA`s by itself.\n\n::: {.callout-important title=\"There are different ways in which you can use this function:\"}\n\n* By default, the function tests if the correlation coefficient is *different from 0* or not. In other words, it tests if the two variables are associated *in some way*, be it positive or negative. (This is also the procedure that is described in Kellstedt and Whitten). \n* You can also let the function test more specifically if the correlation coefficient is *larger than 0*---if there is a positive correlation. To do that, you set the `alternative` option to `\"greater\"`.\n* And you can test specifically if the correlation coefficient is *smaller than 0*---if there is a negative relationship. In that case, you set `alternative=\"less\"`.\n\n:::\n\n\\newpage\n### Testing if the true *r* is equal to 0 or not (default setting)\nFor example, to test if there is *any* relationship, positive or negative, between `agea` and `relig_num`, you would just run the following:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(x = ess7$agea, y = ess7$relig_num)\n## \n## \tPearson's product-moment correlation\n## \n## data:  ess7$agea and ess7$relig_num\n## t = 8.5076, df = 1429, p-value < 2.2e-16\n## alternative hypothesis: true correlation is not equal to 0\n## 95 percent confidence interval:\n##  0.1696758 0.2683316\n## sample estimates:\n##      cor \n## 0.219565\n```\n:::\n\n::: {.cell}\n\n:::\n\n\n\n\n\nIf you take a quick look at the output, you see under `alternative` what `R` has been testing: \n\n* The *alternative* hypothesis is that the true *r* **is not equal** to 0.\n* The corresponding *null* hypothesis (which is not specifically mentioned in the output) is then that the true *r* **is equal** to 0.\n\nAnd, since you have now learned about two other statistical tests and their interpretation, you should know what to look for next: the *p*-value! Based on this result, can we reject the null hypothesis (see also Kellstedt/Whitten 2018, Chapter 8)? As in the previous tutorial on the *t*-test, you can use `format.pval()` to translate the *p*-value into something that is a bit easier to interpret:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nformat.pval(2.2e-16, digits = 3, eps = 0.01)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"<0.01\"\n```\n\n\n:::\n:::\n\n\n\n\n\n\nAs in the previous tutorial, the result is a very small number -- smaller than 0.01.\n\nYou can read the other results as follows:\n\n* `sample estimates:` shows you the sample correlation coefficient, which you already know from earlier;\n* `95 percent confidence interval:` is obviously the 95% confidence interval for the correlation coefficient. \n* `t` is the *t* statistic. As explained in Kellstedt and Whitten (2018, 183), we use the *t*-test in this case to test if the correlation coefficient is equal to 0 or not;\n* `df` are the degrees of freedom (see Kellstedt and Whitten 2018, 183);\n\n\\newpage\n### Testing if the the true *r* is greater than 0\nBut we actually started from the more specific hypothesis that there is a *positive* relationship between `age` and `relig_num`---that *r* should be greater than 0. \n\nTo test if this is true, we need to run:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(x = ess7$agea, y = ess7$relig_num,\n         alternative = \"greater\")\n## \n## \tPearson's product-moment correlation\n## \n## data:  ess7$agea and ess7$relig_num\n## t = 8.5076, df = 1429, p-value < 2.2e-16\n## alternative hypothesis: true correlation is greater than 0\n## 95 percent confidence interval:\n##  0.1777628 1.0000000\n## sample estimates:\n##      cor \n## 0.219565\n```\n:::\n\n\n\n\n\nAnd if you take another quick look at the output, you see that `R` has been testing a different hypothesis now: \n\n* The *alternative* hypothesis is now that the true *r* (in the population) is greater than 0.\n* The corresponding *null* hypothesis is that the true *r* is equal to or smaller than 0.\n\nAs before, can we reject the null hypothesis here -- and what would that mean substantively?\n\n\\newpage\n### Testing if the true *r* is smaller than 0\n\nFinally, let's see what happens if we test if the true *r* is smaller than 0. In this case, the hypotheses are as follows:\n\n* The *alternative* hypothesis is that the true *r* is smaller than 0.\n* The corresponding *null* hypothesis is that the true *r* is equal to or larger than 0.\n\nTo test this, we set `alternative=\"less\"`:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor.test(x = ess7$agea, y = ess7$relig_num,\n         alternative = \"less\")\n## \n## \tPearson's product-moment correlation\n## \n## data:  ess7$agea and ess7$relig_num\n## t = 8.5076, df = 1429, p-value = 1\n## alternative hypothesis: true correlation is less than 0\n## 95 percent confidence interval:\n##  -1.0000000  0.2605762\n## sample estimates:\n##      cor \n## 0.219565\n```\n:::\n\n\n\n\n\n\nDo you see the difference to the earlier results? Can you figure out what this means?\n\n\n\\newpage\n# Visualization\n\nYou may be wondering: The previous two tutorials included a part on descriptive statistics before showing how to run the formal statistical tests, but not this one --- why?! \n\nIt is also quite important to visualize the data when looking at correlations because two variables can have the exact same degree of correlation between them but the actual relationship can be very different. The statistican Francis Anscombe famously illustrated this with the four graphs below (also known as \"Anscombe's Quartet\").^[Anscombe, F. J. (1973). Graphs in statistical analysis. *The American Statistician*, 27(1):17–21.] The underlying data are available as a built-in dataset in `R` called `anscombe`:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nanscombe\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   x1 x2 x3 x4    y1   y2    y3    y4\n1  10 10 10  8  8.04 9.14  7.46  6.58\n2   8  8  8  8  6.95 8.14  6.77  5.76\n3  13 13 13  8  7.58 8.74 12.74  7.71\n4   9  9  9  8  8.81 8.77  7.11  8.84\n5  11 11 11  8  8.33 9.26  7.81  8.47\n6  14 14 14  8  9.96 8.10  8.84  7.04\n7   6  6  6  8  7.24 6.13  6.08  5.25\n8   4  4  4 19  4.26 3.10  5.39 12.50\n9  12 12 12  8 10.84 9.13  8.15  5.56\n10  7  7  7  8  4.82 7.26  6.42  7.91\n11  5  5  5  8  5.68 4.74  5.73  6.89\n```\n\n\n:::\n:::\n\n\n\n\n\n\n\\newpage\nWhen visualizing the relationship between each of the `x` and `y` pairs, we get the following pattern:\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](tutorial_7b_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nAs mentioned, there is *technically speaking* the same degree of correlation between the variables (indicated by the gray dashed lines) --- but the underlying patterns are obviously very different. This is often the case when we work with real data, and it is therefore usually important to visualize relationships between two variables in addition to calculating correlation coefficients or (later) regression models.\n\nThe following parts of the tutorial show you different options for how to visualize the relationship between age and religiosity.\n\n## `geom_point()`\n\nPerhaps you remember that it is fairly easy to visualize the relationship between two continuous variables with a scatter plot (i.e., with `geom_point()`). And this is true, in principle. In most cases, `geom_point()` will get the job done.\n\nBut, unfortunately, not in this case. Consider what happens when we try to visualize the data with `geom_point()`:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ness7 %>% \n  ggplot(aes(x = agea, y = relig_num)) +\n    geom_point()\n## Warning: Removed 5 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n```\n\n::: {.cell-output-display}\n![](tutorial_7b_files/figure-html/scat-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nYou can see clearly that, well, you cannot really see anything clearly. \n\nThe problem: While `agea` is a proper metric variable that can have any value between 0 and somewhere above 100, this is not the case for the `relig_num` variable. This variable is technically an ordinal variable and can only have whole numbers between 0 and 10 (1,2,3,...) but not any numbers in between (e.g., 1.2234, 6.2562). This means that the observations on the `relig_num` variable all fall into a few discrete categories, and this makes it very difficult to see where in the graph most observations are, and if there is any association in the data.\n\n\\newpage\n## `geom_jitter()`\nOne thing you can do in this case is to \"jitter\" the data. \"Jittering\" means that you gently shake the data out of their discrete categories. In technical terms, you add a bit of random noise to the data.\n\nTo \"jitter\" the data in the graph, you just replace `geom_point()` with `geom_jitter()`:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ness7 %>% \n  ggplot(aes(x = agea, y= relig_num)) +\n    geom_jitter() +\n    labs(caption = \"The data were jittered.\")\n## Warning: Removed 5 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n```\n\n::: {.cell-output-display}\n![](tutorial_7b_files/figure-html/jit-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nNow it is *a bit* easier to see where most of the observations are. Important: If you jitter your data, you need to tell your readers (e.g., by including a note with the `caption` option).\n\nStill, the graph is hard to read and it is not clear what, if any, relationship there is between the data.\n\n\\newpage\nYou can make it a bit easier to see where observations \"pile up\" by making the points more transparent. To do that, you use the `alpha` option within `geom_jitter()`. `alpha` can range from 0 (completely transparent/invisible) to 1 (completely intransparent/solid).\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ness7 %>% \n  ggplot(aes(x = agea, y = relig_num)) +\n    geom_jitter(alpha = .3) +\n    labs(caption = \"The data were jittered.\")\n## Warning: Removed 5 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n```\n\n::: {.cell-output-display}\n![](tutorial_7b_files/figure-html/jit2-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nBut again, the pattern in the data is still difficult to see.\n\n\\newpage\n## `geom_smooth()`\n\nWhat you can do now is to add a \"fitted line\" to the graph. This means that `R` estimates a linear (i.e., OLS) regression (which you will learn about in the next weeks) and plots the predicted or \"fitted\" values from this estimation to the graph. This can show you more clearly any patterns in the data.\n\nTo add such a fitted line, you use `geom_smooth()`. Importantly, you also need to specify that you want this line to be based on a linear model (`method = \"lm\"`).^[Otherwise, `R` will use a fancy \"smoothed\" regression estimation procedure, which usually produces a very pretty result---but which is also difficult to understand. It is often better to keep it simple and just use the OLS method, which most people understand.] In this case, we also turn off the confidence intervals (`se = FALSE`), but you can of course show these if you like!\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ness7 %>% \n  ggplot(aes(x = agea, y = relig_num)) +\n    geom_jitter(alpha = .3) +\n    geom_smooth(method = \"lm\", se = FALSE) +\n    labs(caption = \"The data were jittered.\")\n## `geom_smooth()` using formula = 'y ~ x'\n## Warning: Removed 5 rows containing non-finite outside the scale range\n## (`stat_smooth()`).\n## Warning: Removed 5 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n```\n\n::: {.cell-output-display}\n![](tutorial_7b_files/figure-html/smooth-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nAnd now you can clearly see the *positive* association between the two variables: As age (`agea`) increases, religiosity (`relig_num`) increases as well!\n\n\\newpage\n\nWith some more polishing, we get a decent publication-quality graph:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ness7 %>% \n  ggplot(aes(x = agea, y = relig_num)) +\n    geom_jitter(size = 3, alpha = .3) +\n    geom_smooth(method = \"lm\", se = FALSE, \n                color = \"black\", linetype = \"dashed\") +\n    labs(x = \"Age\", y = \"''How religious are you?''\",\n         caption = \"The data were jittered.\") +\n    theme_classic()\n## `geom_smooth()` using formula = 'y ~ x'\n## Warning: Removed 5 rows containing non-finite outside the scale range\n## (`stat_smooth()`).\n## Warning: Removed 5 rows containing missing values or values outside the scale range\n## (`geom_point()`).\n```\n\n::: {.cell-output-display}\n![](tutorial_7b_files/figure-html/smooth2-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nNote that we made the points a bit larger (`size = 3`) and changed the color and type of the fitted line.\n\n\\newpage\n\n## `geom_hex()`\n\nA different --- and more fancy --- way to visualize the data is with a so called *hexbin* plot. A hexbin plot is, in essence, a two-dimensional histogram: The computer (or you as the user) defines a set of \"bins\" to put the data into, and then visualizes the number of observations within each bin with a different shade or color. The difference to a regular histogram is that this happens in two dimensions, and the bins have a hexagonal shape.\n\nTo create a hexbin plot with `ggplot()`, you use `geom_hex()`:\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ness7 %>% \n  ggplot(aes(x = agea, y = relig_num)) +\n    geom_hex()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 5 rows containing non-finite outside the scale range\n(`stat_binhex()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](tutorial_7b_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nObviously, this is not yet very fancy looking, so we have do a bit of polishing.\n\n\\newpage\n\n### Adjusting the binsize\n\nJust like with a regular histogram, we can adjust either the number of bins or the size of each bin. You may notice that the plot above uses relatively small hexagonals to group the data, and the pattern in the data might become clearer if we use larger bins. To do that, we adjust the `binwidth` option within `geom_hex()`:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ness7 %>% \n  ggplot(aes(x = agea, y = relig_num)) +\n    geom_hex(binwidth = c(5,1))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 5 rows containing non-finite outside the scale range\n(`stat_binhex()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](tutorial_7b_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nNote that we specify **two** numbers here: The size of bins along the x-axis (5) and their size along the y-axis (1). Basically, we group the age-variable into 5-year intervals and the reliosity-variable into 1-step intervals. (You can play around with different bin sizes to see what makes most sense.)\n\n\\newpage\n\n### Adjusting the color\n\nThe standard blue fill color might not be ideal in all circumstances. It is also not ideal that a lighter shade means \"more observations\" --- many readers might find the opposite more intuitive.\n\nWe can change the fill color with the `scale_fill_gradient()` option. Here, we specify a gray scheme (which is ideal if the result is supposed to be printed out on paper) and we declare that light shades should represent few observations and dark shades should indicate that there are many observations:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ness7 %>% \n  ggplot(aes(x = agea, y = relig_num)) +\n    geom_hex(binwidth = c(5,1)) +\n    scale_fill_gradient(low = \"grey80\", high = \"grey10\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 5 rows containing non-finite outside the scale range\n(`stat_binhex()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](tutorial_7b_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nAlternatively, if you really want colors, you can use the viridis color scheme, a color-blind friendly scheme that you can call with `scale_fill_viridis_c()`:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ness7 %>% \n  ggplot(aes(x = agea, y = relig_num)) +\n    geom_hex(binwidth = c(5,1)) +\n    scale_fill_viridis_c(direction = -1)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 5 rows containing non-finite outside the scale range\n(`stat_binhex()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](tutorial_7b_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nNote that we specify `direction = -1` to make sure that lighter colors represent fewer observations and vice versa.\n\n### Final polishing\n\nFinally, we can again add a fitted line, add informative labels, move the legend to the bottom, and adjust the overall theme to give the graph a more polished look:\n\n\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ness7 %>% \n  ggplot(aes(x = agea, y = relig_num)) +\n    geom_hex(binwidth = c(5,1)) +\n    scale_fill_viridis_c(direction = -1) +\n    geom_smooth(method = \"lm\", se = FALSE, \n                color = \"grey\", linetype = \"dashed\") +\n    labs(x = \"Age\", y = \"''How religious are you?''\",\n         caption = \"Binwidths = 5 (age) & 1 (religiosity)\",\n         fill = \"Observations\") +\n    theme_classic() +\n    theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 5 rows containing non-finite outside the scale range\n(`stat_binhex()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 5 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](tutorial_7b_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\n\n\n\nAs above, it is visible that there is a positive relationship between age and religiosity --- older people tend to be more religius --- but the relationship is driven mainly by two groups of observations: A larger group of younger people that is not religious at all, and people between 50 and 75 who are moderately religious. The pattern here roughly corresponds to case #3 in Anscombe's Quartet above.\n\n\\newpage\n\n# Conclusion\n\nThis was the last of the three tutorials on bivariate statistical tests and how you implement them in `R`. You now now how to look for relationships between different types of variables:\n\n* Two categorical variables with the $\\chi^2$ test.\n* One binominal and one continuous variable with the difference-of-means *t*-test.\n* Two continuous variables with the Pearson correlation coefficient (*r*). \n\nAs before, there are de-bugging exercises you can do to practice more. You know the drill by now...this time: the correlation between body height and body weight.\n\n<!-- # Exercises -->\n\n<!-- ## Exercise 1: Immigrants' effects on religious practices (again...) -->\n\n<!-- You will remember the `rlgueim` variable from the previous tutorial --- respondents were asked if they think immigrants undermine or enrich religious practices in their host countries. In this exercise, you will check if people's attitudes toward this issue vary depending on their age. -->\n\n<!-- **Assumption: You have gone through the entire tutorial, step by step and without errors, until here.** -->\n\n<!-- 1. Make sure you are familiar with the `rlgueim` variable: What does it measure *exactly*, and how does it look like? Apply any necessary transformations to the variable to prepare it for a statistical analysis. -->\n<!-- 2. Calculate the Person correlation between the `rlgueim` variable (or a transformed version) and `agea`; -->\n<!-- 3. Test if this correlation is statistically significant; -->\n<!-- 4. Interpret the results; -->\n\n\n",
    "supporting": [
      "tutorial_7b_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}