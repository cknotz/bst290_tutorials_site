{
  "hash": "83eec99e672b347ed7bf39608ddefa85",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Simulating survey studies & calculating confidence intervals\"\nauthor: \"Carlo Knotz\"\nbibliography: /Users/carloknotz/Documents/BibDesk_library/library.bib\ntoc: true\nnumber-sections: true\nformat:\n  html: default\n  pdf: default\nlightbox: true\nlang: en\neditor_options: \n  chunk_output_type: console\n---\n\n## Introduction\n\nThis tutorial shows how you can simulate many, many different iterations of a fictional survey study -- and how the different results are, collectively, distributed. In other words, it shows you the main idea behind the *Central Limit Theorem* and how it is used in applied social research to learn about a population based on small random samples.\n\n**Important:** Some of these simulations take a few minutes to run. Be patient.\n\n\\newpage\n### Setup\n\nThese are the packages you'll need (use `install.packages()` to install anything that is missing, which is likely `ggpubr`):\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n-- Attaching core tidyverse packages ------------------------ tidyverse 2.0.0 --\nv dplyr     1.1.4     v readr     2.1.5\nv forcats   1.0.0     v stringr   1.5.1\nv ggplot2   4.0.0     v tibble    3.2.1\nv lubridate 1.9.4     v tidyr     1.3.1\nv purrr     1.2.0     \n-- Conflicts ------------------------------------------ tidyverse_conflicts() --\nx dplyr::filter() masks stats::filter()\nx dplyr::lag()    masks stats::lag()\ni Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\ntheme_set(theme_classic())\nlibrary(ggpubr)\n```\n:::\n\n\n\\newpage\n\n## Simulating a population and drawing many samples from it\n\n### Simulating our fictional population\n\nImagine that we want to figure out the average level of happiness in a country similar to Norway, with a population of 5.5 million people. We measure happiness on a 0-10 scale.\n\nIn `R`, we can create or simulate a such a population of 5.5 million individuals and their levels of happiness with a few lines of code, as shown below. The main tool we use here is the `sample()` function, which -- as the same suggests -- samples randomly from a pre-specified set of numbers.^[See also `?sample` for details.] In our case, we want to sample from the numbers 0 to 10, which we specify as `seq(0,10,1)` (\"from 0 to 10, in steps of 1\"), we want to do that 5.5 million times (`size = 5.5*10^6`), and we want the different numbers to have certain probabilities to be chosen (which all need to add up to 1, or 100%). \n\nWe run the `sample()` function within a function to create a `data.frame` and save the result as the variable `happy`. We also create another variable called `idno`, which is just a unique ID for each of the 5.5 million \"individuals\". The resulting `data.frame` -- our \"population\" -- gets saved as `pop`.\n\nNote that we also set a \"seed\" number so that we can always exactly reproduce the same results later; otherwise there will be small random variations in the results. You always need to run the `set.seed()` function together with the code immediately below for results to stay the same every time you run the analysis:\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\npop <- data.frame(idno = seq(1,5.5*10^6,1),\n                  happy = sample(seq(0,10,1),\n                                 size = 5.5*10^6,\n                                 replace = T,\n                                 prob = c(0.01,.01,.02,.03,.04,.09,.10,.13,.20,.29,0.05)))\n```\n:::\n\n\nThe \"population\" will now appear as `pop` in your *Environment* tab.\n\nWe can then calculate the \"true\" average level of happiness in our population. This is the \"target\" value we are after. In a real social scientific study, this is the value that we do not know (because we very rarely have data for an entire population) but want to measure with a social survey:\n\n::: {.cell}\n\n```{.r .cell-code}\npopmean <- mean(pop$happy)\n```\n:::\n\n\nWe can also visualize the distribution of happiness in the population plus the average value as a vertical line:\n\n::: {.cell}\n\n```{.r .cell-code}\npop |> \n  group_by(happy) |> \n  summarize(obs = n()) |> \n  mutate(share = obs/sum(obs)) |> \n  ggplot(aes(x = happy, y = share)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = round(100*share, digits = 1)), vjust = -1) +\n  geom_vline(xintercept = popmean, color = \"orange\") +\n  scale_x_continuous(breaks = seq(0,10,1)) +\n  scale_y_continuous(labels = scales::percent,\n                     limits = c(0,.35)) +\n  labs(y = \"Percent\", x = \"How happy are you?\",\n       title = \"Entire population of 5.5 mil. (simulated)\",\n       caption = paste0(\"Mean in population ('true value') = \",round(popmean, digits = 2))) -> popvis\n\npopvis\n```\n\n::: {.cell-output-display}\n![](surveysim_files/figure-pdf/unnamed-chunk-4-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\\newpage\n### Taking a first sample\n\nNow we pretend that we are doing a social science survey by taking a random sample of 1000 \"respondents\" from the population with `slice_sample()`. Here again, we set a seed number so that we get the same results every time.\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nsam1 <- pop |> \n  slice_sample(n = 1000)\n```\n:::\n\n\nIf you take a look at your *Environment* tab, you see the sample data as `sam1`: 1000 random picks from the \"population\" (`pop`) and their levels of happiness.\n\nThen we also calculate the average level of happiness in the sample and visualize the sample data:\n\n::: {.cell}\n\n```{.r .cell-code}\nsam1mean <- mean(sam1$happy)\n\nsam1 |> \n  group_by(happy) |> \n  summarize(obs = n()) |> \n  mutate(share = obs/sum(obs)) |> \n  ggplot(aes(x = happy, y = share)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = round(100*share, digits = 1)), vjust = -1) +\n  geom_vline(xintercept = sam1mean, color = \"orange\") +\n  scale_x_continuous(breaks = seq(0,10,1)) +\n  scale_y_continuous(labels = scales::percent,\n                     limits = c(0,.35)) +\n  labs(y = \"Percent\", x = \"How happy are you?\",\n       title = \"Sample 1 (N = 1000; randomly selected)\",\n       caption = paste0(\"Mean in sample ('estimate') = \",round(sam1mean, digits = 2))) -> sam1vis\n\nsam1vis\n```\n\n::: {.cell-output-display}\n![](surveysim_files/figure-pdf/unnamed-chunk-6-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nWe can also use `ggarrange()` from the `ggpubr` package to directly compare the sample and population data:\n\n::: {.cell}\n\n```{.r .cell-code}\nggpubr::ggarrange(popvis,sam1vis, nrow = 2)\n```\n\n::: {.cell-output-display}\n![](surveysim_files/figure-pdf/unnamed-chunk-7-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nIf you take a careful look, you see that the overall pattern is the same, but there are small variations in the percentages and in the average value.\n\n\\newpage\n### Taking a second sample\n\nLet's now take a second sample. Here, we specicy a different seed number so that we don't get the same result again (but so that the entire analysis is still reproducible):\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(17)\nsam2 <- pop |> \n  slice_sample(n = 1000)\n```\n:::\n\n\nThen we again calculate the sample mean in the second sample and visualize the new sample data next to the original population data:\n\n::: {.cell}\n\n```{.r .cell-code}\nsam2mean <- mean(sam2$happy)\n\nsam2 |> \n  group_by(happy) |> \n  summarize(obs = n()) |> \n  mutate(share = obs/sum(obs)) |> \n  ggplot(aes(x = happy, y = share)) +\n  geom_bar(stat = \"identity\") +\n  geom_text(aes(label = round(100*share, digits = 1)), vjust = -1) +\n  geom_vline(xintercept = sam2mean, color = \"orange\") +\n  scale_x_continuous(breaks = seq(0,10,1)) +\n  scale_y_continuous(labels = scales::percent,\n                     limits = c(0,.35)) +\n  labs(y = \"Percent\", x = \"How happy are you?\",\n       title = \"Sample 2 (N = 1000; randomly selected)\",\n       caption = paste0(\"Mean in sample ('estimate') = \",round(sam2mean, digits = 2))) -> sam2vis\n\nggpubr::ggarrange(popvis,sam2vis, nrow = 2)\n```\n\n::: {.cell-output-display}\n![](surveysim_files/figure-pdf/unnamed-chunk-9-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nYou should see again that there are small differences between the sample and the population, but the overall patterns are very similar.\n\n\\newpage\n### Comparing the samples\n\nYou can see these small differences also when we compare the two samples directly with each other:\n\n::: {.cell}\n\n```{.r .cell-code}\nggpubr::ggarrange(sam1vis,sam2vis, nrow = 2)\n```\n\n::: {.cell-output-display}\n![](surveysim_files/figure-pdf/unnamed-chunk-10-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\\newpage\n### Scaling it up to 1000 samples\n\nSo far, we took only two random samples with 1000 observations (\"respondents\") per sample. In a real-life scenario, this is about as much as we could realistically do. \n\nWithin this simulation, however, we can use `R` to take many, many samples from our \"population\" (almost) simultaneously and then see how the main statistic we are interested in, the sample average, varies between the different samples.\n\nTo do this, we first create a `data.frame` in which we later save the different simulation results: **a dataset of results**. We call this `sampdist`:\n\n::: {.cell}\n\n```{.r .cell-code}\nsampdist <- data.frame(sample = seq(1,1000,1),\n                       result = c(c(sam1mean,sam2mean),rep(NA,998)))\n```\n:::\n\n\nThis dataset includes two variables, one is `sample` which simply tells us the number of each sample from 1 to 1000. The other variable (called `result`) stores the average happiness value from each sample. To create the `results` variable, we start with the first two sample means and then add 998 `NA` values to get a total of 1000.\n\n\\newpage\nNext, we use a **loop** to simulate 1000 samples. A loop is a fundamental concept in programming (also outside of `R`). It is, very simply put, an instruction to the computer to repeat a certain operation a certain number of times, one after the other. \n\nIn `R`, you can specify loops with the `for()` function, as shown below. What we are tell `R` with this code is, translated to human language:\n\n  1. For all values of `k`, which we define to be the numbers between 3 and 1000...\n  2. ...go the `pop` dataset, draw a random sample from it, and store that sample as `loopsam`...\n  3. ...calculate the mean of the `happy` variable in that sample...\n  4. ...and then write the result, the mean of sample `k`, into the `k`-th row and the second column of our results dataset.\n  \nNotice that we put the code after the `for()` function within curly braces (`{}`) to show `R` that this all fits together and belows to the loop. We start at 3 because we already have values for the first two rows of the results dataset. \n\nBe aware: Loops are not very efficient and this calculation can take a few moments.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfor(k in 3:1000) {\n  \n  loopsam <- pop |> \n    slice_sample(n = 1000)\n  \n  sampdist[k,2] <- mean(loopsam$happy)\n}\n```\n:::\n\n\nLet's have a look at the first few observations in our results dataset:\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(sampdist)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  sample result\n1      1  7.283\n2      2  7.269\n3      3  7.158\n4      4  7.251\n5      5  7.348\n6      6  7.148\n```\n\n\n:::\n:::\n\n\nWhat you see here is the first 6 samples and their sample means. The first two are the same as above, and the other four were generated by the loop. You also see, and this is important, that the sample means vary *slightly*, but they are typically somewhere around 7.2.\n\n\\newpage\nLet's now calculate the average over all of these averages -- the \"average result\" -- and compare that to the \"true\" population mean from earlier:\n\n::: {.cell}\n\n```{.r .cell-code}\nsampdistmean <- mean(sampdist$result)\nsampdistmean\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.182107\n```\n\n\n:::\n\n```{.r .cell-code}\npopmean\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.185346\n```\n\n\n:::\n:::\n\n\nThe average result -- the \"mean of all means\" -- is 7.18.\n\nThen we can also visualize the distribution of all our different test results together with the \"true\" population mean from earlier and the average result:\n\n::: {.cell}\n\n```{.r .cell-code}\nsampdist |> \n  ggplot(aes(x = result)) +\n  geom_histogram(color = \"white\") +\n  geom_vline(xintercept = sampdistmean, color = \"cornflowerblue\", linetype = \"dashed\") +\n  geom_vline(xintercept = popmean, color = \"orange\") +\n  labs(x = \"Sample means\", y = \"Samples\",\n       title = \"Distribution of results ('sampling distribution')\",\n       caption = paste0(\"Population mean ('true value'; orange) = \",round(popmean, digits = 2),\n                        \"\\n Mean over all results ('mean of means'; blue, dashed) = \",round(sampdistmean, digits = 2)))\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](surveysim_files/figure-pdf/unnamed-chunk-15-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nDo you notice something here? What does this distribution remind you of, and why do you think do we get this shape (especially since the original distribution does not at all look like that? If you can answer this question, you have understood the main idea behind the *Central Limit Theorem*).\n\n\\newpage\n### Then we do 10'000 surveys\n\n1000 simulated results are nice, but we *can* also go higher, let's say to 10'000. To do that, we create a new results `data.frame` (which we save under the same name as before), but now with 10'000 rows instead of 1000:\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(42)\nsampdist <- data.frame(sample = seq(1,10000,1),\n                       result = c(c(sam1mean,sam2mean),rep(NA,9998)))\n```\n:::\n\n\nOnce we have that, we again run a loop, but from numbers 3 to 10'000. Caution, this now takes a bit longer still:\n\n::: {.cell}\n\n```{.r .cell-code}\nfor(k in 3:10000) {\n  \n  loopsam <- pop |> \n    slice_sample(n = 1000)\n  \n  sampdist[k,2] <- mean(loopsam$happy)\n}\n```\n:::\n\n\nLet's have a look at new \"mean of means\" of the 10'000 new samples and compare it to the overall population mean:\n\n::: {.cell}\n\n```{.r .cell-code}\nsampdistmean <- mean(sampdist$result)\nsampdistmean\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.185184\n```\n\n\n:::\n\n```{.r .cell-code}\npopmean\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.185346\n```\n\n\n:::\n:::\n\n\nNotice how they are very close to each other, but still not exactly identical?\n\n\\newpage\nThen we also visualize the new distribution of all 10'000 sample means:\n\n::: {.cell}\n\n```{.r .cell-code}\nsampdist |> \n  ggplot(aes(x = result)) +\n  geom_histogram(color = \"white\") +\n  geom_vline(xintercept = sampdistmean, color = \"cornflowerblue\", linetype = \"dashed\") +\n  geom_vline(xintercept = popmean, color = \"orange\") +\n  labs(x = \"Sample means\", y = \"Samples\",\n       title = \"Distribution of results ('sampling distribution')\",\n       caption = paste0(\"Population mean ('true value'; orange) = \",round(popmean, digits = 2),\n                        \"\\n Mean over all results ('mean of means'; blue, dashed) = \",round(sampdistmean, digits = 2))) \n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](surveysim_files/figure-pdf/unnamed-chunk-19-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nDo you notice what changed compared to the 1000 results earlier? Why would that be?\n\n\\newpage\n### The results in context\n\nYou might now get the impression that there is quite a bit of variation between the different samples, that the result \"jumps\" quite a bit between different repetitions of our fictional survey. However, notice also that the scale of the above graph only covers a small part of the entire scale of our original happiness variable, which goes from 0 to 10.\n\nTo show the \"sampling variation\", the random differences between our 10'000 samples, in context, we expand the scale of our x-axis to the true scale of the original variable with the `limits` option in the code below:\n\n::: {.cell}\n\n```{.r .cell-code}\nsampdist |> \n  ggplot(aes(x = result)) +\n  geom_histogram(color = \"white\", binwidth = 0.05) +\n  scale_x_continuous(limits = c(0,10), \n                     breaks = seq(0,10,1)) +\n  labs(x = \"Sample means\", y = \"Samples\",\n       title = \"Distribution of results ('sampling distribution')\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](surveysim_files/figure-pdf/unnamed-chunk-20-1.pdf){fig-pos='H'}\n:::\n:::\n\n\n\\newpage\n### Comparing our results distribution (\"sampling distribution\") to the Normal distribution\n\nYou might (should) have already noticed what how our different \"survey\" results are distributed, but just to make sure: Here is a comparison of our sampling distribution to the normal distribution:\n\n::: {.cell}\n\n```{.r .cell-code}\nsampdist |> \n  ggplot(aes(x = result)) +\n  geom_histogram(color = \"white\") +\n  geom_vline(xintercept = sampdistmean, color = \"cornflowerblue\", linetype = \"dashed\") +\n  geom_vline(xintercept = popmean, color = \"orange\") +\n  labs(x = \"Sample means\", y = \"Samples\",\n       title = \"Distribution of results ('sampling distribution')\") -> samdist10k\nsamdist10k\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](surveysim_files/figure-pdf/unnamed-chunk-21-1.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\np1 <- ggplot(data = data.frame(x = c(-3, 3)), aes(x)) +\n  stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1)) + ylab(\"\") +\n  scale_y_continuous(breaks = NULL) +\n  scale_x_continuous(breaks = NULL) +  \n  labs(x = \"\", title = \"Normal distribution\")\np1\n```\n\n::: {.cell-output-display}\n![](surveysim_files/figure-pdf/unnamed-chunk-21-2.pdf){fig-pos='H'}\n:::\n\n```{.r .cell-code}\nggpubr::ggarrange(samdist10k,p1, ncol = 2)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](surveysim_files/figure-pdf/unnamed-chunk-21-3.pdf){fig-pos='H'}\n:::\n:::\n\n\n\\newpage\n### Now 10'000 new samples, but with a smaller sample size (200 instead of 1'000)\n\nIn the previous simulations, we always worked with a sample size of 1000. First, we took 1000 samples of 1000 \"respondents\" each, then we took 10'000 samples of 1000 respondents. \n\nLet's see what happens when we take 10'000 new samples, but now with a size of 200 \"respondents\" per sample:\n\n::: {.cell}\n\n```{.r .cell-code}\nsampdist_200 <- data.frame(sample = seq(1,10000,1),\n                           result = rep(NA,10000))\n\nset.seed(42)\nfor(k in 1:10000) {\n  \n  loopsam <- pop |> \n    slice_sample(n = 200)\n  \n  sampdist_200[k,2] <- mean(loopsam$happy)\n}\n\nsampdist_200mean <- mean(sampdist_200$result)\nsampdist_200mean\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.184244\n```\n\n\n:::\n\n```{.r .cell-code}\npopmean\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.185346\n```\n\n\n:::\n:::\n\n\nThe new \"mean of means\", now based on 200 \"respondents\" per sample, is still very close to the \"true\" population mean.\n\nTo see what changes when we use smaller samples, we show both sampling distributions, the one for the samples with $N = 1000$ and the new one for the samples with $N = 200$, in the same graph. We also calculate the standard deviation of each of the distributions:\n\n::: {.cell}\n\n```{.r .cell-code}\nsampdist_SD <- sd(sampdist$result)\nsampdist_200_SD <- sd(sampdist_200$result)\n\nsampdist |> \n  left_join(sampdist_200, by = \"sample\",\n            suffix = c(\"1000\",\"200\")) |> \n  pivot_longer(cols = -1,\n               names_to = \"size\",\n               values_to = \"meanval\") |> \n  mutate(size = gsub(\"result\",\"\",size)) |> \n  ggplot(aes(x = meanval,fill = size)) +\n  geom_histogram(alpha=0.3, position=\"identity\", \n                 color = \"grey\") +\n  scale_fill_manual(values = c(\"cornflowerblue\",\"orange\")) +\n  labs(x = \"Sample means\", y = \"Samples\",\n       fill = \"Size of each sample\",\n       title = \"Distribution of results ('sampling distribution')\",\n       caption = paste0(\"Population mean ('true value') = \",round(popmean, digits = 2),\n                        \"\\n Mean over all results ('mean of means') for samples of 1000 = \",\n                        round(sampdistmean, digits = 2),\" (SD = \",round(sampdist_SD, digits = 2),\")\",\n                        \"\\n Mean over all results ('mean of means') for samples of 200 = \",\n                        round(sampdist_200mean, digits = 2),\" (SD = \",round(sampdist_200_SD, digits = 2),\")\")) +\n  theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![The variability of results when using different sample sizes](surveysim_files/figure-pdf/fig-sizecomp-1.pdf){#fig-sizecomp fig-pos='H'}\n:::\n:::\n\n\nDo you notice where the main difference is between the sampling distribution based on $N = 1000$ and the one based on $N = 200$?\n\nIf you had to design a social survey to measure happiness (or something else) in the Norwegian population, how could you use the results of this simulation to guide your study design?\n\n## How can we use this to say something about how *confident* we can be about our sample data?\n\nThe previous simulations illustrated a few important things about taking random samples from a population:\n\n1. If you repeatedly take many, many random samples from a population and calculate the average value of some variable in each sample, then each sample will give you a different result. \n2. All taken together, however, the different sample means are distributed normally around the true population mean: Some sample means are a little bit lower than the true population mean, some are a bit higher, but there are about as many higher ones than lower ones (the distribution is symmetric).\n3. The average result --- the average over all sample means --- will be very close to the true population mean. If you would take an infinite number of samples, the average result would be identical to the true mean.\n4. Sample means are most likely to be close to the true mean. It *can* happen that individual samples are far off target, but this is rare.\n5. The larger the individual samples, the closer the individual sample means are to the true mean (see also @fig-sizecomp). Smaller samples are more \"wiggly\", meaning that the individual means vary more strongly around the true population mean.\n\n\nIn a real-life survey study, we only ever work with a single sample, but we can use what we learned to say something about the results we get from that single sample. That is the foundation of *inferential statistics*.\n\n### Quantifying the variation between samples\n\nIn @fig-sizecomp, we used the standard deviation (SD) to quantify the spread or variability in our results. The main result was that there is less variability in results based on larger compared to smaller samples.\n\nJust to reiterate what we did there, we calculate the standard deviations again:\n\n::: {.cell}\n\n```{.r .cell-code}\n# SD for sample means based on N=1000\nsd(sampdist$result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.06754951\n```\n\n\n:::\n\n```{.r .cell-code}\n# SD f or sample means based on N=200\nsd(sampdist_200$result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1522054\n```\n\n\n:::\n:::\n\n\nThis means that the \"typical\" result based on a sample size of $N=1000$ is on average ca. 0.7 units away from the true population mean, while the typical result based on a sample size of $N=200$ is on average ca. 0.15 units away from the true population mean. In other words, larger samples are less \"wiggly\" than smaller samples. In yet other words, increasing the sample size from 200 to 1000 cuts the \"wiggliness\" of our results in half.\n\nThese standard deviations of our sample means are also called *standard errors*. They quantify the extent to which our sample results vary by random chance across different repetitions of our survey. Again: Smaller sample size = more random variation, larger sample size = less random variation. \n\n### Calculating the standard error based on a single sample\n\nIn a real-life scenario, we cannot take repeated samples from a population --- but we can still figure out what the standard error or the random variation in results would *hypothetically* be *if* we could draw many repeated samples using a simple formula.\n\nAs explained in Solbakken [-@Solbakken2019, chapter 5.7], the formula to calculate the standard error of a sample mean is:\n\n$$ SE = \\frac{SD}{\\sqrt{N}}$$\nIn human language, we calculate the sample standard deviation (SD) of whatever variable we are interested in, and then divide that by the square root of the sample size ($N$).\n\nLet's see if this works in our case. We go back to our very first sample (`sam1`), which included 1000 randomly drawn observations from the overall population.\n\nWe first calculate the standard deviation of the `happy` variable (the variable we are interested in):\n\n::: {.cell}\n\n```{.r .cell-code}\nsd_1000 <- sd(sam1$happy)\n```\n:::\n\n\nThen we divide the result by the square root of the sample size (1000):\n\n::: {.cell}\n\n```{.r .cell-code}\nse_1000 <- sd_1000/sqrt(1000)\nse_1000\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.06707764\n```\n\n\n:::\n:::\n\n\nThe result is almost the same as the standard error we got based on the simulation above.\n\nLet's see what happens if we instead use a sample size of $N=200$. To do that, we draw a single random sample of 200 observations from the population and repeat the calculation:\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(21)\npop |> \n  slice_sample(n = 200) -> singsam_200\n\nsd_200 <- sd(singsam_200$happy)\nse_200 <- sd_200/sqrt(200)\nse_200\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1512361\n```\n\n\n:::\n:::\n\n\nAgain, we are very close to the value that we got from our simulation.\n\n### Margins of errors\n\nThe standard error basically tells us how far away the average or \"typical\" result is from the true population mean based on a given sample size. \n\nThat is useful, but we often want to be more certain or \"confident\". We want to be able to confidently say that most results, say, 90 or 95% of all possible results, are within a certain range. \n\nWe can also do that. To do that, we use the fact that all sampling distributions --- the distributions of many different sample results --- based on random samples are approximately normal or \"bell-shaped\", and that (thanks to Carl Gauss) we can exactly quantify how many observations of any normal distribution fall within a given range. \n\nSpecifically, and as visualized in @fig-stnorm:\n\n - 90% of all observations fall within a range of $+1.645 \\times SD$ and $-1.645 \\times SD$ around the mean\n - 95% of all observations fall within a range of $+1.96 \\times SD$ and $-1.96 \\times SD$ around the mean\n - 99% of all observations fall within a range of $+2.576 \\times SD$ and $-2.576 \\times SD$ around the mean\n\n\n::: {.cell}\n::: {.cell-output-display}\n![Standard normal distribution with critical values indicated](surveysim_files/figure-pdf/fig-stnorm-1.pdf){#fig-stnorm}\n:::\n:::\n\n\nIf we now combine this with the results of our calculation and simulation above, then we can construct so called *margins of errors*: They tell us, simply put, how certain or uncertain our result is.\n\nLet's illustrate this with the first sample of 1000 observations. We know based on the calculation and simulation above that the *standard error* (the SD of our sampling distribution) for a sample size of $N=1000$ is roughly 0.07. We also know that, if we would repeat our survey many times over, 95% of all those hypothetical surveys will fall within +1.96 and -1.96 times this standard deviation of 0.07.\n\nThis means that the 95% margin of error for our first sample is therefore:\n\n::: {.cell}\n\n```{.r .cell-code}\n1.96*se_1000\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.1314722\n```\n\n\n:::\n:::\n\n \nIn detail, this means that if we would repeat our survey 100 times, then 95 of those surveys would produce a sample average that is within [+0.13;-0.13] from the true population mean.\n \nWe can use the same procedure to calculate the 95% margin of error for the smaller sample with $N=200$:\n\n::: {.cell}\n\n```{.r .cell-code}\n1.96*se_200\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2964228\n```\n\n\n:::\n:::\n\n \nYou see that the margin of error for the smaller sample is a bit more than twice that of the one for the larger sample. This means that if we would re-do our smaller survey with an $N$ of 200, then 95% of all sample means would be in a range of [0.3;-0.3] from the true population mean. Clearly, larger samples are more accurate, and with *margins of errors* we can quantify exactly how (in-)accurate a sample of a given size is.\n\n### Confidence intervals\n\nWe can then extend this logic further to construct an *interval* that (most likely) contains the true population value. To do that, we simply take the sample mean and then add and subtract the margin of error. For example, for a 95% confidence interval, we use the values of the standard normal distribution that include 95% of that distribution ($\\pm1.96$), multiply these with the standard *error*, and combine that with the sample mean ($\\bar{X}$):\n$$\nCI_{95} = \\bar{X} \\pm 1.96 \\times SE\n$$\n\nFor our large sample, this calculation looks as follows:\n\n::: {.cell}\n\n```{.r .cell-code}\nsam1mean <- mean(sam1$happy)\nlower_1000 <- sam1mean - 1.96 * se_1000\nlower_1000\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.151528\n```\n\n\n:::\n\n```{.r .cell-code}\nupper_1000 <- sam1mean + 1.96 * se_1000\nupper_1000\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.414472\n```\n\n\n:::\n:::\n\n\nJust to make sure, we compare our calculation with the one we can get with `t.test()`:\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(sam1$happy)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  sam1$happy\nt = 108.58, df = 999, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 7.151371 7.414629\nsample estimates:\nmean of x \n    7.283 \n```\n\n\n:::\n:::\n\n\nThe results are almost identical.^[The slight difference is because `R` uses a different type of distribution, the *t* distribution, which takes into account how many observations we really have in each of the two samples. The procedure we use here, which is based on the standard normal distribution, simply assumes that we have a \"large\" sample. In applied research, we usually (but not always) use the *t*-distribution.]\n\nEquivalently, for the smaller sample we get:\n\n::: {.cell}\n\n```{.r .cell-code}\nsam200_mean <- mean(singsam_200$happy)\nsam200_mean\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.28\n```\n\n\n:::\n\n```{.r .cell-code}\nlower_200 <- sam200_mean - 1.96 * se_200\nlower_200\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.983577\n```\n\n\n:::\n\n```{.r .cell-code}\nupper_200 <- sam200_mean + 1.96 * se_200\nupper_200\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 7.576423\n```\n\n\n:::\n:::\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(singsam_200$happy)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  singsam_200$happy\nt = 48.137, df = 199, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 6.981769 7.578231\nsample estimates:\nmean of x \n     7.28 \n```\n\n\n:::\n:::\n\n\nOnce, again, the results are almost identical.\n\nThe differences between the larger and smaller sample are easier to see if we visualize the two results together:\n\n::: {.cell}\n\n```{.r .cell-code}\ndata.frame(sample = c(\"1000\",\"200\"),\n           means = c(sam1mean,sam200_mean),\n           lower = c(lower_1000,lower_200),\n           upper = c(upper_1000,upper_200)) |> \n  ggplot(aes(x = means, xmin = lower, xmax = upper, y = sample)) +\n    geom_point() +\n    geom_linerange() +\n    geom_vline(xintercept = popmean, linetype = \"dashed\") +\n    labs(x = \"Mean value\", y = \"Sample size\",\n         caption = \"95% confidence intervals\",\n         title = paste0(\"True population mean = \",round(popmean, digits = 2)))\n```\n\n::: {.cell-output-display}\n![](surveysim_files/figure-pdf/unnamed-chunk-33-1.pdf){fig-pos='H'}\n:::\n:::\n\n \nIn this case, both of our confidence intervals include the true population mean and you see again how the larger sample has a smaller confidence interval.\n \n### How to read a confidence interval\n\nIf after all this simulating and math-ing your head is spinning wildly, you are definitely not alone or the first to experience this. Confidence intervals and margins of error are tricky and even \"senior\" researchers misunderstand how they work and what they really indicate [see e.g., @Greenlandetal2016; @Beliaetal2005; @Austin2002; @Schenker2001].\n\nWe can, however, again use simulations to clarify how to read a confidence interval. We start again from our original population and then draw 100 random samples of $N=1000$ observations per sample. For each of the samples, we calculate the sample mean of the `happy` variable and a 95% confidence interval around it. Then we visualize all results together with the true population mean.\n\nFirst, we create an empty `data.frame` to store the different results into:\n\n::: {.cell}\n\n```{.r .cell-code}\ncidata <- data.frame(sample_id = seq(1,100,1),\n                     sample_means = rep(NA,100),\n                     sample_lower = rep(NA,100),\n                     sample_upper = rep(NA,100))\nhead(cidata)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  sample_id sample_means sample_lower sample_upper\n1         1           NA           NA           NA\n2         2           NA           NA           NA\n3         3           NA           NA           NA\n4         4           NA           NA           NA\n5         5           NA           NA           NA\n6         6           NA           NA           NA\n```\n\n\n:::\n:::\n\n\nThen we run a loop, as above, and calculate both the sample means and the confidence intervals from each sample and add them to the results `data.frame`:\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(20)\nfor(k in 1:100){\n  pop |> \n    slice_sample(n = 1000) -> loopsam\n\ncidata[k,2] <- mean(loopsam$happy) # store the sample mean into the second column\n\n# Calculate CI\nloopci <- t.test(loopsam$happy)\n\ncidata[k,3] <- loopci$conf.int[1] # lower value\ncidata[k,4] <- loopci$conf.int[2] # upper value\n  \n}\n```\n:::\n\n \nLet's have a quick peek at our results:\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(cidata)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  sample_id sample_means sample_lower sample_upper\n1         1        7.194     7.057717     7.330283\n2         2        7.211     7.083211     7.338789\n3         3        7.218     7.087524     7.348476\n4         4        7.224     7.091715     7.356285\n5         5        7.151     7.018535     7.283465\n6         6        7.149     7.014419     7.283581\n```\n\n\n:::\n:::\n\n \nAs a last step, we also create a \"dummy\" variable that indicates if a given confidence interval overlaps with the true population mean or not:\n\n::: {.cell}\n\n```{.r .cell-code}\ncidata |> \n  mutate(overlap = case_when(\n    popmean < sample_lower | popmean > sample_upper ~ \"Miss\",\n    popmean >= sample_lower | popmean <= sample_upper  ~ \"Hit\")) -> cidata\n```\n:::\n\n \nNow we visualize the results, highlighting the confidence intervals that that were \"misses\" in red:\n\n::: {.cell}\n\n```{.r .cell-code}\ncidata |> \n  ggplot(aes(x = sample_means, xmin = sample_lower,\n             xmax = sample_upper, color = overlap,\n             y = sample_id)) +\n    geom_point() +\n    geom_linerange() +\n    geom_vline(xintercept = popmean, linetype = \"dashed\") +\n    scale_color_manual(values = c(\"grey40\",\"tomato\")) +\n    labs(x = \"Sample means & 95% confidence intervals\",\n         y = \"Sample ID\", color = \"\",\n         caption = \"Vertical dashed line indicates true population mean\") +\n    theme(legend.position = \"bottom\")\n```\n\n::: {.cell-output-display}\n![](surveysim_files/figure-pdf/unnamed-chunk-38-1.pdf){fig-pos='H'}\n:::\n:::\n\n\nYou see that in 95 out of the 100 \"studies\" we did, the 95% confidence interval \"hit\" the mark and overlaps with the true population mean. In five cases, however, we were unlucky and \"missed\": The confidence intervals do not overlap with the true population mean.\n\nAnd this leads to *one* correct interpretation of a confidence interval: *If we were to re-do our study many, many times, then in 95% of these studies, the 95% confidence interval would overlap with the true population mean.*\n\nThis interpretation, however, is a bit clunky and not that helpful in practice. To get to a second and perhaps more helpful interpretation, we go back to the confidence interval for the very first sample we took from the population:\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(sam1$happy)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tOne Sample t-test\n\ndata:  sam1$happy\nt = 108.58, df = 999, p-value < 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 7.151371 7.414629\nsample estimates:\nmean of x \n    7.283 \n```\n\n\n:::\n:::\n\n\nThis is also what a real-life result would look like: We have one single sample, one single sample mean, and one single confidence interval. In this case, this interval ranges from 7.15 to 7.41. \n\nBut we also know now that this one confidence interval is one of *many different possible* confidence intervals that we *could have gotten* --- and we know that each of these infinite hypothetical intervals has a 95% chance of including the true population mean. This means that our confidence interval from 7.15 to 7.41 has a 95% chance of including the true population mean. A less precise interpretation is that we are 95% confident that we have \"captured\" the true population mean with our confidence interval [see @Kellstedt2018, 153].\n\n\n\n\n\n\n\n\n\n",
    "supporting": [
      "surveysim_files/figure-pdf"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}